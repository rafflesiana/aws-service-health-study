,service_name,summary,date,status,details,description,service,datetime,simple_service_name,region_name,start_time,end_time,timezone,failure_time,year,month,day,iso_date,problem,report
0,Auto Scaling (N. Virginia),[RESOLVED] Increased API Error Rates,1525914401,1,,"<div><span class=""yellowfg""> 6:06 PM PDT</span>&nbsp;We are investigating increased API error rates in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 6:26 PM PDT</span>&nbsp;We are continuing to investigate increased API error rates in the US-EAST-1 Region</div><div><span class=""yellowfg""> 7:24 PM PDT</span>&nbsp;We have identified the root cause of the elevated API error rates and can confirm that most customers have recovered. We are continuing to work towards full resolution.</div><div><span class=""yellowfg""> 7:40 PM PDT</span>&nbsp;Between 5:42 PM and 7:04 PM PDT, we experienced increased API error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",autoscaling-us-east-1,2018-05-10 01:06:41,Auto Scaling,N. Virginia,5:42 PM,7:04 PM,PDT,82.0,2018,5,10,2018-05-10,increased api error rates," 6:06 PM PDT -  We are investigating increased API error rates in the US-EAST-1 Region.
 6:26 PM PDT -  We are continuing to investigate increased API error rates in the US-EAST-1 Region
 7:24 PM PDT -  We have identified the root cause of the elevated API error rates and can confirm that most customers have recovered. We are continuing to work towards full resolution.
 7:40 PM PDT -  Between 5:42 PM and 7:04 PM PDT, we experienced increased API error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
1,Amazon Elastic Compute Cloud (N. Virginia),[RESOLVED] Delayed network provisioning,1526397141,1,,"<div><span class=""yellowfg""> 8:12 AM PDT</span>&nbsp;We are investigating delayed network provisioning for newly launched instances in a single Availability Zone in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 8:45 AM PDT</span>&nbsp;Between 5:27 AM and 8:17 AM PDT we experienced delayed network provisioning for newly launched instances in a single Availability Zone in the US-EAST-1 Region. The issue have been resolved and the service is operating normally. </div>",ec2-us-east-1,2018-05-15 15:12:21,Elastic Compute Cloud,N. Virginia,5:27 AM,8:17 AM,PDT,170.0,2018,5,15,2018-05-15,delayed network provisioning," 8:12 AM PDT -  We are investigating delayed network provisioning for newly launched instances in a single Availability Zone in the US-EAST-1 Region.
 8:45 AM PDT -  Between 5:27 AM and 8:17 AM PDT we experienced delayed network provisioning for newly launched instances in a single Availability Zone in the US-EAST-1 Region. The issue have been resolved and the service is operating normally. "
2,Amazon Elastic Load Balancing (N. Virginia),[RESOLVED] Increased provisioning times,1526397279,1,,"<div><span class=""yellowfg""> 8:14 AM PDT</span>&nbsp;We are investigating increased provisioning times for load balancers in the US-EAST-1 Region. Registration of back-end instances and connectivity to existing instances are not affected. </div><div><span class=""yellowfg""> 8:53 AM PDT</span>&nbsp;Between 6:05 AM and 8:17 AM PDT we experienced delayed network provisioning for newly launched instances in a single Availability Zone in the US-EAST-1 Region. Registration of back-end instances and connectivity to existing instances are not affected. The issue have been resolved and the service is operating normally. </div>",elb-us-east-1,2018-05-15 15:14:39,Elastic Load Balancing,N. Virginia,6:05 AM,8:17 AM,PDT,132.0,2018,5,15,2018-05-15,increased provisioning times," 8:14 AM PDT -  We are investigating increased provisioning times for load balancers in the US-EAST-1 Region. Registration of back-end instances and connectivity to existing instances are not affected. 
 8:53 AM PDT -  Between 6:05 AM and 8:17 AM PDT we experienced delayed network provisioning for newly launched instances in a single Availability Zone in the US-EAST-1 Region. Registration of back-end instances and connectivity to existing instances are not affected. The issue have been resolved and the service is operating normally. "
3,Amazon Elastic MapReduce (N. Virginia),[RESOLVED] Delays in starting clusters,1526397532,1,,"<div><span class=""yellowfg""> 8:19 AM PDT</span>&nbsp;We are investigating delays in starting Amazon EMR clusters in the US-EAST-1 Region. Existing clusters are not impacted.</div><div><span class=""yellowfg""> 8:40 AM PDT</span>&nbsp;Between 7:00 AM and 8:15 AM PDT we experienced delays in starting Amazon EMR clusters in the US-EAST-1 Region. Existing clusters were not impacted. The issue has been resolved and the service is operating normally.</div>",emr-us-east-1,2018-05-15 15:18:52,Elastic MapReduce,N. Virginia,7:00 AM,8:15 AM,PDT,75.0,2018,5,15,2018-05-15,delays in starting clusters," 8:19 AM PDT -  We are investigating delays in starting Amazon EMR clusters in the US-EAST-1 Region. Existing clusters are not impacted.
 8:40 AM PDT -  Between 7:00 AM and 8:15 AM PDT we experienced delays in starting Amazon EMR clusters in the US-EAST-1 Region. Existing clusters were not impacted. The issue has been resolved and the service is operating normally."
4,Amazon Simple Storage Service (N. Virginia),[RESOLVED] Elevated error rates,1526466746,1,,"<div><span class=""yellowfg""> 3:32 AM PDT</span>&nbsp;Between 2:35 AM and 2:39 AM PDT, we experienced elevated error rates for requests in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",s3-us-standard,2018-05-16 10:32:26,Simple Storage Service,N. Virginia,2:35 AM,2:39 AM,PDT,4.0,2018,5,16,2018-05-16,elevated error rates," 3:32 AM PDT -  Between 2:35 AM and 2:39 AM PDT, we experienced elevated error rates for requests in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
5,AWS CodeBuild (N. Virginia),[RESOLVED] Increased build error rates,1526469181,1,,"<div><span class=""yellowfg""> 4:13 AM PDT</span>&nbsp;We are investigating increased Build error rates in US-EAST-1 Region. </div><div><span class=""yellowfg""> 5:12 AM PDT</span>&nbsp;Between 2:50 AM and 4:55 AM PDT, we experienced elevated Build error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",codebuild-us-east-1,2018-05-16 11:13:01,CodeBuild,N. Virginia,2:50 AM,4:55 AM,PDT,125.0,2018,5,16,2018-05-16,increased build error rates," 4:13 AM PDT -  We are investigating increased Build error rates in US-EAST-1 Region. 
 5:12 AM PDT -  Between 2:50 AM and 4:55 AM PDT, we experienced elevated Build error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
6,Amazon AppStream 2.0 (N. Virginia),[RESOLVED] Increased error rates,1526472913,1,,"<div><span class=""yellowfg""> 5:15 AM PDT</span>&nbsp;We are currently investigating increased error rates in provisioning streaming instances in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 5:22 AM PDT</span>&nbsp;Between 2:50 AM and 5:06 AM PDT, we experienced increased error rates in provisioning streaming instances. The issue has been resolved now and the service is operating normally.</div>",appstream2-us-east-1,2018-05-16 12:15:13,AppStream 2.0,N. Virginia,2:50 AM,5:06 AM,PDT,136.0,2018,5,16,2018-05-16,increased error rates," 5:15 AM PDT -  We are currently investigating increased error rates in provisioning streaming instances in the US-EAST-1 Region.
 5:22 AM PDT -  Between 2:50 AM and 5:06 AM PDT, we experienced increased error rates in provisioning streaming instances. The issue has been resolved now and the service is operating normally."
7,AWS Systems Manager (N. Virginia),[RESOLVED] Increased error rates,1526473109,1,,"<div><span class=""yellowfg""> 5:18 AM PDT</span>&nbsp;Between 2:50 AM and 4:55 AM PDT we experienced increased API error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",ec2systemsmanager-us-east-1,2018-05-16 12:18:29,Systems Manager,N. Virginia,2:50 AM,4:55 AM,PDT,125.0,2018,5,16,2018-05-16,increased error rates, 5:18 AM PDT -  Between 2:50 AM and 4:55 AM PDT we experienced increased API error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.
8,Amazon Elastic Compute Cloud (Sao Paulo),[RESOLVED] Network Connectivity,1526569732,1,,"<div><span class=""yellowfg""> 8:09 AM PDT</span>&nbsp;We are investigating connectivity issues for some instances in the SA-EAST-1 Region.</div><div><span class=""yellowfg""> 8:32 AM PDT</span>&nbsp; Between 7:26 AM and 7:34 AM PDT, and between 7:57 AM to 8:05 AM PDT, some instances experienced connectivity issues in the SA-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",ec2-sa-east-1,2018-05-17 15:08:52,Elastic Compute Cloud,Sao Paulo,7:26 AM,7:34 AM,PDT,8.0,2018,5,17,2018-05-17,network connectivity," 8:09 AM PDT -  We are investigating connectivity issues for some instances in the SA-EAST-1 Region.
 8:32 AM PDT -   Between 7:26 AM and 7:34 AM PDT, and between 7:57 AM to 8:05 AM PDT, some instances experienced connectivity issues in the SA-EAST-1 Region. The issue has been resolved and the service is operating normally."
9,Amazon WorkMail (N. Virginia),[RESOLVED] Increased Outlook Sync Latencies and Errors,1527016380,1,,"<div><span class=""yellowfg"">12:13 PM PDT</span>&nbsp;We are investigating increased WorkMail Outlook synchronization latencies and error rates in the US-EAST-1 Region impacting the sending and receiving of new messages or appointments in Outlook clients.</div><div><span class=""yellowfg"">12:30 PM PDT</span>&nbsp;Between 11:06 AM and 12:09 PM PDT we experienced increased WorkMail Outlook synchronization latencies and error rates in the US-EAST-1 Region, impacting the sending and receiving of new messages or appointments in Outlook clients. The issue has been resolved and the service is operating normally.</div>",workmail-us-east-1,2018-05-22 19:13:00,WorkMail,N. Virginia,11:06 AM,12:09 PM,PDT,63.0,2018,5,22,2018-05-22,increased outlook sync latencies and errors,"12:13 PM PDT -  We are investigating increased WorkMail Outlook synchronization latencies and error rates in the US-EAST-1 Region impacting the sending and receiving of new messages or appointments in Outlook clients.
12:30 PM PDT -  Between 11:06 AM and 12:09 PM PDT we experienced increased WorkMail Outlook synchronization latencies and error rates in the US-EAST-1 Region, impacting the sending and receiving of new messages or appointments in Outlook clients. The issue has been resolved and the service is operating normally."
10,Amazon Cognito (N. Virginia),[RESOLVED] Elevated Sign In Error Rates,1527647488,1,,"<div><span class=""yellowfg""> 7:31 PM PDT</span>&nbsp;We are investigating reports that some users are unable to sign in or sign up to applications using Facebook identities with Amazon Cognito User Pools.</div><div><span class=""yellowfg""> 8:17 PM PDT</span>&nbsp;Recently, some users were unable to sign in or sign up to applications using Facebook identities with Amazon Cognito User Pools. The issue has been resolved and the service is operating normally.</div>",cognito-us-east-1,2018-05-30 02:31:28,Cognito,N. Virginia,7:31 PM,8:17 PM,PDT,46.0,2018,5,30,2018-05-30,elevated sign in error rates," 7:31 PM PDT -  We are investigating reports that some users are unable to sign in or sign up to applications using Facebook identities with Amazon Cognito User Pools.
 8:17 PM PDT -  Recently, some users were unable to sign in or sign up to applications using Facebook identities with Amazon Cognito User Pools. The issue has been resolved and the service is operating normally."
11,Amazon Elastic Compute Cloud (Ohio),[RESOLVED] Internet Connectivity,1527752189,1,,"<div><span class=""yellowfg"">12:36 AM PDT</span>&nbsp;We are currently investigating connectivity issues in the US-EAST-2 Region.</div><div><span class=""yellowfg""> 1:01 AM PDT</span>&nbsp;Between 12:11 AM and 12:45 AM PDT we experienced impaired Internet connectivity in the US-EAST-2 Region. The issue has been resolved and the service is operating normally.</div>",ec2-us-east-2,2018-05-31 07:36:29,Elastic Compute Cloud,Ohio,12:11 AM,12:45 AM,PDT,34.0,2018,5,31,2018-05-31,internet connectivity,"12:36 AM PDT -  We are currently investigating connectivity issues in the US-EAST-2 Region.
 1:01 AM PDT -  Between 12:11 AM and 12:45 AM PDT we experienced impaired Internet connectivity in the US-EAST-2 Region. The issue has been resolved and the service is operating normally."
12,AWS Internet Connectivity (Ohio),[RESOLVED] Internet Connectivity,1527752941,1,,"<div><span class=""yellowfg"">12:49 AM PDT</span>&nbsp;
We are currently investigating connectivity issues in the US-EAST-2 Region. </div><div><span class=""yellowfg"">12:59 AM PDT</span>&nbsp;Between 12:11 AM and 12:45 AM PDT we experienced impaired Internet connectivity in the US-EAST-2 Region. The issue has been resolved and the service is operating normally.</div>",internetconnectivity-us-east-2,2018-05-31 07:49:01,Internet Connectivity,Ohio,12:11 AM,12:45 AM,PDT,34.0,2018,5,31,2018-05-31,internet connectivity,"12:49 AM PDT -  We are currently investigating connectivity issues in the US-EAST-2 Region. 
12:59 AM PDT -  Between 12:11 AM and 12:45 AM PDT we experienced impaired Internet connectivity in the US-EAST-2 Region. The issue has been resolved and the service is operating normally."
13,Amazon Elastic File System (Oregon),[RESOLVED] Increased Error Rates,1527800578,1,,"<div><span class=""yellowfg""> 2:03 PM PDT</span>&nbsp;We are investigating increased error rates for file system requests in the US-WEST-2 Region.</div><div><span class=""yellowfg""> 2:58 PM PDT</span>&nbsp;Between 1:00 PM and 2:29 PM PDT, we experienced increased error rates for file system read/write requests in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",elasticfilesystem-us-west-2,2018-05-31 21:02:58,Elastic File System,Oregon,1:00 PM,2:29 PM,PDT,89.0,2018,5,31,2018-05-31,increased error rates," 2:03 PM PDT -  We are investigating increased error rates for file system requests in the US-WEST-2 Region.
 2:58 PM PDT -  Between 1:00 PM and 2:29 PM PDT, we experienced increased error rates for file system read/write requests in the US-WEST-2 Region. The issue has been resolved and the service is operating normally."
14,Amazon Elastic Compute Cloud (N. Virginia),[RESOLVED] Instance Connectivity Issues,1527804823,1,,"<div><span class=""yellowfg""> 3:13 PM PDT</span>&nbsp;We are investigating connectivity issues affecting some instances in a single Availability Zone in the US-EAST-1 Region.
</div><div><span class=""yellowfg""> 3:42 PM PDT</span>&nbsp;We can confirm that there has been an issue in one of the datacenters that makes up one of US-EAST-1 Availability Zones. This was a result of a power event impacting a small percentage of the physical servers in that datacenter as well as some of the networking devices. Customers with EC2 instances in this availability zone may see issues with connectivity to the affected instances. We are seeing recovery and continue to work toward full resolution.</div><div><span class=""yellowfg""> 4:29 PM PDT</span>&nbsp;We have restored power to the vast majority of the affected instances and continue to work towards full recovery.</div><div><span class=""yellowfg""> 5:36 PM PDT</span>&nbsp;﻿﻿Beginning at 2:52 PM PDT a small percentage of EC2 servers lost power in a single Availability Zone in the US-EAST-1 Region. This resulted in some impaired EC2 instances and degraded performance for some EBS volumes in the affected Availability Zone. Power was restored at 3:22 PM PDT, at which point the vast majority of instances and volumes saw recovery. We have been working to recover the remaining instances and volumes. The small number of remaining instances and volumes are hosted on hardware which was adversely affected by the loss of power. While we will continue to work to recover all affected instances and volumes, for immediate recovery, we recommend replacing any remaining affected instances or volumes if possible.</div>",ec2-us-east-1,2018-05-31 22:13:43,Elastic Compute Cloud,N. Virginia,2:52 PM,3:22 PM,PDT,30.0,2018,5,31,2018-05-31,instance connectivity issues," 3:13 PM PDT -  We are investigating connectivity issues affecting some instances in a single Availability Zone in the US-EAST-1 Region.
 3:42 PM PDT -  We can confirm that there has been an issue in one of the datacenters that makes up one of US-EAST-1 Availability Zones. This was a result of a power event impacting a small percentage of the physical servers in that datacenter as well as some of the networking devices. Customers with EC2 instances in this availability zone may see issues with connectivity to the affected instances. We are seeing recovery and continue to work toward full resolution.
 4:29 PM PDT -  We have restored power to the vast majority of the affected instances and continue to work towards full recovery.
 5:36 PM PDT -  ﻿﻿Beginning at 2:52 PM PDT a small percentage of EC2 servers lost power in a single Availability Zone in the US-EAST-1 Region. This resulted in some impaired EC2 instances and degraded performance for some EBS volumes in the affected Availability Zone. Power was restored at 3:22 PM PDT, at which point the vast majority of instances and volumes saw recovery. We have been working to recover the remaining instances and volumes. The small number of remaining instances and volumes are hosted on hardware which was adversely affected by the loss of power. While we will continue to work to recover all affected instances and volumes, for immediate recovery, we recommend replacing any remaining affected instances or volumes if possible."
15,Amazon Redshift (N. Virginia),[RESOLVED] Connectivity Issues,1527806609,1,,"<div><span class=""yellowfg""> 3:43 PM PDT</span>&nbsp;We are investigating connectivity issues affecting some Redshift clusters in a single Availability Zone in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 4:07 PM PDT</span>&nbsp;Between 2:52 PM and 3:55 PM PDT we experienced connectivity issues affecting some clusters in a single Availability Zone in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",redshift-us-east-1,2018-05-31 22:43:29,Redshift,N. Virginia,2:52 PM,3:55 PM,PDT,63.0,2018,5,31,2018-05-31,connectivity issues," 3:43 PM PDT -  We are investigating connectivity issues affecting some Redshift clusters in a single Availability Zone in the US-EAST-1 Region.
 4:07 PM PDT -  Between 2:52 PM and 3:55 PM PDT we experienced connectivity issues affecting some clusters in a single Availability Zone in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. "
16,Amazon WorkSpaces (N. Virginia),[RESOLVED] Connectivity Issues,1527806975,1,,"<div><span class=""yellowfg""> 3:49 PM PDT</span>&nbsp;We are investigating elevated error rates for connections to some WorkSpaces in a single Availability Zone in the US-EAST-1 Region.
</div><div><span class=""yellowfg""> 4:18 PM PDT</span>&nbsp;Between 2:52 PM and 4:15 PM PDT we experienced increased error rates for connections to some WorkSpaces in a single Availability Zone in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",workspaces-us-east-1,2018-05-31 22:49:35,WorkSpaces,N. Virginia,2:52 PM,4:15 PM,PDT,83.0,2018,5,31,2018-05-31,connectivity issues," 3:49 PM PDT -  We are investigating elevated error rates for connections to some WorkSpaces in a single Availability Zone in the US-EAST-1 Region.
 4:18 PM PDT -  Between 2:52 PM and 4:15 PM PDT we experienced increased error rates for connections to some WorkSpaces in a single Availability Zone in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
17,Amazon Relational Database Service (N. Virginia),[RESOLVED] Connectivity Issues,1527807509,1,,"<div><span class=""yellowfg""> 3:58 PM PDT</span>&nbsp;We are investigating connectivity issues affecting some database instances in a single Availability Zone in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 4:12 PM PDT</span>&nbsp;Beginning at 2:52 PM PDT, we experienced connectivity issues affecting some database instances in a single Availability Zone in the US-EAST-1 Region. Multi-AZ database instances functioned correctly and remain available. A small number of Single-AZ database instances remain unavailable and we are working to resolve the issue.</div><div><span class=""yellowfg""> 5:42 PM PDT</span>&nbsp;Beginning at 2:52 PM PDT, we experienced connectivity issues affecting some database instances in a single Availability Zone in the US-EAST-1 Region. Multi-AZ database instances functioned correctly and remained available. Connectivity was restored at 3:22 PM PDT, at which point the vast majority of database instances recovered. A small number of Single-AZ database instances remain unavailable and we are continuing to work to recover all affected database instances.</div>",rds-us-east-1,2018-05-31 22:58:29,Relational Database Service,N. Virginia,2:52 PM,3:22 PM,PDT,30.0,2018,5,31,2018-05-31,connectivity issues," 3:58 PM PDT -  We are investigating connectivity issues affecting some database instances in a single Availability Zone in the US-EAST-1 Region.
 4:12 PM PDT -  Beginning at 2:52 PM PDT, we experienced connectivity issues affecting some database instances in a single Availability Zone in the US-EAST-1 Region. Multi-AZ database instances functioned correctly and remain available. A small number of Single-AZ database instances remain unavailable and we are working to resolve the issue.
 5:42 PM PDT -  Beginning at 2:52 PM PDT, we experienced connectivity issues affecting some database instances in a single Availability Zone in the US-EAST-1 Region. Multi-AZ database instances functioned correctly and remained available. Connectivity was restored at 3:22 PM PDT, at which point the vast majority of database instances recovered. A small number of Single-AZ database instances remain unavailable and we are continuing to work to recover all affected database instances."
18,Amazon Route 53,[RESOLVED] Increased Propagation Time,1528291058,1,,"<div><span class=""yellowfg""> 6:17 AM PDT</span>&nbsp;We are investigating increased propagation times of DNS edits to the Route 53 DNS servers. This does not impact queries to existing DNS records.</div><div><span class=""yellowfg""> 6:47 AM PDT</span>&nbsp;Between 5:42 AM and 6:45 AM PDT, we experienced increased propagation times of DNS edits to the Route 53 DNS servers. This did not impact queries to existing DNS records. The issue has been resolved and the service is operating normally.</div>",route53,2018-06-06 13:17:38,Route 53,Global,5:42 AM,6:45 AM,PDT,63.0,2018,6,6,2018-06-06,increased propagation time," 6:17 AM PDT -  We are investigating increased propagation times of DNS edits to the Route 53 DNS servers. This does not impact queries to existing DNS records.
 6:47 AM PDT -  Between 5:42 AM and 6:45 AM PDT, we experienced increased propagation times of DNS edits to the Route 53 DNS servers. This did not impact queries to existing DNS records. The issue has been resolved and the service is operating normally."
19,Auto Scaling (N. Virginia),[RESOLVED] Increased API Error Rates,1528330928,1,,"<div><span class=""yellowfg""> 5:22 PM PDT</span>&nbsp;We are investigating increased error rates for API calls in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 5:33 PM PDT</span>&nbsp;Between 5:05 PM and 5:26 PM PDT we experienced increased error rates for API calls in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",autoscaling-us-east-1,2018-06-07 00:22:08,Auto Scaling,N. Virginia,5:05 PM,5:26 PM,PDT,21.0,2018,6,7,2018-06-07,increased api error rates," 5:22 PM PDT -  We are investigating increased error rates for API calls in the US-EAST-1 Region.
 5:33 PM PDT -  Between 5:05 PM and 5:26 PM PDT we experienced increased error rates for API calls in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
20,Amazon CloudFront,[RESOLVED] Delays in propagating certain configuration changes to a few CloudFront Edge locations ,1528895774,1,,"<div><span class=""yellowfg""> 6:16 AM PDT</span>&nbsp;We are investigating longer than usual propagation times to propagate certain configuration changes to a few of our edge locations: the creation and deletion of CloudFront distributions and the updating of certificates for a CloudFront distribution. All other CloudFront configuration changes are propagating normally. End-user requests for content from our edge locations are not affected by this issue and are being served normally. </div><div><span class=""yellowfg""> 6:59 AM PDT</span>&nbsp;Between 3:25 AM and 6:45 AM PDT, CloudFront experienced longer than usual propagation times to propagate certain configuration changes to a few of our edge locations. This issue has been resolved and the service is operating normally. During this time, end user requests for content from our edge locations were not affected by this issue and were being served normally.</div>",cloudfront,2018-06-13 13:16:14,CloudFront,Global,3:25 AM,6:45 AM,PDT,200.0,2018,6,13,2018-06-13,delays in propagating certain configuration changes to a few cloudfront edge locations," 6:16 AM PDT -  We are investigating longer than usual propagation times to propagate certain configuration changes to a few of our edge locations: the creation and deletion of CloudFront distributions and the updating of certificates for a CloudFront distribution. All other CloudFront configuration changes are propagating normally. End-user requests for content from our edge locations are not affected by this issue and are being served normally. 
 6:59 AM PDT -  Between 3:25 AM and 6:45 AM PDT, CloudFront experienced longer than usual propagation times to propagate certain configuration changes to a few of our edge locations. This issue has been resolved and the service is operating normally. During this time, end user requests for content from our edge locations were not affected by this issue and were being served normally."
21,Amazon Elastic Load Balancing (N. Virginia),[RESOLVED] Increased API Error Rates ,1528907930,1,,"<div><span class=""yellowfg""> 9:39 AM PDT</span>&nbsp;We are investigating increased error rates and latencies for ELB APIs in the US-EAST-1 Region. Traffic to existing load balancers is not affected.</div><div><span class=""yellowfg"">10:11 AM PDT</span>&nbsp;Between 8:59 AM and 9:58 AM PDT we experienced increased error rates and latencies for the ELB APIs in the US-EAST-1 Region. The issue affected load balancer provisioning and target registration for Application and Network Load Balancer. Classic Load Balancer, as well as traffic to all existing load balancers, was not affected. The issue has been resolved and the service is operating normally. </div>",elb-us-east-1,2018-06-13 16:38:50,Elastic Load Balancing,N. Virginia,8:59 AM,9:58 AM,PDT,59.0,2018,6,13,2018-06-13,increased api error rates," 9:39 AM PDT -  We are investigating increased error rates and latencies for ELB APIs in the US-EAST-1 Region. Traffic to existing load balancers is not affected.
10:11 AM PDT -  Between 8:59 AM and 9:58 AM PDT we experienced increased error rates and latencies for the ELB APIs in the US-EAST-1 Region. The issue affected load balancer provisioning and target registration for Application and Network Load Balancer. Classic Load Balancer, as well as traffic to all existing load balancers, was not affected. The issue has been resolved and the service is operating normally. "
22,AWS CodeDeploy (N. Virginia),[RESOLVED] Elevated API Error Rates,1529347567,1,,"<div><span class=""yellowfg"">11:46 AM PDT</span>&nbsp;We are investigating elevated error rates for APIs in the US-EAST-1 Region.</div><div><span class=""yellowfg"">12:31 PM PDT</span>&nbsp;Between 11:11 AM and 12:00 PM PDT we experienced elevated error rates for APIs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",codedeploy-us-east-1,2018-06-18 18:46:07,CodeDeploy,N. Virginia,11:11 AM,12:00 PM,PDT,49.0,2018,6,18,2018-06-18,elevated api error rates,"11:46 AM PDT -  We are investigating elevated error rates for APIs in the US-EAST-1 Region.
12:31 PM PDT -  Between 11:11 AM and 12:00 PM PDT we experienced elevated error rates for APIs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. "
23,AWS Cloud9 (N. Virginia),[RESOLVED] Increased API Error Rates,1529609394,1,,"<div><span class=""yellowfg"">12:30 PM PDT</span>&nbsp;Between 11:01 AM and 11:20 AM PDT we experienced increased error rates in creating and accessing Cloud9 environments in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",cloud9-us-east-1,2018-06-21 19:29:54,Cloud9,N. Virginia,11:01 AM,11:20 AM,PDT,19.0,2018,6,21,2018-06-21,increased api error rates,12:30 PM PDT -  Between 11:01 AM and 11:20 AM PDT we experienced increased error rates in creating and accessing Cloud9 environments in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. 
24,AWS Identity and Access Management,[RESOLVED] Increased Error Rates and Latencies,1529622992,1,,"<div><span class=""yellowfg""> 4:16 PM PDT</span>&nbsp;We are investigating increased error rates and latencies for the IAM APIs.</div><div><span class=""yellowfg""> 4:24 PM PDT</span>&nbsp;Between 3:38 PM and 4:11 PM PDT we experienced increased IAM API error rates and latencies. The issue has been resolved and the service is operating normally.</div>",iam,2018-06-21 23:16:32,Identity and Access Management,Global,3:38 PM,4:11 PM,PDT,33.0,2018,6,21,2018-06-21,increased error rates and latencies," 4:16 PM PDT -  We are investigating increased error rates and latencies for the IAM APIs.
 4:24 PM PDT -  Between 3:38 PM and 4:11 PM PDT we experienced increased IAM API error rates and latencies. The issue has been resolved and the service is operating normally."
25,Amazon Elastic File System (N. Virginia),[RESOLVED] Increased API Error Rates,1530065273,1,,"<div><span class=""yellowfg""> 7:08 PM PDT</span>&nbsp;Between 5:14 PM and 6:09 PM PDT, we experienced increased error rates for API requests in the US-EAST-1 Region. The issue has been resolved and the service is now operating normally.</div>",elasticfilesystem-us-east-1,2018-06-27 02:07:53,Elastic File System,N. Virginia,5:14 PM,6:09 PM,PDT,55.0,2018,6,27,2018-06-27,increased api error rates," 7:08 PM PDT -  Between 5:14 PM and 6:09 PM PDT, we experienced increased error rates for API requests in the US-EAST-1 Region. The issue has been resolved and the service is now operating normally."
26,AWS IoT Core (N. Virginia),[RESOLVED] Increased Error Rates,1530296421,1,,"<div><span class=""yellowfg"">11:20 AM PDT</span>&nbsp;We are investigating increased error rates for connections to AWS IoT Core, affecting some customers in the US-EAST-1 Region. </div><div><span class=""yellowfg"">11:38 AM PDT</span>&nbsp;We continue to investigate increased error rates for connections to AWS IoT Core, affecting some customers in the US-EAST-1 Region. </div><div><span class=""yellowfg"">12:02 PM PDT</span>&nbsp;Error rates for connections to AWS IoT Core in the US-EAST-1 Region are improving. We continue to work towards full resolution. </div><div><span class=""yellowfg"">12:44 PM PDT</span>&nbsp;Between 9:26 AM and 11:56 AM PDT we experienced intermittent elevated error rates for connections to AWS IoT Core in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",awsiot-us-east-1,2018-06-29 18:20:21,IoT Core,N. Virginia,9:26 AM,11:56 AM,PDT,150.0,2018,6,29,2018-06-29,increased error rates,"11:20 AM PDT -  We are investigating increased error rates for connections to AWS IoT Core, affecting some customers in the US-EAST-1 Region. 
11:38 AM PDT -  We continue to investigate increased error rates for connections to AWS IoT Core, affecting some customers in the US-EAST-1 Region. 
12:02 PM PDT -  Error rates for connections to AWS IoT Core in the US-EAST-1 Region are improving. We continue to work towards full resolution. 
12:44 PM PDT -  Between 9:26 AM and 11:56 AM PDT we experienced intermittent elevated error rates for connections to AWS IoT Core in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. "
27,Amazon Chime,[RESOLVED] Availability,1530900123,1,,"<div><span class=""yellowfg"">11:02 AM PDT</span>&nbsp;We are investigating meeting availability issues in the US-EAST-1 Region.</div><div><span class=""yellowfg"">11:18 AM PDT</span>&nbsp;We continue to investigate meeting availability issues in the US-EAST-1 Region. </div><div><span class=""yellowfg"">11:49 AM PDT</span>&nbsp;Between 10:24 AM PDT and 11:31 AM PDT, we experienced meeting availability issues in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",chime,2018-07-06 18:02:03,Chime,Global,10:24 AM,11:31 AM,PDT,67.0,2018,7,6,2018-07-06,availability,"11:02 AM PDT -  We are investigating meeting availability issues in the US-EAST-1 Region.
11:18 AM PDT -  We continue to investigate meeting availability issues in the US-EAST-1 Region. 
11:49 AM PDT -  Between 10:24 AM PDT and 11:31 AM PDT, we experienced meeting availability issues in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. "
28,Amazon Route 53,[RESOLVED] Increased Propagation Time,1531395752,1,,"<div><span class=""yellowfg""> 4:44 AM PDT</span>&nbsp;We are investigating increased propagation times of DNS edits to the Route 53 DNS servers. This does not impact queries to existing DNS records.</div><div><span class=""yellowfg""> 5:15 AM PDT</span>&nbsp;We can confirm increased propagation times of DNS edits to the Route 53 DNS servers and continue to work towards resolution. This does not impact queries to existing DNS records.</div><div><span class=""yellowfg""> 6:02 AM PDT</span>&nbsp;Propagation of DNS edits for Route 53 records in Public Hosted Zones has recovered. We are still working towards recovery for DNS edits for records in Route 53 Private Hosted Zones.</div><div><span class=""yellowfg""> 6:24 AM PDT</span>&nbsp;Between 4:00 AM and 6:14 AM PDT we experienced increased propagation times of DNS edits to a single Route 53 edge location. This does not impact queries to existing DNS records. The issue has been resolved and the service is operating normally.</div>",route53,2018-07-12 11:42:32,Route 53,Global,4:00 AM,6:14 AM,PDT,134.0,2018,7,12,2018-07-12,increased propagation time," 4:44 AM PDT -  We are investigating increased propagation times of DNS edits to the Route 53 DNS servers. This does not impact queries to existing DNS records.
 5:15 AM PDT -  We can confirm increased propagation times of DNS edits to the Route 53 DNS servers and continue to work towards resolution. This does not impact queries to existing DNS records.
 6:02 AM PDT -  Propagation of DNS edits for Route 53 records in Public Hosted Zones has recovered. We are still working towards recovery for DNS edits for records in Route 53 Private Hosted Zones.
 6:24 AM PDT -  Between 4:00 AM and 6:14 AM PDT we experienced increased propagation times of DNS edits to a single Route 53 edge location. This does not impact queries to existing DNS records. The issue has been resolved and the service is operating normally."
29,AWS Management Console,[RESOLVED] Increased Error Rates,1531774261,1,,"<div><span class=""yellowfg""> 1:51 PM PDT</span>&nbsp;We are currently experiencing intermittent errors accessing the AWS Management Console. AWS services are operating normally.</div><div><span class=""yellowfg""> 2:10 PM PDT</span>&nbsp;We are currently experiencing intermittent errors accessing the AWS Management Console when using root account login credentials. The underlying AWS services and console logins using IAM users are operating normally.</div><div><span class=""yellowfg""> 2:47 PM PDT</span>&nbsp;We have identified the root cause for the intermittent errors accessing the AWS Management Console when using root account login credentials. We are beginning to see recovery, and continue to work toward full resolution. The underlying AWS services and console logins using IAM users continue to operate normally.</div><div><span class=""yellowfg""> 3:44 PM PDT</span>&nbsp;Between 12:28 PM and 3:13 PM PDT we experienced intermittent errors accessing the AWS Management Console. AWS services were not affected by this event. This issue has been resolved and the service is operating normally.</div>",management-console,2018-07-16 20:51:01,Management Console,Global,12:28 PM,3:13 PM,PDT,165.0,2018,7,16,2018-07-16,increased error rates," 1:51 PM PDT -  We are currently experiencing intermittent errors accessing the AWS Management Console. AWS services are operating normally.
 2:10 PM PDT -  We are currently experiencing intermittent errors accessing the AWS Management Console when using root account login credentials. The underlying AWS services and console logins using IAM users are operating normally.
 2:47 PM PDT -  We have identified the root cause for the intermittent errors accessing the AWS Management Console when using root account login credentials. We are beginning to see recovery, and continue to work toward full resolution. The underlying AWS services and console logins using IAM users continue to operate normally.
 3:44 PM PDT -  Between 12:28 PM and 3:13 PM PDT we experienced intermittent errors accessing the AWS Management Console. AWS services were not affected by this event. This issue has been resolved and the service is operating normally."
30,Amazon Elastic Compute Cloud (Frankfurt),[RESOLVED] Network Connectivity,1531962885,1,,"<div><span class=""yellowfg""> 6:14 PM PDT</span>&nbsp;We are investigating connectivity issues to a single Availability Zone in the EU-CENTRAL-1 Region.</div><div><span class=""yellowfg""> 6:34 PM PDT</span>&nbsp;Between 5:39 PM and 5:49 PM PDT we experienced network connectivity issues for some instances in a single Availability Zone in the EU-CENTRAL-1 Region. The issue has been resolved and the service is operating normally.</div>",ec2-eu-central-1,2018-07-19 01:14:45,Elastic Compute Cloud,Frankfurt,5:39 PM,5:49 PM,PDT,10.0,2018,7,19,2018-07-19,network connectivity," 6:14 PM PDT -  We are investigating connectivity issues to a single Availability Zone in the EU-CENTRAL-1 Region.
 6:34 PM PDT -  Between 5:39 PM and 5:49 PM PDT we experienced network connectivity issues for some instances in a single Availability Zone in the EU-CENTRAL-1 Region. The issue has been resolved and the service is operating normally."
31,Amazon Simple Queue Service (Tokyo),[解決済み]SQSでのコネクションリセット及びソケットタイムアウトの発生 | [RESOLVED] Connection reset and socket timeout for SQS ,1533208334,1,,"<div><span class=""yellowfg""> 4:12 AM PDT</span>&nbsp;日本時間 2018/08/02 17:30 から 17:55 にかけて、東京リージョンの SQS においてコネクションリセット及びソケットタイムアウトの発生率が上昇しました。問題は既に解消し、現在サービスは正常に稼働しております。| Between 1:30AM and 1:55 AM PDT we experienced connection resets and socket timeout rates in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally. </div><div><span class=""yellowfg""> 4:22 AM PDT</span>&nbsp;日本時間 2018/08/02 17:30 から 17:55 にかけて、東京リージョンの SQS においてコネクションリセット及びソケットタイムアウトの発生率が上昇しました。問題は既に解消し、現在サービスは正常に稼働しております。| Between 1:30AM and 1:55 AM PDT we experienced connection resets and socket timeout rates in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally.</div>",sqs-ap-northeast-1,2018-08-02 11:12:14,Simple Queue Service,Tokyo,1:30AM,1:55 AM,PDT,25.0,2018,8,2,2018-08-02,connection reset and socket timeout for sqs," 4:12 AM PDT -  日本時間 2018/08/02 17:30 から 17:55 にかけて、東京リージョンの SQS においてコネクションリセット及びソケットタイムアウトの発生率が上昇しました。問題は既に解消し、現在サービスは正常に稼働しております。| Between 1:30AM and 1:55 AM PDT we experienced connection resets and socket timeout rates in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally. 
 4:22 AM PDT -  日本時間 2018/08/02 17:30 から 17:55 にかけて、東京リージョンの SQS においてコネクションリセット及びソケットタイムアウトの発生率が上昇しました。問題は既に解消し、現在サービスは正常に稼働しております。| Between 1:30AM and 1:55 AM PDT we experienced connection resets and socket timeout rates in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally."
32,Amazon Route 53,[RESOLVED]  Increased Console and API error rates,1533545240,1,,"<div><span class=""yellowfg""> 1:47 AM PDT</span>&nbsp;Between 12:49 AM and 1:04 AM PDT customers using the Route 53 console and API experienced an elevated error rate. The issue has been resolved and both the Route 53 console and API are operating normally. Route 53 DNS and health checks were both operating normally during this time.</div>",route53,2018-08-06 08:47:20,Route 53,Global,12:49 AM,1:04 AM,PDT,15.0,2018,8,6,2018-08-06,increased console and api error rates, 1:47 AM PDT -  Between 12:49 AM and 1:04 AM PDT customers using the Route 53 console and API experienced an elevated error rate. The issue has been resolved and both the Route 53 console and API are operating normally. Route 53 DNS and health checks were both operating normally during this time.
33,Amazon Elastic Compute Cloud (N. Virginia),[RESOLVED] Instance launch errors and connectivity issues,1533674185,0,,"<div><span class=""yellowfg""> 1:49 PM PDT</span>&nbsp;Between 1:06 PM and 1:33 PM PDT we experienced increased error rates for new instance launches and intermittent network connectivity issues for some running instances in a single Availability Zone in the US-EAST-1 Region. The issue has been resolved and the service is operating normally</div>",ec2-us-east-1,2018-08-07 20:36:25,Elastic Compute Cloud,N. Virginia,1:06 PM,1:33 PM,PDT,27.0,2018,8,7,2018-08-07,instance launch errors and connectivity issues, 1:49 PM PDT -  Between 1:06 PM and 1:33 PM PDT we experienced increased error rates for new instance launches and intermittent network connectivity issues for some running instances in a single Availability Zone in the US-EAST-1 Region. The issue has been resolved and the service is operating normally
34,Amazon Simple Notification Service (Tokyo),[RESOLVED] SNSのエラーレート上昇について | Elevated error rates for SNS,1533695504,1,,"<div><span class=""yellowfg""> 7:31 PM PDT</span>&nbsp;東京リージョンにおきまして、現在SNS Publish APIのエラーレートが上昇しており、原因を調査しております。| We are currently investigating elevated error rates for the SNS Publish API in the AP-NORTHEAST-1 Region.</div><div><span class=""yellowfg""> 8:34 PM PDT</span>&nbsp;東京リージョンにおきまして、日本時間 2018年8月8日 10:47-12:08の間、SNS Publish APIのエラーレートが上昇しておりました。事象は復旧し、サービスは正常に稼働しております。| Between 6:47 PM and 8:08 PM PDT we experienced elevated error rates for the SNS Publish API in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally. </div>",sns-ap-northeast-1,2018-08-08 02:31:44,Simple Notification Service,Tokyo,6:47 PM,8:08 PM,PDT,81.0,2018,8,8,2018-08-08,sns elevated error rates for sns," 7:31 PM PDT -  東京リージョンにおきまして、現在SNS Publish APIのエラーレートが上昇しており、原因を調査しております。| We are currently investigating elevated error rates for the SNS Publish API in the AP-NORTHEAST-1 Region.
 8:34 PM PDT -  東京リージョンにおきまして、日本時間 2018年8月8日 10:47-12:08の間、SNS Publish APIのエラーレートが上昇しておりました。事象は復旧し、サービスは正常に稼働しております。| Between 6:47 PM and 8:08 PM PDT we experienced elevated error rates for the SNS Publish API in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally. "
35,AWS Management Console,[RESOLVED] エラーレートの上昇について | Elevated Error Rates,1533696595,1,,"<div><span class=""yellowfg""> 7:50 PM PDT</span>&nbsp;東京リージョンにおきまして、AWS マネジメントコンソールのエラーレートが上昇しており、原因を調査しております。| We are investigating elevated error rates for the AWS Management Console in the AP-NORTHEAST-1 Region.</div><div><span class=""yellowfg""> 8:26 PM PDT</span>&nbsp;東京リージョンにおきまして、日本時間 2018年8月8日 10:47-12:08にかけて AWS マネージメントコンソールのエラーレートが上昇しておりました。事象は復旧し、コンソールは正常に稼働しております。| Between 6:47 PM and 8:08 PM PDT, users experienced elevated error rates for the AWS Management Console in the AP-NORTHEAST-1 Region. The issue has been resolved and the Console is operating normally. </div>",management-console,2018-08-08 02:49:55,Management Console,Global,6:47 PM,8:08 PM,PDT,81.0,2018,8,8,2018-08-08,elevated error rates," 7:50 PM PDT -  東京リージョンにおきまして、AWS マネジメントコンソールのエラーレートが上昇しており、原因を調査しております。| We are investigating elevated error rates for the AWS Management Console in the AP-NORTHEAST-1 Region.
 8:26 PM PDT -  東京リージョンにおきまして、日本時間 2018年8月8日 10:47-12:08にかけて AWS マネージメントコンソールのエラーレートが上昇しておりました。事象は復旧し、コンソールは正常に稼働しております。| Between 6:47 PM and 8:08 PM PDT, users experienced elevated error rates for the AWS Management Console in the AP-NORTHEAST-1 Region. The issue has been resolved and the Console is operating normally. "
36,Amazon Elastic MapReduce (Mumbai),[RESOLVED] Failures in starting clusters,1533722524,1,,"<div><span class=""yellowfg""> 3:02 AM PDT</span>&nbsp;We are investigating increased error rates impacting start up of clusters operations in the AP-SOUTH-1 Region.</div><div><span class=""yellowfg""> 4:13 AM PDT</span>&nbsp;Between 5:10 PM on August 7th, and 3:50 AM PDT on August 8th, we experienced increased error rates in starting clusters in the AP-SOUTH-1 Region. The issue has been resolved and the service is operating normally.</div>",emr-ap-south-1,2018-08-08 10:02:04,Elastic MapReduce,Mumbai,5:10 PM,3:50 AM,PDT,640.0,2018,8,8,2018-08-08,failures in starting clusters," 3:02 AM PDT -  We are investigating increased error rates impacting start up of clusters operations in the AP-SOUTH-1 Region.
 4:13 AM PDT -  Between 5:10 PM on August 7th, and 3:50 AM PDT on August 8th, we experienced increased error rates in starting clusters in the AP-SOUTH-1 Region. The issue has been resolved and the service is operating normally."
37,Amazon Elastic MapReduce (Paris),[RESOLVED] Failures in starting clusters,1533722689,1,,"<div><span class=""yellowfg""> 3:05 AM PDT</span>&nbsp;We are investigating increased error rates impacting start up of clusters operations in the EU-WEST-3 Region.</div><div><span class=""yellowfg""> 4:16 AM PDT</span>&nbsp;Between 5:10 PM on August 7th, and 3:50 AM PDT on August 8th, we experienced increased error rates in starting clusters in the EU-WEST-3 Region. The issue has been resolved and the service is operating normally.</div>",emr-eu-west-3,2018-08-08 10:04:49,Elastic MapReduce,Paris,5:10 PM,3:50 AM,PDT,640.0,2018,8,8,2018-08-08,failures in starting clusters," 3:05 AM PDT -  We are investigating increased error rates impacting start up of clusters operations in the EU-WEST-3 Region.
 4:16 AM PDT -  Between 5:10 PM on August 7th, and 3:50 AM PDT on August 8th, we experienced increased error rates in starting clusters in the EU-WEST-3 Region. The issue has been resolved and the service is operating normally."
38,Amazon Elastic MapReduce (Ohio),[RESOLVED] Failures in starting clusters,1533722825,1,,"<div><span class=""yellowfg""> 3:07 AM PDT</span>&nbsp;We are investigating increased error rates impacting start up of clusters operations in the US-EAST-2 Region.</div><div><span class=""yellowfg""> 4:18 AM PDT</span>&nbsp;Between 5:10 PM on August 7th, and 3:50 AM PDT on August 8th, we experienced increased error rates in starting clusters in the US-EAST-2 Region. The issue has been resolved and the service is operating normally.</div>",emr-us-east-2,2018-08-08 10:07:05,Elastic MapReduce,Ohio,5:10 PM,3:50 AM,PDT,640.0,2018,8,8,2018-08-08,failures in starting clusters," 3:07 AM PDT -  We are investigating increased error rates impacting start up of clusters operations in the US-EAST-2 Region.
 4:18 AM PDT -  Between 5:10 PM on August 7th, and 3:50 AM PDT on August 8th, we experienced increased error rates in starting clusters in the US-EAST-2 Region. The issue has been resolved and the service is operating normally."
39,Amazon Elastic MapReduce (N. Virginia),[RESOLVED] Failures in starting clusters,1533722962,1,,"<div><span class=""yellowfg""> 3:09 AM PDT</span>&nbsp;We are investigating increased error rates impacting start up of clusters operations in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 4:19 AM PDT</span>&nbsp;Between 5:10 PM on August 7th, and 3:50 AM PDT on August 8th, we experienced increased error rates in starting clusters in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",emr-us-east-1,2018-08-08 10:09:22,Elastic MapReduce,N. Virginia,5:10 PM,3:50 AM,PDT,640.0,2018,8,8,2018-08-08,failures in starting clusters," 3:09 AM PDT -  We are investigating increased error rates impacting start up of clusters operations in the US-EAST-1 Region.
 4:19 AM PDT -  Between 5:10 PM on August 7th, and 3:50 AM PDT on August 8th, we experienced increased error rates in starting clusters in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
40,Amazon Elastic MapReduce (Ireland),[RESOLVED] Failures in starting clusters,1533723112,1,,"<div><span class=""yellowfg""> 3:12 AM PDT</span>&nbsp;We are investigating increased error rates impacting start up of clusters operations in the EU-WEST-1 Region.</div><div><span class=""yellowfg""> 4:21 AM PDT</span>&nbsp;Between 5:10 PM on August 7th, and 3:50 AM PDT on August 8th, we experienced increased error rates in starting clusters in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",emr-eu-west-1,2018-08-08 10:11:52,Elastic MapReduce,Ireland,5:10 PM,3:50 AM,PDT,640.0,2018,8,8,2018-08-08,failures in starting clusters," 3:12 AM PDT -  We are investigating increased error rates impacting start up of clusters operations in the EU-WEST-1 Region.
 4:21 AM PDT -  Between 5:10 PM on August 7th, and 3:50 AM PDT on August 8th, we experienced increased error rates in starting clusters in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally."
41,Amazon Elastic MapReduce (Frankfurt),[RESOLVED] Failures in starting clusters,1533723257,1,,"<div><span class=""yellowfg""> 3:14 AM PDT</span>&nbsp;We are investigating increased error rates impacting start up of clusters operations in the EU-CENTRAL-1 Region</div><div><span class=""yellowfg""> 4:23 AM PDT</span>&nbsp;Between 5:10 PM on August 7th, and 3:50 AM PDT on August 8th, we experienced increased error rates in starting clusters in the EU-CENTRAL-1 Region. The issue has been resolved and the service is operating normally.</div>",emr-eu-central-1,2018-08-08 10:14:17,Elastic MapReduce,Frankfurt,5:10 PM,3:50 AM,PDT,640.0,2018,8,8,2018-08-08,failures in starting clusters," 3:14 AM PDT -  We are investigating increased error rates impacting start up of clusters operations in the EU-CENTRAL-1 Region
 4:23 AM PDT -  Between 5:10 PM on August 7th, and 3:50 AM PDT on August 8th, we experienced increased error rates in starting clusters in the EU-CENTRAL-1 Region. The issue has been resolved and the service is operating normally."
42,Amazon Elastic MapReduce (N. California),[RESOLVED] Failures in starting clusters,1533723374,1,,"<div><span class=""yellowfg""> 3:16 AM PDT</span>&nbsp;We are investigating increased error rates impacting start up of clusters operations in the US-WEST-1 Region.</div><div><span class=""yellowfg""> 4:26 AM PDT</span>&nbsp;Between 5:10 PM on August 7th, and 3:50 AM PDT on August 8th, we experienced increased error rates in starting clusters in the US-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",emr-us-west-1,2018-08-08 10:16:14,Elastic MapReduce,N. California,5:10 PM,3:50 AM,PDT,640.0,2018,8,8,2018-08-08,failures in starting clusters," 3:16 AM PDT -  We are investigating increased error rates impacting start up of clusters operations in the US-WEST-1 Region.
 4:26 AM PDT -  Between 5:10 PM on August 7th, and 3:50 AM PDT on August 8th, we experienced increased error rates in starting clusters in the US-WEST-1 Region. The issue has been resolved and the service is operating normally."
43,Amazon Elastic MapReduce (Singapore),[RESOLVED] Failures in starting clusters,1533723484,1,,"<div><span class=""yellowfg""> 3:18 AM PDT</span>&nbsp;We are investigating increased error rates impacting start up of clusters operations in the AP-SOUTHEAST-1 Region.</div><div><span class=""yellowfg""> 4:29 AM PDT</span>&nbsp;Between 5:10 PM on August 7th, and 3:50 AM PDT on August 8th, we experienced increased error rates in starting clusters in the AP-SOUTHEAST-1 Region. The issue has been resolved and the service is operating normally.</div>",emr-ap-southeast-1,2018-08-08 10:18:04,Elastic MapReduce,Singapore,5:10 PM,3:50 AM,PDT,640.0,2018,8,8,2018-08-08,failures in starting clusters," 3:18 AM PDT -  We are investigating increased error rates impacting start up of clusters operations in the AP-SOUTHEAST-1 Region.
 4:29 AM PDT -  Between 5:10 PM on August 7th, and 3:50 AM PDT on August 8th, we experienced increased error rates in starting clusters in the AP-SOUTHEAST-1 Region. The issue has been resolved and the service is operating normally."
44,Amazon Elastic MapReduce (Sydney),[RESOLVED] Failures in starting clusters,1533723818,1,,"<div><span class=""yellowfg""> 3:23 AM PDT</span>&nbsp;We are investigating increased error rates impacting start up of clusters operations in the AP-SOUTHEAST-2 Region.</div><div><span class=""yellowfg""> 4:31 AM PDT</span>&nbsp;Between 5:10 PM on August 7th, and 3:50 AM PDT on August 8th, we experienced increased error rates in starting clusters in the AP-SOUTHEAST-2 Region. The issue has been resolved and the service is operating normally.</div>",emr-ap-southeast-2,2018-08-08 10:23:38,Elastic MapReduce,Sydney,5:10 PM,3:50 AM,PDT,640.0,2018,8,8,2018-08-08,failures in starting clusters," 3:23 AM PDT -  We are investigating increased error rates impacting start up of clusters operations in the AP-SOUTHEAST-2 Region.
 4:31 AM PDT -  Between 5:10 PM on August 7th, and 3:50 AM PDT on August 8th, we experienced increased error rates in starting clusters in the AP-SOUTHEAST-2 Region. The issue has been resolved and the service is operating normally."
45,AWS CodeDeploy (Ireland),[RESOLVED] Increased Deployment Latencies,1534782477,1,,"<div><span class=""yellowfg""> 9:28 AM PDT</span>&nbsp;We are investigating increased deployment latencies in the EU-WEST-1 Region.</div><div><span class=""yellowfg""> 9:59 AM PDT</span>&nbsp;Between 7:27 AM and 9:56 AM PDT we experienced increased deployment latencies in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",codedeploy-eu-west-1,2018-08-20 16:27:57,CodeDeploy,Ireland,7:27 AM,9:56 AM,PDT,149.0,2018,8,20,2018-08-20,increased deployment latencies," 9:28 AM PDT -  We are investigating increased deployment latencies in the EU-WEST-1 Region.
 9:59 AM PDT -  Between 7:27 AM and 9:56 AM PDT we experienced increased deployment latencies in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally."
46,AWS Single Sign-On (N. Virginia),[RESOLVED] Increased Error Rates and Latencies,1534799678,1,,"<div><span class=""yellowfg""> 2:15 PM PDT</span>&nbsp;We can confirm increased error rates and latencies for the SSO service in the US-EAST-1 Region and continue to work towards resolution.</div><div><span class=""yellowfg""> 2:39 PM PDT</span>&nbsp;Between 10:30 AM and 2:18 PM PDT we experienced increased error rates and latencies for the SSO service in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",sso-us-east-1,2018-08-20 21:14:38,Single Sign-On,N. Virginia,10:30 AM,2:18 PM,PDT,228.0,2018,8,20,2018-08-20,increased error rates and latencies," 2:15 PM PDT -  We can confirm increased error rates and latencies for the SSO service in the US-EAST-1 Region and continue to work towards resolution.
 2:39 PM PDT -  Between 10:30 AM and 2:18 PM PDT we experienced increased error rates and latencies for the SSO service in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
47,Amazon Connect (N. Virginia),[RESOLVED] Call quality degradation,1534866429,1,,"<div><span class=""yellowfg""> 8:47 AM PDT</span>&nbsp;We are investigating degraded call quality in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 9:21 AM PDT</span>&nbsp;Between 7:10 AM and 8:55 AM PDT, we experienced degraded call quality in the US-EAST-1 Region. Agents and end-customers may have experienced increased call latency or static on calls during this time. The issue has been resolved and the service is operating normally. </div>",connect-us-east-1,2018-08-21 15:47:09,Connect,N. Virginia,7:10 AM,8:55 AM,PDT,105.0,2018,8,21,2018-08-21,call quality degradation," 8:47 AM PDT -  We are investigating degraded call quality in the US-EAST-1 Region.
 9:21 AM PDT -  Between 7:10 AM and 8:55 AM PDT, we experienced degraded call quality in the US-EAST-1 Region. Agents and end-customers may have experienced increased call latency or static on calls during this time. The issue has been resolved and the service is operating normally. "
48,Amazon Cognito (Oregon),[RESOLVED] Increased API Error Rates,1535399349,1,,"<div><span class=""yellowfg"">12:49 PM PDT</span>&nbsp;We are investigating increased error rates for API calls for Cognito User Pools in the US-WEST-2 Region.</div><div><span class=""yellowfg""> 1:16 PM PDT</span>&nbsp;We can confirm increased error rates for API calls for Cognito User Pools in the US-WEST-2 Region and continue to work towards resolution.</div><div><span class=""yellowfg""> 1:34 PM PDT</span>&nbsp;Between 11:33 AM and 1:25 PM PDT we experienced increased error rates for API calls for Cognito User Pools in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",cognito-us-west-2,2018-08-27 19:49:09,Cognito,Oregon,11:33 AM,1:25 PM,PDT,112.0,2018,8,27,2018-08-27,increased api error rates,"12:49 PM PDT -  We are investigating increased error rates for API calls for Cognito User Pools in the US-WEST-2 Region.
 1:16 PM PDT -  We can confirm increased error rates for API calls for Cognito User Pools in the US-WEST-2 Region and continue to work towards resolution.
 1:34 PM PDT -  Between 11:33 AM and 1:25 PM PDT we experienced increased error rates for API calls for Cognito User Pools in the US-WEST-2 Region. The issue has been resolved and the service is operating normally."
49,Amazon Simple Queue Service (N. Virginia),[RESOLVED] Elevated error rates for SQS,1536163992,1,,"<div><span class=""yellowfg""> 9:13 AM PDT</span>&nbsp;We are currently investigating elevated error rates for SQS in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 9:30 AM PDT</span>&nbsp;Between 7:38 AM and 9:18 AM PDT, we experienced elevated API error rates and connection failures in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",sqs-us-east-1,2018-09-05 16:13:12,Simple Queue Service,N. Virginia,7:38 AM,9:18 AM,PDT,100.0,2018,9,5,2018-09-05,elevated error rates for sqs," 9:13 AM PDT -  We are currently investigating elevated error rates for SQS in the US-EAST-1 Region.
 9:30 AM PDT -  Between 7:38 AM and 9:18 AM PDT, we experienced elevated API error rates and connection failures in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. "
50,Amazon WorkMail (N. Virginia),[RESOLVED] Degraded WorkMail performance,1536725279,1,,"<div><span class=""yellowfg""> 9:08 PM PDT</span>&nbsp;We are investigating degraded WorkMail service performance in the US-EAST-1 Region, impacting IMAP client usability and synchronization of messages.
There is no impact to Outlook, EWS client applications or mobile clients.</div><div><span class=""yellowfg""> 9:40 PM PDT</span>&nbsp;We have identified the cause of degraded WorkMail service performance in the US-EAST-1 Region, impacting IMAP client usability and synchronization of messages and continue working towards resolution. There is no impact to Outlook, EWS client applications or mobile clients.</div><div><span class=""yellowfg""> 9:59 PM PDT</span>&nbsp;Between 7:10 PM and 9:46 PM PDT we experienced degraded WorkMail service performance in the US-EAST-1 Region, impacting IMAP client usability and synchronization of messages. The issue has been resolved and the service is operating normally.</div>",workmail-us-east-1,2018-09-12 04:07:59,WorkMail,N. Virginia,7:10 PM,9:46 PM,PDT,156.0,2018,9,12,2018-09-12,degraded workmail performance," 9:08 PM PDT -  We are investigating degraded WorkMail service performance in the US-EAST-1 Region, impacting IMAP client usability and synchronization of messages.There is no impact to Outlook, EWS client applications or mobile clients.
 9:40 PM PDT -  We have identified the cause of degraded WorkMail service performance in the US-EAST-1 Region, impacting IMAP client usability and synchronization of messages and continue working towards resolution. There is no impact to Outlook, EWS client applications or mobile clients.
 9:59 PM PDT -  Between 7:10 PM and 9:46 PM PDT we experienced degraded WorkMail service performance in the US-EAST-1 Region, impacting IMAP client usability and synchronization of messages. The issue has been resolved and the service is operating normally."
51,Amazon Elastic Compute Cloud (Ireland),[RESOLVED] Network Connectivity,1537949151,1,,"<div><span class=""yellowfg""> 1:06 AM PDT</span>&nbsp;We are investigating increased error rates for new launches in the EU-WEST-1 Region.

</div><div><span class=""yellowfg""> 1:37 AM PDT</span>&nbsp;We can confirm increased API error rates and connectivity issues for some instances in a single Availability Zone in the EU-WEST-1 Region.</div><div><span class=""yellowfg""> 2:14 AM PDT</span>&nbsp;We continue to investigate connectivity issues from some instances to some AWS services in the EU-WEST-1 Region. We have identified the root cause and are taking steps to resolve the issue.</div><div><span class=""yellowfg""> 3:03 AM PDT</span>&nbsp;We have resolved the connectivity issues from EC2 instances to the affected AWS services in the EU-WEST-1 Region. We continue to see elevated error rates for the RunInstances EC2 API, which we are working to resolve. Internet connectivity and connectivity between EC2 instances remain unaffected.</div><div><span class=""yellowfg""> 3:28 AM PDT</span>&nbsp;Starting at 12:15 AM PDT we experienced increased API error rates for the EC2 API, and connectivity issues between EC2 instances and AWS services in the EU-WEST-1 Region. At 2:29 AM PDT, the connectivity issues between EC2 instances and AWS services were resolved. At 2:59 AM PDT, the increased API error rates for the EC2 APIs were fully resolved. Internet connectivity and connectivity between EC2 instances was not affected. The issue has been resolved and the service is operating normally.</div>",ec2-eu-west-1,2018-09-26 08:05:51,Elastic Compute Cloud,Ireland,2:29 AM,2:59 AM,PDT,30.0,2018,9,26,2018-09-26,network connectivity," 1:06 AM PDT -  We are investigating increased error rates for new launches in the EU-WEST-1 Region.
 1:37 AM PDT -  We can confirm increased API error rates and connectivity issues for some instances in a single Availability Zone in the EU-WEST-1 Region.
 2:14 AM PDT -  We continue to investigate connectivity issues from some instances to some AWS services in the EU-WEST-1 Region. We have identified the root cause and are taking steps to resolve the issue.
 3:03 AM PDT -  We have resolved the connectivity issues from EC2 instances to the affected AWS services in the EU-WEST-1 Region. We continue to see elevated error rates for the RunInstances EC2 API, which we are working to resolve. Internet connectivity and connectivity between EC2 instances remain unaffected.
 3:28 AM PDT -  Starting at 12:15 AM PDT we experienced increased API error rates for the EC2 API, and connectivity issues between EC2 instances and AWS services in the EU-WEST-1 Region. At 2:29 AM PDT, the connectivity issues between EC2 instances and AWS services were resolved. At 2:59 AM PDT, the increased API error rates for the EC2 APIs were fully resolved. Internet connectivity and connectivity between EC2 instances was not affected. The issue has been resolved and the service is operating normally."
52,AWS CloudHSM (Ireland),[RESOLVED] Connectivity issues,1537956232,1,,"<div><span class=""yellowfg""> 3:11 AM PDT</span>&nbsp;We are continuing to investigate connectivity issues impacting CloudHSM and CloudHSM Classic HSMs in a single Availability Zone in EU-WEST-1 Region.</div><div><span class=""yellowfg""> 3:44 AM PDT</span>&nbsp;We can confirm that current generation CloudHSM instances are operating normally. CloudHSM Classic instances in a single Availability Zone in the EU-WEST-1 Region are experiencing network connectivity errors. As we work to recover the remaining CloudHSM Classic instances, we will continue to provide updates via the Personal Health Dashboard.</div>",cloudhsm-eu-west-1,2018-09-26 10:03:52,CloudHSM,Ireland,3:11 AM,3:44 AM,PDT,33.0,2018,9,26,2018-09-26,connectivity issues," 3:11 AM PDT -  We are continuing to investigate connectivity issues impacting CloudHSM and CloudHSM Classic HSMs in a single Availability Zone in EU-WEST-1 Region.
 3:44 AM PDT -  We can confirm that current generation CloudHSM instances are operating normally. CloudHSM Classic instances in a single Availability Zone in the EU-WEST-1 Region are experiencing network connectivity errors. As we work to recover the remaining CloudHSM Classic instances, we will continue to provide updates via the Personal Health Dashboard."
53,Amazon Elastic File System (Ireland),[RESOLVED] Network Connectivity,1537958697,0,,"<div><span class=""yellowfg""> 4:13 AM PDT</span>&nbsp;Between 12:15 AM and 2:12 AM PDT, we experienced increased error rates for console requests in EU-WEST-1 Region. The issue has been resolved and the service is now operating normally.</div>",elasticfilesystem-eu-west-1,2018-09-26 10:44:57,Elastic File System,Ireland,12:15 AM,2:12 AM,PDT,117.0,2018,9,26,2018-09-26,network connectivity," 4:13 AM PDT -  Between 12:15 AM and 2:12 AM PDT, we experienced increased error rates for console requests in EU-WEST-1 Region. The issue has been resolved and the service is now operating normally."
54,AWS Direct Connect (Ireland),[RESOLVED] Network Connectivity,1537960452,0,,"<div><span class=""yellowfg""> 4:19 AM PDT</span>&nbsp;Between 12:15 AM and 2:29 AM PDT, we experienced increased packet loss, impacting AWS Direct Connect connectivity for some customers in the EU-WEST-1 Region. The issue has been resolved and the service is now operating normally.</div>",directconnect-eu-west-1,2018-09-26 11:14:12,Direct Connect,Ireland,12:15 AM,2:29 AM,PDT,134.0,2018,9,26,2018-09-26,network connectivity," 4:19 AM PDT -  Between 12:15 AM and 2:29 AM PDT, we experienced increased packet loss, impacting AWS Direct Connect connectivity for some customers in the EU-WEST-1 Region. The issue has been resolved and the service is now operating normally."
55,Amazon Redshift (N. Virginia),[RESOLVED] Increased API Error Rates,1538097782,1,,"<div><span class=""yellowfg""> 6:23 PM PDT</span>&nbsp;Between 5:45 PM and 6:10 PM PDT we experienced increased API error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",redshift-us-east-1,2018-09-28 01:23:02,Redshift,N. Virginia,5:45 PM,6:10 PM,PDT,25.0,2018,9,28,2018-09-28,increased api error rates, 6:23 PM PDT -  Between 5:45 PM and 6:10 PM PDT we experienced increased API error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.
56,AWS Internet Connectivity (Frankfurt),[RESOLVED] Network connectivity,1538413958,1,,"<div><span class=""yellowfg"">10:12 AM PDT</span>&nbsp;We are investigating intermittent connectivity issues between the EU-CENTRAL-1 Region and other AWS Regions.</div><div><span class=""yellowfg"">10:24 AM PDT</span>&nbsp;We can confirm intermittent issues for Internet and Inter-region network connectivity within the EU-CENTRAL-1 Region. We have mitigated the impact but continue to work towards full resolution.</div><div><span class=""yellowfg"">10:59 AM PDT</span>&nbsp;Between 9:21 AM and 9:31 AM PDT and between 9:41 AM and 9:51 AM PDT we experienced Internet connectivity issues for the EU-CENTRAL-1 Region and Inter-region connectivity issues between the EU-CENTRAL-1 Region and other AWS regions. The issue has been resolved and the service is operating normally.</div>",internetconnectivity-eu-central-1,2018-10-01 17:12:38,Internet Connectivity,Frankfurt,9:41 AM,9:51 AM,PDT,10.0,2018,10,1,2018-10-01,network connectivity,"10:12 AM PDT -  We are investigating intermittent connectivity issues between the EU-CENTRAL-1 Region and other AWS Regions.
10:24 AM PDT -  We can confirm intermittent issues for Internet and Inter-region network connectivity within the EU-CENTRAL-1 Region. We have mitigated the impact but continue to work towards full resolution.
10:59 AM PDT -  Between 9:21 AM and 9:31 AM PDT and between 9:41 AM and 9:51 AM PDT we experienced Internet connectivity issues for the EU-CENTRAL-1 Region and Inter-region connectivity issues between the EU-CENTRAL-1 Region and other AWS regions. The issue has been resolved and the service is operating normally."
57,Amazon Elastic Compute Cloud (Oregon),[RESOLVED] Increased API Errors ,1538424731,1,,"<div><span class=""yellowfg""> 1:12 PM PDT</span>&nbsp;We are investigating increased error rates and latencies for the EC2 APIs in the US-WEST-2 Region.</div><div><span class=""yellowfg""> 2:06 PM PDT</span>&nbsp;We continue to investigate increased error rates and latencies for the EC2 APIs and EC2 Management Console in the US-WEST-2 Region. Existing EC2 instances are not affected.</div><div><span class=""yellowfg""> 3:14 PM PDT</span>&nbsp;We are seeing recovery for the increased error rates and latencies affecting the EC2 APIs in the US-WEST-2 Region. Some requests may return a ""request limit exceeded"" error as we work toward full recovery. We recommend retrying failed requests.</div><div><span class=""yellowfg""> 4:41 PM PDT</span>&nbsp;Error rates and latencies for the EC2 APIs in the US-WEST-2 Region remain at normal levels. A small percentage of requests are still returning a ""request limit exceeded"" error as we work toward full recovery. We recommend retrying failed requests.</div><div><span class=""yellowfg""> 5:33 PM PDT</span>&nbsp;Starting at 11:20 AM PDT, we experienced periods of increased error rates and latencies for the EC2 APIs in the US-WEST-2 Region. At 2:02 PM PDT, error rates and latencies recovered for the majority of the affected APIs. At 2:55 PM PDT error rates and latencies returned to normal levels but some customers continued to experience ""request limit exceeded"" errors. At 5:10 PM PDT, all error rates, including ""request limit exceeded"" errors, returned to normal levels. The issue has been resolved and the service is operating normally.</div>",ec2-us-west-2,2018-10-01 20:12:11,Elastic Compute Cloud,Oregon,11:20 AM,5:10 PM,PDT,350.0,2018,10,1,2018-10-01,increased api errors," 1:12 PM PDT -  We are investigating increased error rates and latencies for the EC2 APIs in the US-WEST-2 Region.
 2:06 PM PDT -  We continue to investigate increased error rates and latencies for the EC2 APIs and EC2 Management Console in the US-WEST-2 Region. Existing EC2 instances are not affected.
 3:14 PM PDT -  We are seeing recovery for the increased error rates and latencies affecting the EC2 APIs in the US-WEST-2 Region. Some requests may return a ""request limit exceeded"" error as we work toward full recovery. We recommend retrying failed requests.
 4:41 PM PDT -  Error rates and latencies for the EC2 APIs in the US-WEST-2 Region remain at normal levels. A small percentage of requests are still returning a ""request limit exceeded"" error as we work toward full recovery. We recommend retrying failed requests.
 5:33 PM PDT -  Starting at 11:20 AM PDT, we experienced periods of increased error rates and latencies for the EC2 APIs in the US-WEST-2 Region. At 2:02 PM PDT, error rates and latencies recovered for the majority of the affected APIs. At 2:55 PM PDT error rates and latencies returned to normal levels but some customers continued to experience ""request limit exceeded"" errors. At 5:10 PM PDT, all error rates, including ""request limit exceeded"" errors, returned to normal levels. The issue has been resolved and the service is operating normally."
58,Auto Scaling (Oregon),[RESOLVED] Increased Launch Latencies ,1538428181,1,,"<div><span class=""yellowfg""> 2:09 PM PDT</span>&nbsp;We are investigating increased launch times for EC2 instances managed by Auto Scaling in the US-WEST-2 Region.</div><div><span class=""yellowfg""> 3:29 PM PDT</span>&nbsp;Between 12:32 PM PDT and 2:40 PM PDT, we experienced elevated launch times for EC2 instance managed by Auto Scaling in the US-WEST-2 Region. The issue has been resolved and the service is operating normally. </div>",autoscaling-us-west-2,2018-10-01 21:09:41,Auto Scaling,Oregon,12:32 PM,2:40 PM,PDT,128.0,2018,10,1,2018-10-01,increased launch latencies," 2:09 PM PDT -  We are investigating increased launch times for EC2 instances managed by Auto Scaling in the US-WEST-2 Region.
 3:29 PM PDT -  Between 12:32 PM PDT and 2:40 PM PDT, we experienced elevated launch times for EC2 instance managed by Auto Scaling in the US-WEST-2 Region. The issue has been resolved and the service is operating normally. "
59,AWS Lambda (Oregon),[RESOLVED] Increased API Error Rates,1538428351,1,,"<div><span class=""yellowfg""> 2:12 PM PDT</span>&nbsp;We are investigating increased error rates for newly created functions in the US-WEST-2 Region. This includes functions created via the console as well as those created via the API or CLI.</div><div><span class=""yellowfg""> 3:51 PM PDT</span>&nbsp;We continue to investigate increased error rates for newly created VPC functions in the US-WEST-2 Region. This includes VPC functions created via the console as well as those created via the API or CLI. The issue has been resolved for non-VPC functions. </div><div><span class=""yellowfg""> 5:14 PM PDT</span>&nbsp;Between 12:47 PM PDT and 4:55 PM PDT, we experienced increased error rates for newly created functions in US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",lambda-us-west-2,2018-10-01 21:12:31,Lambda,Oregon,12:47 PM,4:55 PM,PDT,248.0,2018,10,1,2018-10-01,increased api error rates," 2:12 PM PDT -  We are investigating increased error rates for newly created functions in the US-WEST-2 Region. This includes functions created via the console as well as those created via the API or CLI.
 3:51 PM PDT -  We continue to investigate increased error rates for newly created VPC functions in the US-WEST-2 Region. This includes VPC functions created via the console as well as those created via the API or CLI. The issue has been resolved for non-VPC functions. 
 5:14 PM PDT -  Between 12:47 PM PDT and 4:55 PM PDT, we experienced increased error rates for newly created functions in US-WEST-2 Region. The issue has been resolved and the service is operating normally."
60,Amazon Elastic Load Balancing (Oregon),[RESOLVED] Increased Provisioning Latencies ,1538428470,1,,"<div><span class=""yellowfg""> 2:14 PM PDT</span>&nbsp;We are investigating increased provisioning times for load balancers in the US-WEST-2 Region. Connectivity to existing load balancers is not affected.</div><div><span class=""yellowfg""> 3:49 PM PDT</span>&nbsp;We are seeing an improvement in the increased provisioning times affecting load balancers in the US-WEST-2 Region. We continue to work towards full recovery.</div><div><span class=""yellowfg""> 5:25 PM PDT</span>&nbsp;Between 1:00 PM and 4:48 PM PDT, we experienced increased provisioning times for load balancers in the US-WEST-2 Region. Connectivity to load balancers was not affected. The issue has been resolved and the service is operating normally.</div>",elb-us-west-2,2018-10-01 21:14:30,Elastic Load Balancing,Oregon,1:00 PM,4:48 PM,PDT,228.0,2018,10,1,2018-10-01,increased provisioning latencies," 2:14 PM PDT -  We are investigating increased provisioning times for load balancers in the US-WEST-2 Region. Connectivity to existing load balancers is not affected.
 3:49 PM PDT -  We are seeing an improvement in the increased provisioning times affecting load balancers in the US-WEST-2 Region. We continue to work towards full recovery.
 5:25 PM PDT -  Between 1:00 PM and 4:48 PM PDT, we experienced increased provisioning times for load balancers in the US-WEST-2 Region. Connectivity to load balancers was not affected. The issue has been resolved and the service is operating normally."
61,Amazon Elastic MapReduce (Oregon),[RESOLVED] Delays in starting clusters,1538429178,1,,"<div><span class=""yellowfg""> 2:26 PM PDT</span>&nbsp;We are investigating delays in starting clusters in the US-WEST-2 Region. Existing clusters are not affected.</div><div><span class=""yellowfg""> 3:53 PM PDT</span>&nbsp;We continue to investigate delays in starting clusters in the US-WEST-2 Region. Existing clusters are not affected. </div><div><span class=""yellowfg""> 5:11 PM PDT</span>&nbsp;Between 1:10 PM and 4:20 PM PDT we experienced delays in starting clusters in US-West-2 Region. Existing clusters were not affected. The issue has been resolved and the service is operating normally.</div><div><span class=""yellowfg""> 5:17 PM PDT</span>&nbsp;Between 1:10 PM and 4:20 PM PDT we experienced delays in starting clusters in US-WEST-2 Region. Existing clusters were not affected. The issue has been resolved and the service is operating normally.</div>",emr-us-west-2,2018-10-01 21:26:18,Elastic MapReduce,Oregon,1:10 PM,4:20 PM,PDT,190.0,2018,10,1,2018-10-01,delays in starting clusters," 2:26 PM PDT -  We are investigating delays in starting clusters in the US-WEST-2 Region. Existing clusters are not affected.
 3:53 PM PDT -  We continue to investigate delays in starting clusters in the US-WEST-2 Region. Existing clusters are not affected. 
 5:11 PM PDT -  Between 1:10 PM and 4:20 PM PDT we experienced delays in starting clusters in US-West-2 Region. Existing clusters were not affected. The issue has been resolved and the service is operating normally.
 5:17 PM PDT -  Between 1:10 PM and 4:20 PM PDT we experienced delays in starting clusters in US-WEST-2 Region. Existing clusters were not affected. The issue has been resolved and the service is operating normally."
62,Amazon WorkSpaces (Oregon),[RESOLVED] Increased Provisioning Error Rates,1538430961,1,,"<div><span class=""yellowfg""> 2:56 PM PDT</span>&nbsp;We are investigating increased error rates and latencies for provisioning new WorkSpaces in the US-WEST-2 Region</div><div><span class=""yellowfg""> 3:21 PM PDT</span>&nbsp;Between 1:00 PM and 3:00 PM PDT, we experienced increased error rates for new WorkSpaces provisioning in the US-WEST-2 Region. Existing WorkSpaces were unaffected. The issue has been resolved and the service is operating normally. </div>",workspaces-us-west-2,2018-10-01 21:56:01,WorkSpaces,Oregon,1:00 PM,3:00 PM,PDT,120.0,2018,10,1,2018-10-01,increased provisioning error rates," 2:56 PM PDT -  We are investigating increased error rates and latencies for provisioning new WorkSpaces in the US-WEST-2 Region
 3:21 PM PDT -  Between 1:00 PM and 3:00 PM PDT, we experienced increased error rates for new WorkSpaces provisioning in the US-WEST-2 Region. Existing WorkSpaces were unaffected. The issue has been resolved and the service is operating normally. "
63,Amazon Elastic Compute Cloud (Oregon),[RESOLVED] Increased API Errors,1538626579,1,,"<div><span class=""yellowfg""> 9:16 PM PDT</span>&nbsp;We're investigating increased error rates and latencies for the EC2 APIs in the US-WEST-2 Region.</div><div><span class=""yellowfg""> 9:45 PM PDT</span>&nbsp;Between 8:48 PM and 9:25 PM PDT we experienced increased error rates and latencies in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",ec2-us-west-2,2018-10-04 04:16:19,Elastic Compute Cloud,Oregon,8:48 PM,9:25 PM,PDT,37.0,2018,10,4,2018-10-04,increased api errors," 9:16 PM PDT -  We're investigating increased error rates and latencies for the EC2 APIs in the US-WEST-2 Region.
 9:45 PM PDT -  Between 8:48 PM and 9:25 PM PDT we experienced increased error rates and latencies in the US-WEST-2 Region. The issue has been resolved and the service is operating normally."
64,AWS Secrets Manager (Oregon),[RESOLVED] Elevated latencies and API Error Rates,1538645129,1,,"<div><span class=""yellowfg""> 2:25 AM PDT</span>&nbsp;We are investigating elevated latencies and API error rates in the US-WEST-2 Region.</div><div><span class=""yellowfg""> 3:09 AM PDT</span>&nbsp;We continue to investigate elevated latencies and API error rates in the US-WEST-2 Region.</div><div><span class=""yellowfg""> 8:04 AM PDT</span>&nbsp;Between 2:04 AM and 7:50 AM PDT we experienced increased API error rates and latencies in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",secretsmanager-us-west-2,2018-10-04 09:25:29,Secrets Manager,Oregon,2:04 AM,7:50 AM,PDT,346.0,2018,10,4,2018-10-04,elevated latencies and api error rates," 2:25 AM PDT -  We are investigating elevated latencies and API error rates in the US-WEST-2 Region.
 3:09 AM PDT -  We continue to investigate elevated latencies and API error rates in the US-WEST-2 Region.
 8:04 AM PDT -  Between 2:04 AM and 7:50 AM PDT we experienced increased API error rates and latencies in the US-WEST-2 Region. The issue has been resolved and the service is operating normally."
65,Amazon Simple Storage Service (Ohio),[RESOLVED] Increased Error Rates,1538766090,2,,"<div><span class=""yellowfg"">12:01 PM PDT</span>&nbsp;We are investigating increased error rates for Amazon S3 requests in the US-EAST-2 Region.</div><div><span class=""yellowfg"">12:16 PM PDT</span>&nbsp;We have identified the root cause of the increased error rates in the US-EAST-2 Region and are working towards resolution.</div><div><span class=""yellowfg"">12:42 PM PDT</span>&nbsp;We have identified the root cause and are seeing some recovery. We are continuing to monitor and work towards full resolution.</div><div><span class=""yellowfg"">12:51 PM PDT</span>&nbsp;Between 11:47 AM and 12:33 PM PDT we experienced increased error rates for Amazon S3 requests in the US-EAST-2 Region. The issue has been resolved and the service is operating normally.</div>",s3-us-east-2,2018-10-05 19:01:30,Simple Storage Service,Ohio,11:47 AM,12:33 PM,PDT,46.0,2018,10,5,2018-10-05,increased error rates,"12:01 PM PDT -  We are investigating increased error rates for Amazon S3 requests in the US-EAST-2 Region.
12:16 PM PDT -  We have identified the root cause of the increased error rates in the US-EAST-2 Region and are working towards resolution.
12:42 PM PDT -  We have identified the root cause and are seeing some recovery. We are continuing to monitor and work towards full resolution.
12:51 PM PDT -  Between 11:47 AM and 12:33 PM PDT we experienced increased error rates for Amazon S3 requests in the US-EAST-2 Region. The issue has been resolved and the service is operating normally."
66,Amazon Glacier (Ohio),[RESOLVED] Increased Error Rates,1538768132,1,,"<div><span class=""yellowfg"">12:35 PM PDT</span>&nbsp;We are investigating increased error rates and latency in the US-EAST-2 Region. </div><div><span class=""yellowfg"">12:48 PM PDT</span>&nbsp;Between 11:47 AM PDT and 12:33 PM PDT we experienced increased API error rates and latencies in the US-EAST-2 Region. The issue has been resolved and the service is operating normally. </div>",glacier-us-east-2,2018-10-05 19:35:32,Glacier,Ohio,11:47 AM,12:33 PM,PDT,46.0,2018,10,5,2018-10-05,increased error rates,"12:35 PM PDT -  We are investigating increased error rates and latency in the US-EAST-2 Region. 
12:48 PM PDT -  Between 11:47 AM PDT and 12:33 PM PDT we experienced increased API error rates and latencies in the US-EAST-2 Region. The issue has been resolved and the service is operating normally. "
67,Amazon Kinesis Firehose (Ohio),[RESOLVED] Delayed Data Delivery,1538768432,1,,"<div><span class=""yellowfg"">12:40 PM PDT</span>&nbsp;We are investigating delays in S3 delivery for Kinesis Firehose in the US-EAST-2 Region.</div><div><span class=""yellowfg""> 1:01 PM PDT</span>&nbsp;Between 11:47 AM and 12:38 PM PDT we experienced increased Firehose to S3 delivery delays in the US-EAST-2 Region. The issue has been resolved and the service is operating normally. </div>",firehose-us-east-2,2018-10-05 19:40:32,Kinesis Firehose,Ohio,11:47 AM,12:38 PM,PDT,51.0,2018,10,5,2018-10-05,delayed data delivery,"12:40 PM PDT -  We are investigating delays in S3 delivery for Kinesis Firehose in the US-EAST-2 Region.
 1:01 PM PDT -  Between 11:47 AM and 12:38 PM PDT we experienced increased Firehose to S3 delivery delays in the US-EAST-2 Region. The issue has been resolved and the service is operating normally. "
68,Amazon Elastic Compute Cloud (Ohio),[RESOLVED] Degraded Volume Performance for EBS Volumes / Insufficient-Data for EC2 Instance Status Checks,1538769418,1,,"<div><span class=""yellowfg"">12:57 PM PDT</span>&nbsp;Between 11:47 AM and 12:33 PM PDT we experienced degraded performance for some EBS volumes in the US-EAST-2 Region. During this time, some customers also experienced insufficient-data in the results from EC2 instance status checks. CloudWatch alarms may have transitioned into ""INSUFFICIENT_DATA"" state if set on the delayed metrics. EC2 instances operated normally during this time; only EC2 CloudWatch metrics were affected. The issue has been resolved and the service is operating normally.</div>",ec2-us-east-2,2018-10-05 19:56:58,Elastic Compute Cloud,Ohio,11:47 AM,12:33 PM,PDT,46.0,2018,10,5,2018-10-05,degraded volume performance for ebs volumes / insufficient-data for ec2 instance status checks,"12:57 PM PDT -  Between 11:47 AM and 12:33 PM PDT we experienced degraded performance for some EBS volumes in the US-EAST-2 Region. During this time, some customers also experienced insufficient-data in the results from EC2 instance status checks. CloudWatch alarms may have transitioned into ""INSUFFICIENT_DATA"" state if set on the delayed metrics. EC2 instances operated normally during this time; only EC2 CloudWatch metrics were affected. The issue has been resolved and the service is operating normally."
69,AWS CodeCommit (Ohio),[RESOLVED] Increased Error Rates,1538769625,1,,"<div><span class=""yellowfg""> 1:00 PM PDT</span>&nbsp;Between 11:47 AM PDT and 12:33 PM PDT we experienced an increased error rate for APIs and Git Push/Pull in the US-EAST-2 Region. The issue has been resolved and the service is operating normally. </div>",codecommit-us-east-2,2018-10-05 20:00:25,CodeCommit,Ohio,11:47 AM,12:33 PM,PDT,46.0,2018,10,5,2018-10-05,increased error rates, 1:00 PM PDT -  Between 11:47 AM PDT and 12:33 PM PDT we experienced an increased error rate for APIs and Git Push/Pull in the US-EAST-2 Region. The issue has been resolved and the service is operating normally. 
70,Amazon Elastic Container Service (Ohio),[RESOLVED] Increased Error Rates,1538770316,1,,"<div><span class=""yellowfg""> 1:12 PM PDT</span>&nbsp;Between 11:47 AM and 12:33 PM PDT we experienced increased error rates when launching Fargate tasks in the US-EAST-2 Region. Running tasks were not impacted. The issue has been resolved and the service is operating normally.</div>",ecs-us-east-2,2018-10-05 20:11:56,Elastic Container Service,Ohio,11:47 AM,12:33 PM,PDT,46.0,2018,10,5,2018-10-05,increased error rates, 1:12 PM PDT -  Between 11:47 AM and 12:33 PM PDT we experienced increased error rates when launching Fargate tasks in the US-EAST-2 Region. Running tasks were not impacted. The issue has been resolved and the service is operating normally.
71,Amazon Connect (N. Virginia),[RESOLVED] Increased Errors,1538776626,1,,"<div><span class=""yellowfg""> 2:57 PM PDT</span>&nbsp;We are currently investigating increased latencies and errors for the Agent Call Control Panel in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 3:08 PM PDT</span>&nbsp;Between 11:27 AM and 2:52 PM PDT, we experienced increased latencies and errors for the Agent Call Control Panel in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",connect-us-east-1,2018-10-05 21:57:06,Connect,N. Virginia,11:27 AM,2:52 PM,PDT,205.0,2018,10,5,2018-10-05,increased errors," 2:57 PM PDT -  We are currently investigating increased latencies and errors for the Agent Call Control Panel in the US-EAST-1 Region.
 3:08 PM PDT -  Between 11:27 AM and 2:52 PM PDT, we experienced increased latencies and errors for the Agent Call Control Panel in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. "
72,Amazon Elastic Compute Cloud (N. Virginia),[RESOLVED] Title: Increased Launch Errors ,1539093649,1,,"<div><span class=""yellowfg""> 7:01 AM PDT</span>&nbsp;Between 5:31 AM and 6:42 AM PDT we experienced increased error rates for instance launches in a single availability zone in the US-EAST-1 Region. Existing instances were unaffected. The issue has been resolved and the service is operating normally.</div>",ec2-us-east-1,2018-10-09 14:00:49,Elastic Compute Cloud,N. Virginia,5:31 AM,6:42 AM,PDT,71.0,2018,10,9,2018-10-09,title: increased launch errors, 7:01 AM PDT -  Between 5:31 AM and 6:42 AM PDT we experienced increased error rates for instance launches in a single availability zone in the US-EAST-1 Region. Existing instances were unaffected. The issue has been resolved and the service is operating normally.
73,Amazon Elasticsearch Service (N. California),[RESOLVED] Domain Connectivity,1539650824,1,,"<div><span class=""yellowfg""> 5:50 PM PDT</span>&nbsp;We are investigating connectivity issues for some domains in the US-WEST-1 Region.</div><div><span class=""yellowfg""> 6:35 PM PDT</span>&nbsp;Between 4:50 PM and 6:20 PM PDT we experienced domain connectivity issues in the US-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",elasticsearch-us-west-1,2018-10-16 00:47:04,Elasticsearch Service,N. California,4:50 PM,6:20 PM,PDT,90.0,2018,10,16,2018-10-16,domain connectivity," 5:50 PM PDT -  We are investigating connectivity issues for some domains in the US-WEST-1 Region.
 6:35 PM PDT -  Between 4:50 PM and 6:20 PM PDT we experienced domain connectivity issues in the US-WEST-1 Region. The issue has been resolved and the service is operating normally."
74,Amazon Elastic Compute Cloud (N. Virginia),[RESOLVED] Network Connectivity,1539804061,1,,"<div><span class=""yellowfg"">12:22 PM PDT</span>&nbsp;We are investigating network connectivity issues for some instances within the US-EAST-1 Region.</div><div><span class=""yellowfg"">12:30 PM PDT</span>&nbsp;Between 11:27 AM and 11:28 AM PDT some instances experienced connectivity issues for network traffic between Availability Zones within the US-EAST-1 Region. The issue was automatically mitigated and we continue to monitor all affected instances. </div><div><span class=""yellowfg""> 1:06 PM PDT</span>&nbsp;Between 11:27 AM and 11:28 AM PDT some instances experienced connectivity issues for network traffic between Availability Zones within the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",ec2-us-east-1,2018-10-17 19:21:01,Elastic Compute Cloud,N. Virginia,11:27 AM,11:28 AM,PDT,1.0,2018,10,17,2018-10-17,network connectivity,"12:22 PM PDT -  We are investigating network connectivity issues for some instances within the US-EAST-1 Region.
12:30 PM PDT -  Between 11:27 AM and 11:28 AM PDT some instances experienced connectivity issues for network traffic between Availability Zones within the US-EAST-1 Region. The issue was automatically mitigated and we continue to monitor all affected instances. 
 1:06 PM PDT -  Between 11:27 AM and 11:28 AM PDT some instances experienced connectivity issues for network traffic between Availability Zones within the US-EAST-1 Region. The issue has been resolved and the service is operating normally. "
75,Amazon Elastic Load Balancing (N. Virginia),[RESOLVED] Increased provisioning and registration times ,1539806007,1,,"<div><span class=""yellowfg"">12:53 PM PDT</span>&nbsp;We are investigating increased provisioning and back-end target registration times for load balancers within the US-EAST-1 Region. Connectivity to existing load balancers is not affected.</div><div><span class=""yellowfg""> 2:18 PM PDT</span>&nbsp;We continue to work on resolving the increased provisioning and back-end target registration times for Classic and Application Load Balancers within the US-EAST-1 Region. Provisioning and back-end target registration times are not affected for Network Load Balancers. Connectivity to all load balancers is not affected.</div><div><span class=""yellowfg""> 2:59 PM PDT</span>&nbsp;Beginning at 11:50 AM PDT we experienced increased provisioning times for Classic and Application Load Balancers within the US-EAST-1 Region. At 12:40 PM PDT, registration times for new back-end targets began to increase. At 2:05 PM PDT, both load balancer provisioning and back-end target registration latencies began to recover. By 2:25 PM PDT, back-end target registration times had returned to normal levels and by 2:44 PM PDT load balancer provisioning had fully recovered. Connectivity to load balancers was not affected. The issue has been resolved and the service is operating normally.</div>",elb-us-east-1,2018-10-17 19:53:27,Elastic Load Balancing,N. Virginia,11:50 AM,2:44 PM,PDT,174.0,2018,10,17,2018-10-17,increased provisioning and registration times,"12:53 PM PDT -  We are investigating increased provisioning and back-end target registration times for load balancers within the US-EAST-1 Region. Connectivity to existing load balancers is not affected.
 2:18 PM PDT -  We continue to work on resolving the increased provisioning and back-end target registration times for Classic and Application Load Balancers within the US-EAST-1 Region. Provisioning and back-end target registration times are not affected for Network Load Balancers. Connectivity to all load balancers is not affected.
 2:59 PM PDT -  Beginning at 11:50 AM PDT we experienced increased provisioning times for Classic and Application Load Balancers within the US-EAST-1 Region. At 12:40 PM PDT, registration times for new back-end targets began to increase. At 2:05 PM PDT, both load balancer provisioning and back-end target registration latencies began to recover. By 2:25 PM PDT, back-end target registration times had returned to normal levels and by 2:44 PM PDT load balancer provisioning had fully recovered. Connectivity to load balancers was not affected. The issue has been resolved and the service is operating normally."
76,Amazon Relational Database Service (N. Virginia),[RESOLVED] Network Connectivity,1539806429,1,,"<div><span class=""yellowfg""> 1:02 PM PDT</span>&nbsp;Between 11:28 AM and 11:31 AM PDT some Multi-AZ and Single-AZ RDS instances experienced network connectivity issues in the US-EAST-1 Region. All affected Multi-AZ RDS instances failed over to a second Availability Zone, which restored connectivity. The majority of the Single-AZ RDS instances have recovered and we continue to work on restoring connectivity to a small number of remaining Single-AZ RDS instances.
</div><div><span class=""yellowfg""> 1:28 PM PDT</span>&nbsp;Between 11:28 AM and 11:31 AM PDT some Multi-AZ and Single-AZ RDS instances experienced network connectivity issues in the US-EAST-1 Region. All affected Multi-AZ RDS instances failed over to a second Availability Zone, which restored connectivity. The remaining Single-AZ RDS instances have now restored connectivity. The issue has been resolved and the service is operating normally.</div>",rds-us-east-1,2018-10-17 20:00:29,Relational Database Service,N. Virginia,11:28 AM,11:31 AM,PDT,3.0,2018,10,17,2018-10-17,network connectivity," 1:02 PM PDT -  Between 11:28 AM and 11:31 AM PDT some Multi-AZ and Single-AZ RDS instances experienced network connectivity issues in the US-EAST-1 Region. All affected Multi-AZ RDS instances failed over to a second Availability Zone, which restored connectivity. The majority of the Single-AZ RDS instances have recovered and we continue to work on restoring connectivity to a small number of remaining Single-AZ RDS instances.
 1:28 PM PDT -  Between 11:28 AM and 11:31 AM PDT some Multi-AZ and Single-AZ RDS instances experienced network connectivity issues in the US-EAST-1 Region. All affected Multi-AZ RDS instances failed over to a second Availability Zone, which restored connectivity. The remaining Single-AZ RDS instances have now restored connectivity. The issue has been resolved and the service is operating normally."
77,Auto Scaling (N. Virginia),[RESOLVED] Increased Error Rates ,1539810083,1,,"<div><span class=""yellowfg""> 2:02 PM PDT</span>&nbsp;We are experiencing increased API error rates and delayed group scaling in the US-EAST-1 Region. Connectivity to existing EC2 Instances is not affected.</div><div><span class=""yellowfg""> 3:45 PM PDT</span>&nbsp;We have identified the root cause of the increased API error rates and delayed group scaling in the US-EAST-1 Region. We are beginning to see recovery and continue to work toward full resolution. </div><div><span class=""yellowfg""> 4:07 PM PDT</span>&nbsp;Between 11:28 AM and 4:05 PM PDT we experienced increased API error rates and delayed group scaling in the US-EAST-1 Region. Connectivity to existing EC2 Instances was not affected. The issue has been resolved and the service is operating normally. </div>",autoscaling-us-east-1,2018-10-17 21:01:23,Auto Scaling,N. Virginia,11:28 AM,4:05 PM,PDT,277.0,2018,10,17,2018-10-17,increased error rates," 2:02 PM PDT -  We are experiencing increased API error rates and delayed group scaling in the US-EAST-1 Region. Connectivity to existing EC2 Instances is not affected.
 3:45 PM PDT -  We have identified the root cause of the increased API error rates and delayed group scaling in the US-EAST-1 Region. We are beginning to see recovery and continue to work toward full resolution. 
 4:07 PM PDT -  Between 11:28 AM and 4:05 PM PDT we experienced increased API error rates and delayed group scaling in the US-EAST-1 Region. Connectivity to existing EC2 Instances was not affected. The issue has been resolved and the service is operating normally. "
78,Amazon Elastic Compute Cloud (N. California),[RESOLVED] Increased API Error Rates and Latencies,1539971754,1,,"<div><span class=""yellowfg"">10:56 AM PDT</span>&nbsp;We are investigating increased API error rates and latencies in the US-WEST-1 Region. </div><div><span class=""yellowfg"">11:27 AM PDT</span>&nbsp;Between 10:22 AM and 10:33 AM PDT, and between 10:47 AM and 11:06 AM PDT, we experienced increased API error rates and latencies in the US-WEST-1 Region. APIs also returned insufficient-data in the results from EC2 instance status checks and CloudWatch alarms transitioned into ""INSUFFICIENT_DATA"" state if set on the delayed metrics. EC2 instances operated normally during this time. The issue has been resolved and the service is operating normally. </div>",ec2-us-west-1,2018-10-19 17:55:54,Elastic Compute Cloud,N. California,10:47 AM,11:06 AM,PDT,19.0,2018,10,19,2018-10-19,increased api error rates and latencies,"10:56 AM PDT -  We are investigating increased API error rates and latencies in the US-WEST-1 Region. 
11:27 AM PDT -  Between 10:22 AM and 10:33 AM PDT, and between 10:47 AM and 11:06 AM PDT, we experienced increased API error rates and latencies in the US-WEST-1 Region. APIs also returned insufficient-data in the results from EC2 instance status checks and CloudWatch alarms transitioned into ""INSUFFICIENT_DATA"" state if set on the delayed metrics. EC2 instances operated normally during this time. The issue has been resolved and the service is operating normally. "
79,Amazon Simple Storage Service (N. California),[RESOLVED] Increased Error Rates,1539972265,1,,"<div><span class=""yellowfg"">11:04 AM PDT</span>&nbsp;We are investigating increased error rates for Amazon S3 requests in the US-WEST-1 Region.</div><div><span class=""yellowfg"">11:19 AM PDT</span>&nbsp;Between 10:22 AM and 10:33 AM PDT, and between 10:47 AM and 11:06 AM PDT, we experienced elevated failure rates for some Amazon S3 requests in the US-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",s3-us-west-1,2018-10-19 18:04:25,Simple Storage Service,N. California,10:47 AM,11:06 AM,PDT,19.0,2018,10,19,2018-10-19,increased error rates,"11:04 AM PDT -  We are investigating increased error rates for Amazon S3 requests in the US-WEST-1 Region.
11:19 AM PDT -  Between 10:22 AM and 10:33 AM PDT, and between 10:47 AM and 11:06 AM PDT, we experienced elevated failure rates for some Amazon S3 requests in the US-WEST-1 Region. The issue has been resolved and the service is operating normally."
80,Amazon DynamoDB (N. California),[RESOLVED] Elevated Failure Rates,1539973333,1,,"<div><span class=""yellowfg"">11:22 AM PDT</span>&nbsp;Between 10:22 AM and 10:33 AM PDT, and between 10:47 AM and 11:06 AM PDT, we experienced elevated failure rates for some Amazon DynamoDB requests in the US-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",dynamodb-us-west-1,2018-10-19 18:22:13,DynamoDB,N. California,10:47 AM,11:06 AM,PDT,19.0,2018,10,19,2018-10-19,elevated failure rates,"11:22 AM PDT -  Between 10:22 AM and 10:33 AM PDT, and between 10:47 AM and 11:06 AM PDT, we experienced elevated failure rates for some Amazon DynamoDB requests in the US-WEST-1 Region. The issue has been resolved and the service is operating normally."
81,Amazon Connect (N. Virginia),[RESOLVED] Degraded Metrics Performance,1540316843,1,,"<div><span class=""yellowfg"">10:47 AM PDT</span>&nbsp;We are investigating delays in forming contact trace records, and degraded service for the metrics and call recording UI in the US-EAST-1 Region. Call handling is not affected.</div><div><span class=""yellowfg"">11:16 AM PDT</span>&nbsp;We continue to investigate delays in forming contact trace records, and degraded performance of the metrics and call recording search experience in the US-EAST-1 region. Call handling is not affected.</div><div><span class=""yellowfg"">12:11 PM PDT</span>&nbsp;Between 9:50 AM and 11:40 AM PDT, we experienced delays in forming contact trace records, and degraded performance of the metrics and call recording search experience in the US-EAST-1 Region. No metrics or call recordings were lost, and call handling was not affected. The issue has been resolved and the service is operating normally. </div>",connect-us-east-1,2018-10-23 17:47:23,Connect,N. Virginia,9:50 AM,11:40 AM,PDT,110.0,2018,10,23,2018-10-23,degraded metrics performance,"10:47 AM PDT -  We are investigating delays in forming contact trace records, and degraded service for the metrics and call recording UI in the US-EAST-1 Region. Call handling is not affected.
11:16 AM PDT -  We continue to investigate delays in forming contact trace records, and degraded performance of the metrics and call recording search experience in the US-EAST-1 region. Call handling is not affected.
12:11 PM PDT -  Between 9:50 AM and 11:40 AM PDT, we experienced delays in forming contact trace records, and degraded performance of the metrics and call recording search experience in the US-EAST-1 Region. No metrics or call recordings were lost, and call handling was not affected. The issue has been resolved and the service is operating normally. "
82,Amazon WorkSpaces (Ireland),[RESOLVED] Connection errors,1540431788,1,,"<div><span class=""yellowfg""> 6:43 PM PDT</span>&nbsp;We are investigating increased error rates when connecting to WorkSpaces in the EU-WEST-1 Region.</div><div><span class=""yellowfg""> 7:18 PM PDT</span>&nbsp;We can confirm increased error rates when connecting to WorkSpaces in the EU-WEST-1 Region. Connections using WorkSpaces Web Access are not affected by this issue.</div><div><span class=""yellowfg""> 7:57 PM PDT</span>&nbsp;We have identified the cause of increased error rates when connecting to some WorkSpaces in the EU-WEST-1 Region and continue working towards resolution. Connections using WorkSpaces Web Access are not affected by this issue.</div><div><span class=""yellowfg"">10:05 PM PDT</span>&nbsp;We have identified the cause of increased error rates when connecting to some WorkSpaces in the EU-WEST-1 Region and are in process of resolving. Connections using WorkSpaces Web Access are not affected by this issue.</div><div><span class=""yellowfg"">10:29 PM PDT</span>&nbsp;We experienced an issue causing increased error rates when connecting to WorkSpaces from native clients in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",workspaces-eu-west-1,2018-10-25 01:43:08,WorkSpaces,Ireland,6:43 PM,10:29 PM,PDT,226.0,2018,10,25,2018-10-25,connection errors," 6:43 PM PDT -  We are investigating increased error rates when connecting to WorkSpaces in the EU-WEST-1 Region.
 7:18 PM PDT -  We can confirm increased error rates when connecting to WorkSpaces in the EU-WEST-1 Region. Connections using WorkSpaces Web Access are not affected by this issue.
 7:57 PM PDT -  We have identified the cause of increased error rates when connecting to some WorkSpaces in the EU-WEST-1 Region and continue working towards resolution. Connections using WorkSpaces Web Access are not affected by this issue.
10:05 PM PDT -  We have identified the cause of increased error rates when connecting to some WorkSpaces in the EU-WEST-1 Region and are in process of resolving. Connections using WorkSpaces Web Access are not affected by this issue.
10:29 PM PDT -  We experienced an issue causing increased error rates when connecting to WorkSpaces from native clients in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally."
83,AWS Identity and Access Management,[RESOLVED] Increased API Latencies,1540486732,1,,"<div><span class=""yellowfg""> 9:59 AM PDT</span>&nbsp;We are investigating an increased latency on administrative APIs. Create, Delete, List, Get, and Update API actions may be impacted in multiple regions. Other AWS services whose features require IAM roles will also be impacted. User authentications and authorizations are not impacted.</div><div><span class=""yellowfg"">10:40 AM PDT</span>&nbsp;We continue to investigate increased latency on administrative APIs. Create, Delete, List, Get, and Update API actions may be impacted in multiple regions. Other AWS services like AWS CloudFormation and AWS Lambda that use IAM roles may also be impacted. User authentications and authorizations are not impacted. </div><div><span class=""yellowfg"">11:31 AM PDT</span>&nbsp;We have identified the root cause and are working towards a resolution for increased latency on administrative APIs. Create, Delete, List, Get, and Update API actions may be impacted in multiple regions. Other AWS services like AWS CloudFormation and AWS Lambda that use IAM roles may also be impacted. User authentications and authorizations are not impacted.</div><div><span class=""yellowfg"">12:40 PM PDT</span>&nbsp;We are seeing improvement in the latency for administrative APIs (Create, Delete, List, Get, and Update). We are continuing to work towards a full resolution. User authentications and authorizations are not impacted. </div><div><span class=""yellowfg"">12:57 PM PDT</span>&nbsp;Between 8:28 AM and 12:31 PM PDT, we experienced increased latency on administrative APIs. Create, Delete, List, Get, and Update API actions were impacted in multiple regions. User authentications and authorizations were not impacted. The issue has been resolved, and the service is operating normally.</div>",iam,2018-10-25 16:58:52,Identity and Access Management,Global,8:28 AM,12:31 PM,PDT,243.0,2018,10,25,2018-10-25,increased api latencies," 9:59 AM PDT -  We are investigating an increased latency on administrative APIs. Create, Delete, List, Get, and Update API actions may be impacted in multiple regions. Other AWS services whose features require IAM roles will also be impacted. User authentications and authorizations are not impacted.
10:40 AM PDT -  We continue to investigate increased latency on administrative APIs. Create, Delete, List, Get, and Update API actions may be impacted in multiple regions. Other AWS services like AWS CloudFormation and AWS Lambda that use IAM roles may also be impacted. User authentications and authorizations are not impacted. 
11:31 AM PDT -  We have identified the root cause and are working towards a resolution for increased latency on administrative APIs. Create, Delete, List, Get, and Update API actions may be impacted in multiple regions. Other AWS services like AWS CloudFormation and AWS Lambda that use IAM roles may also be impacted. User authentications and authorizations are not impacted.
12:40 PM PDT -  We are seeing improvement in the latency for administrative APIs (Create, Delete, List, Get, and Update). We are continuing to work towards a full resolution. User authentications and authorizations are not impacted. 
12:57 PM PDT -  Between 8:28 AM and 12:31 PM PDT, we experienced increased latency on administrative APIs. Create, Delete, List, Get, and Update API actions were impacted in multiple regions. User authentications and authorizations were not impacted. The issue has been resolved, and the service is operating normally."
84,AWS Identity and Access Management,[RESOLVED]  Increased Console Error Rates,1542102750,1,,"<div><span class=""yellowfg""> 1:52 AM PST</span>&nbsp;We are investigating increased error rates in the IAM console impacting policy operations for all IAM entities.</div><div><span class=""yellowfg""> 2:27 AM PST</span>&nbsp;Between 12:33 AM and 1:33 AM PST we experienced increased error rates in the IAM console impacting policy operations for all IAM entities. The issue has been resolved and the service is operating normally.</div>",iam,2018-11-13 09:52:30,Identity and Access Management,Global,12:33 AM,1:33 AM,PST,60.0,2018,11,13,2018-11-13,increased console error rates," 1:52 AM PST -  We are investigating increased error rates in the IAM console impacting policy operations for all IAM entities.
 2:27 AM PST -  Between 12:33 AM and 1:33 AM PST we experienced increased error rates in the IAM console impacting policy operations for all IAM entities. The issue has been resolved and the service is operating normally."
85,AWS Certificate Manager (Ireland),[RESOLVED] Validation and Issuance Delays for New Certificates,1542220203,1,,"<div><span class=""yellowfg"">10:31 AM PST</span>&nbsp;We are investigating delays in validating domains for public certificates and delays issuing private certificates requested through ACM in the EU-WEST-1 Region.</div><div><span class=""yellowfg"">11:23 AM PST</span>&nbsp;We can confirm delays in validating domains for public certificates and delays issuing private certificates requested through ACM in the EU-WEST-1 Region. Existing certificates and previously deployed certificates are unaffected.</div><div><span class=""yellowfg"">12:22 PM PST</span>&nbsp;We have identified the root cause of delays in validating domains for public certificates and delays issuing private certificates requested through ACM in the EU-WEST-1 Region. We are beginning to see recovery and continue to work toward full resolution. Existing certificates and previously deployed certificates are unaffected.</div><div><span class=""yellowfg""> 2:40 PM PST</span>&nbsp;Between 5:45 AM and 1:20 PM PST we experienced delays in validating domains for public certificates and delays issuing private certificates requested through ACM in the EU-WEST-1 Region. Existing certificates and previously deployed certificates were unaffected. The issue has been resolved and the service is operating normally.</div>",certificatemanager-eu-west-1,2018-11-14 18:30:03,Certificate Manager,Ireland,5:45 AM,1:20 PM,PST,455.0,2018,11,14,2018-11-14,validation and issuance delays for new certificates,"10:31 AM PST -  We are investigating delays in validating domains for public certificates and delays issuing private certificates requested through ACM in the EU-WEST-1 Region.
11:23 AM PST -  We can confirm delays in validating domains for public certificates and delays issuing private certificates requested through ACM in the EU-WEST-1 Region. Existing certificates and previously deployed certificates are unaffected.
12:22 PM PST -  We have identified the root cause of delays in validating domains for public certificates and delays issuing private certificates requested through ACM in the EU-WEST-1 Region. We are beginning to see recovery and continue to work toward full resolution. Existing certificates and previously deployed certificates are unaffected.
 2:40 PM PST -  Between 5:45 AM and 1:20 PM PST we experienced delays in validating domains for public certificates and delays issuing private certificates requested through ACM in the EU-WEST-1 Region. Existing certificates and previously deployed certificates were unaffected. The issue has been resolved and the service is operating normally."
86,Amazon Transcribe (Oregon),[RESOLVED] Increased API Error Rates,1542530957,1,,"<div><span class=""yellowfg"">12:49 AM PST</span>&nbsp;We are investigating increased API error rates in the US-WEST-2 Region.</div><div><span class=""yellowfg""> 1:23 AM PST</span>&nbsp;Between 9:51 PM on November 17th, and 1:02 AM PST on November 18th, we experienced increased API error rates in the US-WEST-2 Region. The issue has been resolved and the service is operating normally. </div>",transcribe-us-west-2,2018-11-18 08:49:17,Transcribe,Oregon,9:51 PM,1:02 AM,PST,191.0,2018,11,18,2018-11-18,increased api error rates,"12:49 AM PST -  We are investigating increased API error rates in the US-WEST-2 Region.
 1:23 AM PST -  Between 9:51 PM on November 17th, and 1:02 AM PST on November 18th, we experienced increased API error rates in the US-WEST-2 Region. The issue has been resolved and the service is operating normally. "
87,Amazon Elastic Container Registry (Oregon),[RESOLVED] Elevated API Error Rates,1542674869,2,,"<div><span class=""yellowfg""> 4:48 PM PST</span>&nbsp;We are experiencing elevated error rates for all APIs in the US-WEST-2 Region. </div><div><span class=""yellowfg""> 5:14 PM PST</span>&nbsp;We have identified the root cause of the increased API error rates. We are beginning to see recovery, and continue to work towards full resolution.</div><div><span class=""yellowfg""> 5:27 PM PST</span>&nbsp;Between 4:02 PM and 5:04 PM PST we experienced increased API error rates for the ECR APIs in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",ecr-us-west-2,2018-11-20 00:47:49,Elastic Container Registry,Oregon,4:02 PM,5:04 PM,PST,62.0,2018,11,20,2018-11-20,elevated api error rates," 4:48 PM PST -  We are experiencing elevated error rates for all APIs in the US-WEST-2 Region. 
 5:14 PM PST -  We have identified the root cause of the increased API error rates. We are beginning to see recovery, and continue to work towards full resolution.
 5:27 PM PST -  Between 4:02 PM and 5:04 PM PST we experienced increased API error rates for the ECR APIs in the US-WEST-2 Region. The issue has been resolved and the service is operating normally."
88,Amazon Elastic Compute Cloud (N. Virginia),[RESOLVED] Increased API and Launch Error Rates,1542721813,1,,"<div><span class=""yellowfg""> 5:50 AM PST</span>&nbsp;We are investigating increased API and launch error rates in a single Availability Zone in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 6:20 AM PST</span>&nbsp;Between 5:06 AM and 5:50 AM PST we experienced elevated error rates for instance related APIs and new instance launches in a single Availability Zone in the US-EAST-1 Region. Existing instances were not affected. The issue has been resolved and the service is operating normally.</div>",ec2-us-east-1,2018-11-20 13:50:13,Elastic Compute Cloud,N. Virginia,5:06 AM,5:50 AM,PST,44.0,2018,11,20,2018-11-20,increased api and launch error rates," 5:50 AM PST -  We are investigating increased API and launch error rates in a single Availability Zone in the US-EAST-1 Region.
 6:20 AM PST -  Between 5:06 AM and 5:50 AM PST we experienced elevated error rates for instance related APIs and new instance launches in a single Availability Zone in the US-EAST-1 Region. Existing instances were not affected. The issue has been resolved and the service is operating normally."
89,Amazon Simple Storage Service (Ohio),[RESOLVED] Increased Error Rates,1542766836,1,,"<div><span class=""yellowfg""> 6:20 PM PST</span>&nbsp;Between 5:15 PM and 5:55 PM PST, we experienced intermittent increased error rates for a subset of requests made to the Amazon S3 APIs in the US-EAST-2 Region. The issue has been resolved and the service is operating normally.</div>",s3-us-east-2,2018-11-21 02:20:36,Simple Storage Service,Ohio,5:15 PM,5:55 PM,PST,40.0,2018,11,21,2018-11-21,increased error rates," 6:20 PM PST -  Between 5:15 PM and 5:55 PM PST, we experienced intermittent increased error rates for a subset of requests made to the Amazon S3 APIs in the US-EAST-2 Region. The issue has been resolved and the service is operating normally."
90,Amazon Elastic Compute Cloud (Seoul),[RESOLVED] DNS Resolution Issues,1542844952,2,,"<div><span class=""yellowfg""> 4:02 PM PST</span>&nbsp;We are investigating intermittent DNS resolution issues for some instances in the AP-NORTHEAST-2 Region. </div><div><span class=""yellowfg""> 4:36 PM PST</span>&nbsp;We have identified the cause of the DNS resolution issues for some instances in the AP-NORTHEAST-2 Region and continue working towards resolution.
</div><div><span class=""yellowfg""> 5:01 PM PST</span>&nbsp;Between 3:19 PM and 4:43 PM PST we experienced DNS resolution issues for some instances in the AP-NORTHEAST-2 Region. Some AWS services experienced elevated error rates as a result of this issue. The issue has been resolved and the service is operating normally. <br> <br>Additional details have been provided <a href=""https://aws.amazon.com/message/74876/"">here</a>. Customers who have questions about this event can contact the AWS Support team by <a href=""https://aws.amazon.com/support"">opening a case in the AWS Support Center</a>. </div>",ec2-ap-northeast-2,2018-11-22 00:02:32,Elastic Compute Cloud,Seoul,3:19 PM,4:43 PM,PST,84.0,2018,11,22,2018-11-22,dns resolution issues," 4:02 PM PST -  We are investigating intermittent DNS resolution issues for some instances in the AP-NORTHEAST-2 Region. 
 4:36 PM PST -  We have identified the cause of the DNS resolution issues for some instances in the AP-NORTHEAST-2 Region and continue working towards resolution.
 5:01 PM PST -  Between 3:19 PM and 4:43 PM PST we experienced DNS resolution issues for some instances in the AP-NORTHEAST-2 Region. Some AWS services experienced elevated error rates as a result of this issue. The issue has been resolved and the service is operating normally.  Additional details have been provided here. Customers who have questions about this event can contact the AWS Support team by opening a case in the AWS Support Center. "
91,AWS Lambda (Seoul),[RESOLVED] Increased Error Rates,1542846560,2,,"<div><span class=""yellowfg""> 4:29 PM PST</span>&nbsp;We are investigating increased API error rates in the AP-NORTHEAST-2 Region.</div><div><span class=""yellowfg""> 5:18 PM PST</span>&nbsp;We have identified the root cause and are seeing recovery. We continue to work toward full resolution. </div><div><span class=""yellowfg""> 6:19 PM PST</span>&nbsp;Between 3:19 PM and 5:58 PM PST we experienced increased API error rates in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally.</div>",lambda-ap-northeast-2,2018-11-22 00:29:20,Lambda,Seoul,3:19 PM,5:58 PM,PST,159.0,2018,11,22,2018-11-22,increased error rates," 4:29 PM PST -  We are investigating increased API error rates in the AP-NORTHEAST-2 Region.
 5:18 PM PST -  We have identified the root cause and are seeing recovery. We continue to work toward full resolution. 
 6:19 PM PST -  Between 3:19 PM and 5:58 PM PST we experienced increased API error rates in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally."
92,AWS Elastic Beanstalk (Seoul),[RESOLVED] Increased Error Rates,1542847301,2,,"<div><span class=""yellowfg""> 4:41 PM PST</span>&nbsp;We are currently investigating increased error rates to Elastic Beanstalk APIs in the AP-NORTHEAST-2 Region.</div><div><span class=""yellowfg""> 5:09 PM PST</span>&nbsp;Between 3:19 PM and 4:43 PM PST we experienced increased error rates in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally.</div>",elasticbeanstalk-ap-northeast-2,2018-11-22 00:41:41,Elastic Beanstalk,Seoul,3:19 PM,4:43 PM,PST,84.0,2018,11,22,2018-11-22,increased error rates," 4:41 PM PST -  We are currently investigating increased error rates to Elastic Beanstalk APIs in the AP-NORTHEAST-2 Region.
 5:09 PM PST -  Between 3:19 PM and 4:43 PM PST we experienced increased error rates in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally."
93,Amazon API Gateway (Seoul),[RESOLVED] Increased Error Rates,1542847420,2,,"<div><span class=""yellowfg""> 4:43 PM PST</span>&nbsp;We are investigating increased error rates for invokes in the AP-NORTHEAST-2 Region.</div><div><span class=""yellowfg""> 5:00 PM PST</span>&nbsp;Between 3:19 PM and 4:43 PM PST we experienced increased error rates in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally. </div>",apigateway-ap-northeast-2,2018-11-22 00:43:40,API Gateway,Seoul,3:19 PM,4:43 PM,PST,84.0,2018,11,22,2018-11-22,increased error rates," 4:43 PM PST -  We are investigating increased error rates for invokes in the AP-NORTHEAST-2 Region.
 5:00 PM PST -  Between 3:19 PM and 4:43 PM PST we experienced increased error rates in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally. "
94,Amazon MQ (Seoul),[RESOLVED] Increased Error Rates,1542847452,2,,"<div><span class=""yellowfg""> 4:44 PM PST</span>&nbsp;We are investigating increased API error rates in AP-NORTHEAST-2 Region.</div><div><span class=""yellowfg""> 5:20 PM PST</span>&nbsp;We have identified the root cause and are seeing recovery. We continue to work toward full resolution. </div><div><span class=""yellowfg""> 5:40 PM PST</span>&nbsp;Between 3:19 PM and 5:28 PM PST we experienced increased API error rates in AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally. </div>",mq-ap-northeast-2,2018-11-22 00:44:12,MQ,Seoul,3:19 PM,5:28 PM,PST,129.0,2018,11,22,2018-11-22,increased error rates," 4:44 PM PST -  We are investigating increased API error rates in AP-NORTHEAST-2 Region.
 5:20 PM PST -  We have identified the root cause and are seeing recovery. We continue to work toward full resolution. 
 5:40 PM PST -  Between 3:19 PM and 5:28 PM PST we experienced increased API error rates in AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally. "
95,Amazon Kinesis Firehose (Seoul),[RESOLVED] Increased Error Rates,1542847565,2,,"<div><span class=""yellowfg""> 4:46 PM PST</span>&nbsp;We are investigating increased API fault rates in the AP-NORTHEAST-2 Region.</div><div><span class=""yellowfg""> 5:05 PM PST</span>&nbsp;Between 3:19 PM and 4:43 PM PST we experienced increased API fault rates and latencies in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally. </div>",firehose-ap-northeast-2,2018-11-22 00:46:05,Kinesis Firehose,Seoul,3:19 PM,4:43 PM,PST,84.0,2018,11,22,2018-11-22,increased error rates," 4:46 PM PST -  We are investigating increased API fault rates in the AP-NORTHEAST-2 Region.
 5:05 PM PST -  Between 3:19 PM and 4:43 PM PST we experienced increased API fault rates and latencies in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally. "
96,AWS IoT Core (Seoul),[RESOLVED] Increased Error Rates,1542848046,2,,"<div><span class=""yellowfg""> 4:54 PM PST</span>&nbsp;We are currently investigating increased error rates for AWS IoT Core in the AP-NORTHEAST-2 Region.</div><div><span class=""yellowfg""> 5:09 PM PST</span>&nbsp;Between 3:19 PM and 4:51 PM PST we experienced increased error rates in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally.</div>",awsiot-ap-northeast-2,2018-11-22 00:54:06,IoT Core,Seoul,3:19 PM,4:51 PM,PST,92.0,2018,11,22,2018-11-22,increased error rates," 4:54 PM PST -  We are currently investigating increased error rates for AWS IoT Core in the AP-NORTHEAST-2 Region.
 5:09 PM PST -  Between 3:19 PM and 4:51 PM PST we experienced increased error rates in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally."
97,Amazon WorkSpaces (Seoul),[RESOLVED] Increased Error Rates,1542848776,1,,"<div><span class=""yellowfg""> 5:06 PM PST</span>&nbsp;Between 3:15 PM and 4:40 PM PST we experienced increased errors for connections to WorkSpaces in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally. </div>",workspaces-ap-northeast-2,2018-11-22 01:06:16,WorkSpaces,Seoul,3:15 PM,4:40 PM,PST,85.0,2018,11,22,2018-11-22,increased error rates, 5:06 PM PST -  Between 3:15 PM and 4:40 PM PST we experienced increased errors for connections to WorkSpaces in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally. 
98,Amazon Redshift (Seoul),[RESOLVED] Increased Error Rates,1542849499,1,,"<div><span class=""yellowfg""> 5:18 PM PST</span>&nbsp;Between 3:19 PM and 4:56 PM PST we experienced API and S3 connectivity failures in AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally.</div>",redshift-ap-northeast-2,2018-11-22 01:18:19,Redshift,Seoul,3:19 PM,4:56 PM,PST,97.0,2018,11,22,2018-11-22,increased error rates, 5:18 PM PST -  Between 3:19 PM and 4:56 PM PST we experienced API and S3 connectivity failures in AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally.
99,AWS X-Ray (Seoul),[RESOLVED] Increased Error Rates,1542849575,1,,"<div><span class=""yellowfg""> 5:19 PM PST</span>&nbsp;Between 3:19 PM and 4:43 PM PST we experienced increased error rates for API calls in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally. </div>",xray-ap-northeast-2,2018-11-22 01:19:35,X-Ray,Seoul,3:19 PM,4:43 PM,PST,84.0,2018,11,22,2018-11-22,increased error rates, 5:19 PM PST -  Between 3:19 PM and 4:43 PM PST we experienced increased error rates for API calls in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally. 
100,AWS Billing Console,[RESOLVED] Increased Billing Console Error Rates,1543787631,1,,"<div><span class=""yellowfg""> 1:54 PM PST</span>&nbsp;We are investigating errors affecting the Billing Console including the Dashboard and the Bills page.</div><div><span class=""yellowfg""> 2:22 PM PST</span>&nbsp;Between 12:28 PM and 2:10 PM PST we experienced increased error rates affecting the Billing Console including the Dashboard and the Bills page. The issue has been resolved and the service is operating normally.</div>",billingconsole,2018-12-02 21:53:51,Billing Console,Global,12:28 PM,2:10 PM,PST,102.0,2018,12,2,2018-12-02,increased billing console error rates," 1:54 PM PST -  We are investigating errors affecting the Billing Console including the Dashboard and the Bills page.
 2:22 PM PST -  Between 12:28 PM and 2:10 PM PST we experienced increased error rates affecting the Billing Console including the Dashboard and the Bills page. The issue has been resolved and the service is operating normally."
101,Amazon Elastic File System (N. Virginia),[RESOLVED] Increased Error Rates,1543880790,1,,"<div><span class=""yellowfg""> 3:46 PM PST</span>&nbsp;We are investigating increased error rates for file system creation requests in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 4:13 PM PST</span>&nbsp;We are continuing to experience increased error rates for file system creation requests in the US-EAST-1 Region, and are actively working to resolve the issue.</div><div><span class=""yellowfg""> 5:02 PM PST</span>&nbsp;Between 2:40 PM and 4:45 PM PST, we experienced increased error rates for file system creation requests in the US-EAST-1 Region. The issue has been resolved and the service is now operating normally.</div>",elasticfilesystem-us-east-1,2018-12-03 23:46:30,Elastic File System,N. Virginia,2:40 PM,4:45 PM,PST,125.0,2018,12,3,2018-12-03,increased error rates," 3:46 PM PST -  We are investigating increased error rates for file system creation requests in the US-EAST-1 Region.
 4:13 PM PST -  We are continuing to experience increased error rates for file system creation requests in the US-EAST-1 Region, and are actively working to resolve the issue.
 5:02 PM PST -  Between 2:40 PM and 4:45 PM PST, we experienced increased error rates for file system creation requests in the US-EAST-1 Region. The issue has been resolved and the service is now operating normally."
102,AWS Management Console (US-West),[RESOLVED] Increased Error Rates,1544570280,1,,"<div><span class=""yellowfg""> 3:18 PM PST</span>&nbsp;We are investigating increased error rates for the AWS Management Console in the US-GOV-WEST-1 Region.</div><div><span class=""yellowfg""> 3:56 PM PST</span>&nbsp;Between 2:11 PM and 3:41 PM PST we experienced increased error rates for the AWS Management Console in the US-GOV-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",management-console-us-gov-west-1,2018-12-11 23:18:00,Management Console,US-West,2:11 PM,3:41 PM,PST,90.0,2018,12,11,2018-12-11,increased error rates," 3:18 PM PST -  We are investigating increased error rates for the AWS Management Console in the US-GOV-WEST-1 Region.
 3:56 PM PST -  Between 2:11 PM and 3:41 PM PST we experienced increased error rates for the AWS Management Console in the US-GOV-WEST-1 Region. The issue has been resolved and the service is operating normally."
103,AWS Key Management Service (Sao Paulo),[RESOLVED] Increased Error Rates,1544584045,2,,"<div><span class=""yellowfg""> 7:07 PM PST</span>&nbsp;We are investigating increased error rates on the KMS CreateKey API in the SA-EAST-1 Region.</div><div><span class=""yellowfg""> 7:34 PM PST</span>&nbsp;We have identified the root cause for the increased error rates on the KMS CreateKey API in the SA-EAST-1 Region and continue to work towards resolution.</div><div><span class=""yellowfg""> 8:16 PM PST</span>&nbsp;Between 6:23 PM and 8:03 PM PST, we experienced increased error rates on the KMS CreateKey API in the SA-EAST-1 Region. This issue has been resolved and the service is operating normally.</div>",kms-sa-east-1,2018-12-12 03:07:25,Key Management Service,Sao Paulo,6:23 PM,8:03 PM,PST,100.0,2018,12,12,2018-12-12,increased error rates," 7:07 PM PST -  We are investigating increased error rates on the KMS CreateKey API in the SA-EAST-1 Region.
 7:34 PM PST -  We have identified the root cause for the increased error rates on the KMS CreateKey API in the SA-EAST-1 Region and continue to work towards resolution.
 8:16 PM PST -  Between 6:23 PM and 8:03 PM PST, we experienced increased error rates on the KMS CreateKey API in the SA-EAST-1 Region. This issue has been resolved and the service is operating normally."
104,Amazon Elastic Container Registry (N. Virginia),[RESOLVED] Elevated API Error Rates,1545248645,1,,"<div><span class=""yellowfg"">11:44 AM PST</span>&nbsp;We are investigating increased API error rates for the ECR APIs in the US-EAST-1 Region. </div><div><span class=""yellowfg"">11:58 AM PST</span>&nbsp;Between 11:14 AM and 11:51 AM PST we experienced increased API error rates for the ECR APIs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",ecr-us-east-1,2018-12-19 19:44:05,Elastic Container Registry,N. Virginia,11:14 AM,11:51 AM,PST,37.0,2018,12,19,2018-12-19,elevated api error rates,"11:44 AM PST -  We are investigating increased API error rates for the ECR APIs in the US-EAST-1 Region. 
11:58 AM PST -  Between 11:14 AM and 11:51 AM PST we experienced increased API error rates for the ECR APIs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. "
105,Amazon WorkSpaces (N. Virginia),[RESOLVED] Increased errors terminating WorkSpaces,1546532318,1,,"<div><span class=""yellowfg""> 8:18 AM PST</span>&nbsp;We are investigating increased errors when terminating WorkSpaces in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 9:23 AM PST</span>&nbsp;Between 6:40 AM and 9:14 AM PST Amazon WorkSpaces experienced increased errors when terminating WorkSpaces in the US-EAST-1 Region. The issue causing increased errors has been resolved and the service is operating normally.</div>",workspaces-us-east-1,2019-01-03 16:18:38,WorkSpaces,N. Virginia,6:40 AM,9:14 AM,PST,154.0,2019,1,3,2019-01-03,increased errors terminating workspaces," 8:18 AM PST -  We are investigating increased errors when terminating WorkSpaces in the US-EAST-1 Region.
 9:23 AM PST -  Between 6:40 AM and 9:14 AM PST Amazon WorkSpaces experienced increased errors when terminating WorkSpaces in the US-EAST-1 Region. The issue causing increased errors has been resolved and the service is operating normally."
106,Amazon Elastic Compute Cloud (London),[RESOLVED] Network Connectivity,1547258341,1,,"<div><span class=""yellowfg""> 5:59 PM PST</span>&nbsp;We are investigating network connectivity issues for some instances in a single Available Zone in the EU-WEST-2 Region.</div><div><span class=""yellowfg""> 6:29 PM PST</span>&nbsp;We can confirm that some instances have experienced a loss of power in a single Availability Zone in the EU-WEST-2 Region. Some EBS volumes within the affected Availability Zone are experiencing degraded performance. We continue to work on resolving the issue. </div><div><span class=""yellowfg""> 7:15 PM PST</span>&nbsp;Between 5:33 PM and 7:06 PM PST some instances experienced connectivity issues within a single Availability Zone in the EU-WEST-2 Region. The affected instances did not lose power but became unreachable due to a loss of power to networking devices within the affected Availability Zone. Some EBS volumes experienced degraded performance during this time period and launches of new EC2 instances and the creation of new EBS volumes experienced increased error rates. The issue has been resolved and the service is operating normally. </div>",ec2-eu-west-2,2019-01-12 01:59:01,Elastic Compute Cloud,London,5:33 PM,7:06 PM,PST,93.0,2019,1,12,2019-01-12,network connectivity," 5:59 PM PST -  We are investigating network connectivity issues for some instances in a single Available Zone in the EU-WEST-2 Region.
 6:29 PM PST -  We can confirm that some instances have experienced a loss of power in a single Availability Zone in the EU-WEST-2 Region. Some EBS volumes within the affected Availability Zone are experiencing degraded performance. We continue to work on resolving the issue. 
 7:15 PM PST -  Between 5:33 PM and 7:06 PM PST some instances experienced connectivity issues within a single Availability Zone in the EU-WEST-2 Region. The affected instances did not lose power but became unreachable due to a loss of power to networking devices within the affected Availability Zone. Some EBS volumes experienced degraded performance during this time period and launches of new EC2 instances and the creation of new EBS volumes experienced increased error rates. The issue has been resolved and the service is operating normally. "
107,AWS Lambda (London),[RESOLVED] Increased Error Rates,1547258876,1,,"<div><span class=""yellowfg""> 6:08 PM PST</span>&nbsp;We are investigating increased error rates and elevated latencies for AWS Lambda requests in the EU-WEST-2 Region. Newly created functions and console editing is also affected.</div><div><span class=""yellowfg""> 7:19 PM PST</span>&nbsp;Between 5:33 PM and 7:12 PM PST Lambda experienced elevated error rates and latencies in the EU-WEST-2 Region. The issue has been resolved and the service is operating normally</div>",lambda-eu-west-2,2019-01-12 02:07:56,Lambda,London,5:33 PM,7:12 PM,PST,99.0,2019,1,12,2019-01-12,increased error rates," 6:08 PM PST -  We are investigating increased error rates and elevated latencies for AWS Lambda requests in the EU-WEST-2 Region. Newly created functions and console editing is also affected.
 7:19 PM PST -  Between 5:33 PM and 7:12 PM PST Lambda experienced elevated error rates and latencies in the EU-WEST-2 Region. The issue has been resolved and the service is operating normally"
108,Amazon Relational Database Service (London),[RESOLVED] Network Connectivity,1547262903,1,,"<div><span class=""yellowfg""> 7:15 PM PST</span>&nbsp;We are investigating connectivity issues affecting some instances in a single Availability Zone in the EU-WEST-2 Region.</div><div><span class=""yellowfg""> 7:49 PM PST</span>&nbsp;Beginning at 5:33 PM PST, we experienced connectivity issues affecting some database instances in a single Availability Zone in the EU-WEST-2 Region. Connectivity was restored at 7:11 PM PST, at which point the majority of database instances recovered. A small number of Single-AZ databases remain unavailable and we are continuing to work to recover all affected database instances.</div><div><span class=""yellowfg""> 8:08 PM PST</span>&nbsp;Between 5:33 PM PST and 7:11 PM PST, we experienced connectivity issues affecting some database instances in a single Availability Zone in the EU-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",rds-eu-west-2,2019-01-12 03:15:03,Relational Database Service,London,5:33 PM,7:11 PM,PST,98.0,2019,1,12,2019-01-12,network connectivity," 7:15 PM PST -  We are investigating connectivity issues affecting some instances in a single Availability Zone in the EU-WEST-2 Region.
 7:49 PM PST -  Beginning at 5:33 PM PST, we experienced connectivity issues affecting some database instances in a single Availability Zone in the EU-WEST-2 Region. Connectivity was restored at 7:11 PM PST, at which point the majority of database instances recovered. A small number of Single-AZ databases remain unavailable and we are continuing to work to recover all affected database instances.
 8:08 PM PST -  Between 5:33 PM PST and 7:11 PM PST, we experienced connectivity issues affecting some database instances in a single Availability Zone in the EU-WEST-2 Region. The issue has been resolved and the service is operating normally."
109,Amazon CloudFront,[RESOLVED] Elevated Lambda@Edge errors,1547501684,1,,"<div><span class=""yellowfg""> 1:34 PM PST</span>&nbsp;We are investigating elevated Lambda@Edge error rates that may be affecting some customers. CloudFront customers who do not have Lambda@Edge functions are unaffected by this issue. </div><div><span class=""yellowfg""> 2:11 PM PST</span>&nbsp;Between 11:40 AM PST and 2:07 PM PST, customers may have experienced elevated Lambda@Edge error rates. CloudFront customers who do not have Lambda@Edge functions were unaffected by this issue. The issue has been resolved and the service is operating normally.</div>",cloudfront,2019-01-14 21:34:44,CloudFront,Global,11:40 AM,2:07 PM,PST,147.0,2019,1,14,2019-01-14,elevated lambda@edge errors," 1:34 PM PST -  We are investigating elevated Lambda@Edge error rates that may be affecting some customers. CloudFront customers who do not have Lambda@Edge functions are unaffected by this issue. 
 2:11 PM PST -  Between 11:40 AM PST and 2:07 PM PST, customers may have experienced elevated Lambda@Edge error rates. CloudFront customers who do not have Lambda@Edge functions were unaffected by this issue. The issue has been resolved and the service is operating normally."
110,Amazon Relational Database Service (Ireland),[RESOLVED] Increased API error rates,1548423741,1,,"<div><span class=""yellowfg""> 5:42 AM PST</span>&nbsp;Between 5:03 AM and 5:25 AM PST we experienced increased API error rates and latencies in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",rds-eu-west-1,2019-01-25 13:42:21,Relational Database Service,Ireland,5:03 AM,5:25 AM,PST,22.0,2019,1,25,2019-01-25,increased api error rates, 5:42 AM PST -  Between 5:03 AM and 5:25 AM PST we experienced increased API error rates and latencies in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally.
111,Amazon Chime,[RESOLVED] Increased Error Rates,1548450005,1,,"<div><span class=""yellowfg""> 1:00 PM PST</span>&nbsp;We are investigating contact search latency issues in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 1:17 PM PST</span>&nbsp;We are investigating latency in the ability to search contacts, and the inability to add contacts to rooms and calls in the US-EAST-1 Region. Existing users, rooms, and scheduled calls are not impacted.</div><div><span class=""yellowfg""> 2:42 PM PST</span>&nbsp;Between 11:38 AM and 2:19 PM PST, we experienced latency in the ability to search contacts, and the inability to add contacts to rooms and calls in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",chime,2019-01-25 21:00:05,Chime,Global,11:38 AM,2:19 PM,PST,161.0,2019,1,25,2019-01-25,increased error rates," 1:00 PM PST -  We are investigating contact search latency issues in the US-EAST-1 Region.
 1:17 PM PST -  We are investigating latency in the ability to search contacts, and the inability to add contacts to rooms and calls in the US-EAST-1 Region. Existing users, rooms, and scheduled calls are not impacted.
 2:42 PM PST -  Between 11:38 AM and 2:19 PM PST, we experienced latency in the ability to search contacts, and the inability to add contacts to rooms and calls in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
112,AWS Direct Connect (N. Virginia),[RESOLVED] Increased API errors and latencies,1548593864,1,,"<div><span class=""yellowfg""> 4:57 AM PST</span>&nbsp;We are investigating increased error rates and latencies for AWS Direct Connect APIs in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 5:30 AM PST</span>&nbsp;We continue to investigate increased error rates and latencies for AWS Direct Connect APIs in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 6:06 AM PST</span>&nbsp;We can confirm increased error rates and latencies for AWS Direct Connect APIs in the US-EAST-1 Region and continue to work towards resolution.</div><div><span class=""yellowfg""> 6:31 AM PST</span>&nbsp;Between 4:02 AM and 6:08 AM PST, we experienced increased error rates and latencies for AWS Direct Connect APIs in the US-EAST-1 Region. Connectivity over existing Direct Connect connections was not affected, however, customers may have experienced delays using the Direct Connect console and APIs during this period. The issue has been resolved and the service is operating normally.</div>",directconnect-us-east-1,2019-01-27 12:57:44,Direct Connect,N. Virginia,4:02 AM,6:08 AM,PST,126.0,2019,1,27,2019-01-27,increased api errors and latencies," 4:57 AM PST -  We are investigating increased error rates and latencies for AWS Direct Connect APIs in the US-EAST-1 Region.
 5:30 AM PST -  We continue to investigate increased error rates and latencies for AWS Direct Connect APIs in the US-EAST-1 Region.
 6:06 AM PST -  We can confirm increased error rates and latencies for AWS Direct Connect APIs in the US-EAST-1 Region and continue to work towards resolution.
 6:31 AM PST -  Between 4:02 AM and 6:08 AM PST, we experienced increased error rates and latencies for AWS Direct Connect APIs in the US-EAST-1 Region. Connectivity over existing Direct Connect connections was not affected, however, customers may have experienced delays using the Direct Connect console and APIs during this period. The issue has been resolved and the service is operating normally."
113,AWS Marketplace,[RESOLVED] Increased Subscription Error Rates ,1549280664,1,,"<div><span class=""yellowfg""> 3:44 AM PST</span>&nbsp;We are investigating increased AWS Marketplace subscription error rates.</div><div><span class=""yellowfg""> 4:34 AM PST</span>&nbsp;Between 1:36 AM and 4:07 AM PST we experienced increased AWS Marketplace subscription error rates. The issue has been resolved and the service is operating normally.</div>",marketplace,2019-02-04 11:44:24,Marketplace,Global,1:36 AM,4:07 AM,PST,151.0,2019,2,4,2019-02-04,increased subscription error rates," 3:44 AM PST -  We are investigating increased AWS Marketplace subscription error rates.
 4:34 AM PST -  Between 1:36 AM and 4:07 AM PST we experienced increased AWS Marketplace subscription error rates. The issue has been resolved and the service is operating normally."
114,Amazon Relational Database Service (Oregon),[RESOLVED] Small Number of Amazon Aurora Instances Unavailable,1549679132,1,,"<div><span class=""yellowfg""> 6:25 PM PST</span>&nbsp;We are investigating connectivity issues affecting a small number of Amazon Aurora instances in the US-WEST-2 Region.</div><div><span class=""yellowfg""> 7:20 PM PST</span>&nbsp;The majority of database instances that experienced connectivity issues in the US-WEST-2 Region have recovered. We are continuing to work to resolve the instances still experiencing connectivity issues.</div><div><span class=""yellowfg""> 8:05 PM PST</span>&nbsp;Between 5:14 PM and 7:40 PM PST, we experienced connectivity issues affecting some Amazon Aurora instances in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",rds-us-west-2,2019-02-09 02:25:32,Relational Database Service,Oregon,5:14 PM,7:40 PM,PST,146.0,2019,2,9,2019-02-09,small number of amazon aurora instances unavailable," 6:25 PM PST -  We are investigating connectivity issues affecting a small number of Amazon Aurora instances in the US-WEST-2 Region.
 7:20 PM PST -  The majority of database instances that experienced connectivity issues in the US-WEST-2 Region have recovered. We are continuing to work to resolve the instances still experiencing connectivity issues.
 8:05 PM PST -  Between 5:14 PM and 7:40 PM PST, we experienced connectivity issues affecting some Amazon Aurora instances in the US-WEST-2 Region. The issue has been resolved and the service is operating normally."
115,AWS Identity and Access Management,[RESOLVED] Increased Error Rates and Latencies,1550029704,1,,"<div><span class=""yellowfg""> 7:48 PM PST</span>&nbsp;We are investigating increased error rates and latencies for IAM APIs.</div><div><span class=""yellowfg""> 8:03 PM PST</span>&nbsp;Between 6:20 PM and 7:49 PM PST, we experienced increased error rates and latencies for IAM APIs. The issue has been resolved and the service is operating normally.</div>",iam,2019-02-13 03:48:24,Identity and Access Management,Global,6:20 PM,7:49 PM,PST,89.0,2019,2,13,2019-02-13,increased error rates and latencies," 7:48 PM PST -  We are investigating increased error rates and latencies for IAM APIs.
 8:03 PM PST -  Between 6:20 PM and 7:49 PM PST, we experienced increased error rates and latencies for IAM APIs. The issue has been resolved and the service is operating normally."
116,AWS Elastic Beanstalk (Sydney),[RESOLVED] Increased API Failure Rates,1550092874,1,,"<div><span class=""yellowfg""> 1:21 PM PST</span>&nbsp;We are currently investigating increased error rates to Elastic Beanstalk APIs in the AP-SOUTHEAST-2 Region.</div><div><span class=""yellowfg""> 2:11 PM PST</span>&nbsp;We can confirm increased error rates for the Elastic Beanstalk APIs in the AP-SOUTHEAST-2 Region. We have identified the root cause and continue to work toward resolution.</div><div><span class=""yellowfg""> 2:31 PM PST</span>&nbsp;Between 12:39 PM and 2:12 PM PST we experienced elevated error rates in the AP-SOUTHEAST-2 Region. The issue has been resolved and the service is operating normally.</div>",elasticbeanstalk-ap-southeast-2,2019-02-13 21:21:14,Elastic Beanstalk,Sydney,12:39 PM,2:12 PM,PST,93.0,2019,2,13,2019-02-13,increased api failure rates," 1:21 PM PST -  We are currently investigating increased error rates to Elastic Beanstalk APIs in the AP-SOUTHEAST-2 Region.
 2:11 PM PST -  We can confirm increased error rates for the Elastic Beanstalk APIs in the AP-SOUTHEAST-2 Region. We have identified the root cause and continue to work toward resolution.
 2:31 PM PST -  Between 12:39 PM and 2:12 PM PST we experienced elevated error rates in the AP-SOUTHEAST-2 Region. The issue has been resolved and the service is operating normally."
117,Amazon Relational Database Service (Sydney),[RESOLVED] Increased Create Times and Cluster Unavailability,1550094163,1,,"<div><span class=""yellowfg""> 1:42 PM PST</span>&nbsp;We are investigating increased database create times and cluster unavailability for Amazon Aurora in the AP-SOUTHEAST-2 Region.</div><div><span class=""yellowfg""> 2:09 PM PST</span>&nbsp;We can confirm increased database create times and cluster unavailability for Amazon Aurora in the AP-SOUTHEAST-2 Region. We have identified the root cause and continue to work toward resolution.</div><div><span class=""yellowfg""> 2:49 PM PST</span>&nbsp;We have resolved the issue resulting in increased database create times, and continue to work toward full resolution on Amazon Aurora Cluster Availability.</div><div><span class=""yellowfg""> 4:06 PM PST</span>&nbsp;We are beginning to see recovery for some Amazon Aurora Clusters and continue to work toward full resolution.</div><div><span class=""yellowfg""> 6:11 PM PST</span>&nbsp;Beginning at 11:54 AM PST some Amazon Aurora clusters experienced increased database create times and cluster unavailability in the AP-SOUTHEAST-2 Region. Elevated create times were resolved at 2:27 PM PST, at which point some existing clusters continued to experience availability issues. As of 5:35 PM PST both issues have been resolved and the service is operating normally. In total, the event impacted a little less than 3% of the Aurora databases in the region.</div>",rds-ap-southeast-2,2019-02-13 21:42:43,Relational Database Service,Sydney,11:54 AM,5:35 PM,PST,341.0,2019,2,13,2019-02-13,increased create times and cluster unavailability," 1:42 PM PST -  We are investigating increased database create times and cluster unavailability for Amazon Aurora in the AP-SOUTHEAST-2 Region.
 2:09 PM PST -  We can confirm increased database create times and cluster unavailability for Amazon Aurora in the AP-SOUTHEAST-2 Region. We have identified the root cause and continue to work toward resolution.
 2:49 PM PST -  We have resolved the issue resulting in increased database create times, and continue to work toward full resolution on Amazon Aurora Cluster Availability.
 4:06 PM PST -  We are beginning to see recovery for some Amazon Aurora Clusters and continue to work toward full resolution.
 6:11 PM PST -  Beginning at 11:54 AM PST some Amazon Aurora clusters experienced increased database create times and cluster unavailability in the AP-SOUTHEAST-2 Region. Elevated create times were resolved at 2:27 PM PST, at which point some existing clusters continued to experience availability issues. As of 5:35 PM PST both issues have been resolved and the service is operating normally. In total, the event impacted a little less than 3% of the Aurora databases in the region."
118,Amazon Relational Database Service (Montreal),[RESOLVED] Increased Create Times and Cluster Unavailability,1550107156,1,,"<div><span class=""yellowfg""> 5:19 PM PST</span>&nbsp;We wanted to provide an update for Amazon Aurora in the CA-CENTRAL-1 Region that we previously communicated to affected customers on their Personal Health Dashboards. Beginning at 11:43 AM PST some Amazon Aurora clusters experienced increased database create times and cluster unavailability in the CA-CENTRAL-1 Region. Elevated create times were resolved at 2:14 PM PST, at which point some existing clusters continued to experience availability issues. As of 4:48 PM PST both issues have been resolved and the service is operating normally.</div>",rds-ca-central-1,2019-02-14 01:19:16,Relational Database Service,Montreal,11:43 AM,4:48 PM,PST,305.0,2019,2,14,2019-02-14,increased create times and cluster unavailability," 5:19 PM PST -  We wanted to provide an update for Amazon Aurora in the CA-CENTRAL-1 Region that we previously communicated to affected customers on their Personal Health Dashboards. Beginning at 11:43 AM PST some Amazon Aurora clusters experienced increased database create times and cluster unavailability in the CA-CENTRAL-1 Region. Elevated create times were resolved at 2:14 PM PST, at which point some existing clusters continued to experience availability issues. As of 4:48 PM PST both issues have been resolved and the service is operating normally."
119,Amazon Route 53,[RESOLVED] Increased Change Propagation Time ,1550635211,1,,"<div><span class=""yellowfg""> 8:00 PM PST</span>&nbsp;We are investigating increased propagation times of DNS edits to Route 53 DNS servers. Queries to existing DNS records are not affected by this issue and are being answered normally.</div><div><span class=""yellowfg""> 8:19 PM PST</span>&nbsp;We continue to investigate increased propagation times of DNS edits to the Route 53 DNS servers. This issue will also affect provisioning of services such as EFS, ElasticBeanstalk, CloudFormation, and Amazon MQ. Queries to existing DNS records are not affected by this issue. </div><div><span class=""yellowfg""> 8:51 PM PST</span>&nbsp;Between 7:34 PM and 8:45 PM PST we experienced increased propagation times of DNS edits. Queries to existing DNS records were not affected by this issue. The issue has been resolved and the service is operating normally.</div>",route53,2019-02-20 04:00:11,Route 53,Global,7:34 PM,8:45 PM,PST,71.0,2019,2,20,2019-02-20,increased change propagation time," 8:00 PM PST -  We are investigating increased propagation times of DNS edits to Route 53 DNS servers. Queries to existing DNS records are not affected by this issue and are being answered normally.
 8:19 PM PST -  We continue to investigate increased propagation times of DNS edits to the Route 53 DNS servers. This issue will also affect provisioning of services such as EFS, ElasticBeanstalk, CloudFormation, and Amazon MQ. Queries to existing DNS records are not affected by this issue. 
 8:51 PM PST -  Between 7:34 PM and 8:45 PM PST we experienced increased propagation times of DNS edits. Queries to existing DNS records were not affected by this issue. The issue has been resolved and the service is operating normally."
120,Amazon Chime,[RESOLVED] Availability ,1550790706,1,,"<div><span class=""yellowfg""> 3:11 PM PST</span>&nbsp;We are investigating meeting and chat availability issues.</div><div><span class=""yellowfg""> 3:45 PM PST</span>&nbsp;We continue to investigate meeting and chat availability issues. </div><div><span class=""yellowfg""> 4:41 PM PST</span>&nbsp;We have identified the root cause of meeting and chat availability issues and continue to work toward resolution.</div><div><span class=""yellowfg""> 5:19 PM PST</span>&nbsp;Between 2:51 PM and 5:04 PM PST we experienced meeting and chat availability issues. The issue has been resolved and the service is operating normally.</div>",chime,2019-02-21 23:11:46,Chime,Global,2:51 PM,5:04 PM,PST,133.0,2019,2,21,2019-02-21,availability," 3:11 PM PST -  We are investigating meeting and chat availability issues.
 3:45 PM PST -  We continue to investigate meeting and chat availability issues. 
 4:41 PM PST -  We have identified the root cause of meeting and chat availability issues and continue to work toward resolution.
 5:19 PM PST -  Between 2:51 PM and 5:04 PM PST we experienced meeting and chat availability issues. The issue has been resolved and the service is operating normally."
121,AWS Certificate Manager (N. Virginia),[RESOLVED] Increased Error Rates,1551126347,1,,"<div><span class=""yellowfg"">12:25 PM PST</span>&nbsp;We are experiencing increased error rates for AWS Certificate Manager in the US-EAST-1 Region.</div><div><span class=""yellowfg"">12:59 PM PST</span>&nbsp;We can confirm increased error rates for AWS Certificate Manager in the US-EAST-1 Region. We have identified the root cause and are working toward resolution.</div><div><span class=""yellowfg""> 1:50 PM PST</span>&nbsp;Between 11:48 AM and 1:26 PM PST, we experienced increased error rates for AWS Certificate Manager in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",certificatemanager-us-east-1,2019-02-25 20:25:47,Certificate Manager,N. Virginia,11:48 AM,1:26 PM,PST,98.0,2019,2,25,2019-02-25,increased error rates,"12:25 PM PST -  We are experiencing increased error rates for AWS Certificate Manager in the US-EAST-1 Region.
12:59 PM PST -  We can confirm increased error rates for AWS Certificate Manager in the US-EAST-1 Region. We have identified the root cause and are working toward resolution.
 1:50 PM PST -  Between 11:48 AM and 1:26 PM PST, we experienced increased error rates for AWS Certificate Manager in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
122,Amazon CloudFront,[RESOLVED] Change Propagation Delays,1551301596,1,,"<div><span class=""yellowfg""> 1:06 PM PST</span>&nbsp;We’re investigating longer than usual propagation times for changes to CloudFront configurations. End-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=""yellowfg""> 1:30 PM PST</span>&nbsp;Between 10:09 AM and 1:23 PM PST customers may have experienced longer than usual propagation times while making changes to CloudFront configurations. End-user requests for content from edge locations were not affected. The issue has been resolved and the service is operating normally.</div>",cloudfront,2019-02-27 21:06:36,CloudFront,Global,10:09 AM,1:23 PM,PST,194.0,2019,2,27,2019-02-27,change propagation delays," 1:06 PM PST -  We’re investigating longer than usual propagation times for changes to CloudFront configurations. End-user requests for content from our edge locations are not affected by this issue and are being served normally.
 1:30 PM PST -  Between 10:09 AM and 1:23 PM PST customers may have experienced longer than usual propagation times while making changes to CloudFront configurations. End-user requests for content from edge locations were not affected. The issue has been resolved and the service is operating normally."
123,Amazon Elastic Container Service (Ireland),[RESOLVED] Elevated API Error Rates,1551383330,1,,"<div><span class=""yellowfg"">11:49 AM PST</span>&nbsp;We are investigating elevated API error rates in the EU-WEST-1 Region.</div><div><span class=""yellowfg"">12:30 PM PST</span>&nbsp;Between 11:05 AM and 12:05 PM PST we experienced elevated API error rates in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",ecs-eu-west-1,2019-02-28 19:48:50,Elastic Container Service,Ireland,11:05 AM,12:05 PM,PST,60.0,2019,2,28,2019-02-28,elevated api error rates,"11:49 AM PST -  We are investigating elevated API error rates in the EU-WEST-1 Region.
12:30 PM PST -  Between 11:05 AM and 12:05 PM PST we experienced elevated API error rates in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally."
124,Amazon Elastic Compute Cloud (N. Virginia),[RESOLVED] Increased Launch Failures,1551977827,1,,"<div><span class=""yellowfg""> 8:57 AM PST</span>&nbsp;Between 5:40 AM and 8:20 AM PST, new launches of EC2 instances were erroneously disabled in a single Availability Zone within the US-EAST-1 Region. This caused new launches to fail when targeting the affected Availability Zone and also resulted in health checks reporting instances in the affected Availability Zone as impaired. Customers with Auto Scaling Groups configured to replace instances on impaired EC2 health checks may have had instances replaced as a result of this issue. The Availability Zone has been re-enabled for new launches and Auto Scaling has automatically replaced affected instances. The issue has been resolved and the service is operating normally.</div>",ec2-us-east-1,2019-03-07 16:57:07,Elastic Compute Cloud,N. Virginia,5:40 AM,8:20 AM,PST,160.0,2019,3,7,2019-03-07,increased launch failures," 8:57 AM PST -  Between 5:40 AM and 8:20 AM PST, new launches of EC2 instances were erroneously disabled in a single Availability Zone within the US-EAST-1 Region. This caused new launches to fail when targeting the affected Availability Zone and also resulted in health checks reporting instances in the affected Availability Zone as impaired. Customers with Auto Scaling Groups configured to replace instances on impaired EC2 health checks may have had instances replaced as a result of this issue. The Availability Zone has been re-enabled for new launches and Auto Scaling has automatically replaced affected instances. The issue has been resolved and the service is operating normally."
125,Amazon WorkSpaces (N. Virginia),[RESOLVED] Increased Error Rates,1552347514,1,,"<div><span class=""yellowfg""> 4:38 PM PDT</span>&nbsp;We are investigating increased errors when calling WorkSpaces APIs and connecting to WorkSpaces in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 5:39 PM PDT</span>&nbsp;Between 3:35 PM and 4:57 PM PDT we experienced increased API error rates and connection errors to WorkSpaces in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",workspaces-us-east-1,2019-03-11 23:38:34,WorkSpaces,N. Virginia,3:35 PM,4:57 PM,PDT,82.0,2019,3,11,2019-03-11,increased error rates," 4:38 PM PDT -  We are investigating increased errors when calling WorkSpaces APIs and connecting to WorkSpaces in the US-EAST-1 Region.
 5:39 PM PDT -  Between 3:35 PM and 4:57 PM PDT we experienced increased API error rates and connection errors to WorkSpaces in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
126,Amazon DynamoDB (N. Virginia),[RESOLVED] Latencies in DynamoDB control plane operations,1552426531,1,,"<div><span class=""yellowfg""> 2:35 PM PDT</span>&nbsp;We are currently investigating increased latencies on create, update, and delete table operations with Amazon DynamoDB in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 3:12 PM PDT</span>&nbsp;We have identified the cause of increased latencies on create, update, and delete table operations with Amazon DynamoDB in the US-EAST-1 Region and continue working towards resolution.</div><div><span class=""yellowfg""> 3:52 PM PDT</span>&nbsp;We continue to work on resolution for create, update, and delete table operations with Amazon DynamoDB in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 4:05 PM PDT</span>&nbsp;New requests to create, update, and delete tables with Amazon DynamoDB are processing normally in the US-EAST-1 Region. We continue to work towards resolution for backlogged requests.</div><div><span class=""yellowfg""> 4:20 PM PDT</span>&nbsp;Between 1:15 PM and 4:14 PM PDT, we experienced increased latencies on create, update, and delete table operations with Amazon DynamoDB in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",dynamodb-us-east-1,2019-03-12 21:35:31,DynamoDB,N. Virginia,1:15 PM,4:14 PM,PDT,179.0,2019,3,12,2019-03-12,latencies in dynamodb control plane operations," 2:35 PM PDT -  We are currently investigating increased latencies on create, update, and delete table operations with Amazon DynamoDB in the US-EAST-1 Region.
 3:12 PM PDT -  We have identified the cause of increased latencies on create, update, and delete table operations with Amazon DynamoDB in the US-EAST-1 Region and continue working towards resolution.
 3:52 PM PDT -  We continue to work on resolution for create, update, and delete table operations with Amazon DynamoDB in the US-EAST-1 Region.
 4:05 PM PDT -  New requests to create, update, and delete tables with Amazon DynamoDB are processing normally in the US-EAST-1 Region. We continue to work towards resolution for backlogged requests.
 4:20 PM PDT -  Between 1:15 PM and 4:14 PM PDT, we experienced increased latencies on create, update, and delete table operations with Amazon DynamoDB in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
127,Amazon Elastic Compute Cloud (Oregon),[RESOLVED] Increased API Error Rates,1552507000,1,,"<div><span class=""yellowfg"">12:56 PM PDT</span>&nbsp;We are investigating increased API error rates and launch failures in the US-WEST-2 Region.</div><div><span class=""yellowfg""> 1:13 PM PDT</span>&nbsp;Between 12:29 PM and 12:56 PM PDT, we experienced increased API error rates and launch failures in the US-WEST-2 Region. The issue has been resolved and the service is operating normally. </div>",ec2-us-west-2,2019-03-13 19:56:40,Elastic Compute Cloud,Oregon,12:29 PM,12:56 PM,PDT,27.0,2019,3,13,2019-03-13,increased api error rates,"12:56 PM PDT -  We are investigating increased API error rates and launch failures in the US-WEST-2 Region.
 1:13 PM PDT -  Between 12:29 PM and 12:56 PM PDT, we experienced increased API error rates and launch failures in the US-WEST-2 Region. The issue has been resolved and the service is operating normally. "
128,AWS Management Console,[RESOLVED] Increased Error Rates,1553118849,1,,"<div><span class=""yellowfg""> 2:54 PM PDT</span>&nbsp;We are investigating increased error rates and latencies for the AWS Management Console in the EU-WEST-3 Region.</div><div><span class=""yellowfg""> 3:18 PM PDT</span>&nbsp;Between 2:13 PM and 3:08 PM PDT we experienced increased error rates and latencies for the AWS Management Console in the EU-WEST-3 Region. The issue has been resolved and the service is operating normally. </div>",management-console,2019-03-20 21:54:09,Management Console,Global,2:13 PM,3:08 PM,PDT,55.0,2019,3,20,2019-03-20,increased error rates," 2:54 PM PDT -  We are investigating increased error rates and latencies for the AWS Management Console in the EU-WEST-3 Region.
 3:18 PM PDT -  Between 2:13 PM and 3:08 PM PDT we experienced increased error rates and latencies for the AWS Management Console in the EU-WEST-3 Region. The issue has been resolved and the service is operating normally. "
129,Amazon Elastic Compute Cloud (N. Virginia),[RESOLVED] Increased API Error Rates and Launch Failures,1553278711,1,,"<div><span class=""yellowfg"">11:18 AM PDT</span>&nbsp;We are investigating increased API error rates and launch failures in a single Availability Zone in the US-EAST-1 Region. Existing instances are not affected.</div><div><span class=""yellowfg"">12:06 PM PDT</span>&nbsp;We continue to investigate increased API error rates and launch failures in a single Availability Zone in the US-EAST-1 Region. Existing instances are not affected.</div><div><span class=""yellowfg"">12:44 PM PDT</span>&nbsp;We continue to investigate increased API error rates and launch failures in a single Availability Zone in the US-EAST-1 Region. Existing instances are not affected.</div><div><span class=""yellowfg""> 1:11 PM PDT</span>&nbsp;Between 9:28 AM and 12:52 PM PDT we experienced increased API error rates and launch failures in a single Availability Zone in the US-EAST-1 Region. Existing instances were not affected. The issue has been resolved and the service is operating normally.</div>",ec2-us-east-1,2019-03-22 18:18:31,Elastic Compute Cloud,N. Virginia,9:28 AM,12:52 PM,PDT,204.0,2019,3,22,2019-03-22,increased api error rates and launch failures,"11:18 AM PDT -  We are investigating increased API error rates and launch failures in a single Availability Zone in the US-EAST-1 Region. Existing instances are not affected.
12:06 PM PDT -  We continue to investigate increased API error rates and launch failures in a single Availability Zone in the US-EAST-1 Region. Existing instances are not affected.
12:44 PM PDT -  We continue to investigate increased API error rates and launch failures in a single Availability Zone in the US-EAST-1 Region. Existing instances are not affected.
 1:11 PM PDT -  Between 9:28 AM and 12:52 PM PDT we experienced increased API error rates and launch failures in a single Availability Zone in the US-EAST-1 Region. Existing instances were not affected. The issue has been resolved and the service is operating normally."
130,Amazon CloudWatch (Oregon),[RESOLVED] Delayed Metrics,1553710396,2,,"<div><span class=""yellowfg"">11:14 AM PDT</span>&nbsp;We are investigating increased delays for CloudWatch metrics in the US-WEST-2 Region. CloudWatch alarms may transition into ""INSUFFICIENT_DATA"" state if set on delayed metrics.</div><div><span class=""yellowfg"">11:33 AM PDT</span>&nbsp;We can confirm increased connection errors and latencies for CloudWatch APIs. Some CloudWatch metrics are delayed. We are actively working to resolve the issue.</div><div><span class=""yellowfg"">12:30 PM PDT</span>&nbsp;We have identified the root cause of the increased connection errors and latencies for CloudWatch APIs, as well as the CloudWatch metric delays. We continue to work toward recovery.</div><div><span class=""yellowfg"">12:45 PM PDT</span>&nbsp;We are beginning to see recovery and continue to work toward full recovery. </div><div><span class=""yellowfg""> 1:03 PM PDT</span>&nbsp;Between 10:40 AM and 12:44 PM PDT we experienced increased connection errors and latencies for CloudWatch APIs. During this time CloudWatch alarms may have transitioned into ""INSUFFICIENT_DATA"" state if set on delayed metrics. The issue has been resolved and the service is operating normally. </div>",cloudwatch-us-west-2,2019-03-27 18:13:16,CloudWatch,Oregon,10:40 AM,12:44 PM,PDT,124.0,2019,3,27,2019-03-27,delayed metrics,"11:14 AM PDT -  We are investigating increased delays for CloudWatch metrics in the US-WEST-2 Region. CloudWatch alarms may transition into ""INSUFFICIENT_DATA"" state if set on delayed metrics.
11:33 AM PDT -  We can confirm increased connection errors and latencies for CloudWatch APIs. Some CloudWatch metrics are delayed. We are actively working to resolve the issue.
12:30 PM PDT -  We have identified the root cause of the increased connection errors and latencies for CloudWatch APIs, as well as the CloudWatch metric delays. We continue to work toward recovery.
12:45 PM PDT -  We are beginning to see recovery and continue to work toward full recovery. 
 1:03 PM PDT -  Between 10:40 AM and 12:44 PM PDT we experienced increased connection errors and latencies for CloudWatch APIs. During this time CloudWatch alarms may have transitioned into ""INSUFFICIENT_DATA"" state if set on delayed metrics. The issue has been resolved and the service is operating normally. "
131,AWS Certificate Manager (Oregon),[RESOLVED] Increased Error Rates,1553712991,1,,"<div><span class=""yellowfg"">11:57 AM PDT</span>&nbsp;We are investigating increased connection errors and latencies for the ACM APIs and ACM Management Console in the US-WEST-2 Region. </div><div><span class=""yellowfg"">12:34 PM PDT</span>&nbsp;We have identified the root cause of the increased connection errors and latencies for the ACM APIs and ACM Management Console in the US-WEST-2 Region. We are beginning to see recovery and continue to work toward full resolution.</div><div><span class=""yellowfg"">12:50 PM PDT</span>&nbsp;Between 10:40 AM and 12:40 PM PDT we experienced increased connection errors and latencies for the ACM APIs and ACM Management Console in the US-WEST-2 Region. The issue has been resolved and the service is operating normally. </div>",certificatemanager-us-west-2,2019-03-27 18:56:31,Certificate Manager,Oregon,10:40 AM,12:40 PM,PDT,120.0,2019,3,27,2019-03-27,increased error rates,"11:57 AM PDT -  We are investigating increased connection errors and latencies for the ACM APIs and ACM Management Console in the US-WEST-2 Region. 
12:34 PM PDT -  We have identified the root cause of the increased connection errors and latencies for the ACM APIs and ACM Management Console in the US-WEST-2 Region. We are beginning to see recovery and continue to work toward full resolution.
12:50 PM PDT -  Between 10:40 AM and 12:40 PM PDT we experienced increased connection errors and latencies for the ACM APIs and ACM Management Console in the US-WEST-2 Region. The issue has been resolved and the service is operating normally. "
132,Auto Scaling (Oregon),[RESOLVED] API Faults and Latencies,1553715184,1,,"<div><span class=""yellowfg"">12:33 PM PDT</span>&nbsp;We are investigating increased elevated faults and latencies across the DescribePolicies, PutScalingPolicy and RegisterScalableTarget APIs in the US-WEST-2 Region.</div><div><span class=""yellowfg"">12:54 PM PDT</span>&nbsp;Between 10:40 AM and 12:40 PM PDT we experienced increased elevated faults and latencies across the DescribePolicies, PutScalingPolicy and RegisterScalableTarget APIs in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",autoscaling-us-west-2,2019-03-27 19:33:04,Auto Scaling,Oregon,10:40 AM,12:40 PM,PDT,120.0,2019,3,27,2019-03-27,api faults and latencies,"12:33 PM PDT -  We are investigating increased elevated faults and latencies across the DescribePolicies, PutScalingPolicy and RegisterScalableTarget APIs in the US-WEST-2 Region.
12:54 PM PDT -  Between 10:40 AM and 12:40 PM PDT we experienced increased elevated faults and latencies across the DescribePolicies, PutScalingPolicy and RegisterScalableTarget APIs in the US-WEST-2 Region. The issue has been resolved and the service is operating normally."
133,AWS Lambda (Oregon),[RESOLVED] Increased API Error Rates,1553757070,1,,"<div><span class=""yellowfg"">12:11 AM PDT</span>&nbsp;We are investigating increased API error rates in the US-WEST-2 Region. </div><div><span class=""yellowfg"">12:52 AM PDT</span>&nbsp;We have identified the root cause of the issue causing increased API latencies and errors in the US-WEST-2 Region and continue to work toward resolution.</div><div><span class=""yellowfg""> 2:21 AM PDT</span>&nbsp;We continue to see recovery in error rates for AWS Lambda API requests in the US-WEST-2 Region. Some customers may still experience errors in new function creation, updating existing functions and console editing. We continue to work towards full resolution. </div><div><span class=""yellowfg""> 2:46 AM PDT</span>&nbsp;Between March 27 11:20 PM and March 28 2:35 AM PDT, AWS Lambda experienced API latencies and errors in the US-WEST-2 Region. The issue has been resolved and the service is operating normally. </div>",lambda-us-west-2,2019-03-28 07:11:10,Lambda,Oregon,11:20 PM,2:35 AM,PDT,195.0,2019,3,28,2019-03-28,increased api error rates,"12:11 AM PDT -  We are investigating increased API error rates in the US-WEST-2 Region. 
12:52 AM PDT -  We have identified the root cause of the issue causing increased API latencies and errors in the US-WEST-2 Region and continue to work toward resolution.
 2:21 AM PDT -  We continue to see recovery in error rates for AWS Lambda API requests in the US-WEST-2 Region. Some customers may still experience errors in new function creation, updating existing functions and console editing. We continue to work towards full resolution. 
 2:46 AM PDT -  Between March 27 11:20 PM and March 28 2:35 AM PDT, AWS Lambda experienced API latencies and errors in the US-WEST-2 Region. The issue has been resolved and the service is operating normally. "
134,Amazon API Gateway (Oregon),[RESOLVED] Increased Error Rates,1553757540,1,,"<div><span class=""yellowfg"">12:19 AM PDT</span>&nbsp;We are investigating increased error rates for Lambda integrations and Lambda Authorizers for invokes in the US-WEST-2 Region.</div><div><span class=""yellowfg"">12:58 AM PDT</span>&nbsp;We have identified the root cause of the issue causing increased error rates for Lambda integrations and Lambda Authorizers for invokes in the US-WEST-2 Region and continue to work toward resolution.</div><div><span class=""yellowfg""> 2:36 AM PDT</span>&nbsp;Between March 27 11:20 PM and March 28 2:23 AM PDT customers using Lambda with API Gateway for Integrations and Authorizers experienced elevated API error rates in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",apigateway-us-west-2,2019-03-28 07:19:00,API Gateway,Oregon,11:20 PM,2:23 AM,PDT,183.0,2019,3,28,2019-03-28,increased error rates,"12:19 AM PDT -  We are investigating increased error rates for Lambda integrations and Lambda Authorizers for invokes in the US-WEST-2 Region.
12:58 AM PDT -  We have identified the root cause of the issue causing increased error rates for Lambda integrations and Lambda Authorizers for invokes in the US-WEST-2 Region and continue to work toward resolution.
 2:36 AM PDT -  Between March 27 11:20 PM and March 28 2:23 AM PDT customers using Lambda with API Gateway for Integrations and Authorizers experienced elevated API error rates in the US-WEST-2 Region. The issue has been resolved and the service is operating normally."
135,AWS Resource Access Manager (Oregon),[RESOLVED] API Latencies and Error Rate,1553757596,1,,"<div><span class=""yellowfg"">12:20 AM PDT</span>&nbsp;We are investigating increased API latencies and error rates in the US-WEST-2 Region.</div><div><span class=""yellowfg"">12:55 AM PDT</span>&nbsp;We have identified the root cause of the issue causing increased API latencies and errors in the US-WEST-2 Region and continue to work toward resolution.</div><div><span class=""yellowfg""> 2:36 AM PDT</span>&nbsp;
Between March 27 11:15 PM and March 28 1:49 AM PDT, we experienced increased API error rates and latencies in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",ram-us-west-2,2019-03-28 07:19:56,Resource Access Manager,Oregon,11:15 PM,1:49 AM,PDT,154.0,2019,3,28,2019-03-28,api latencies and error rate,"12:20 AM PDT -  We are investigating increased API latencies and error rates in the US-WEST-2 Region.
12:55 AM PDT -  We have identified the root cause of the issue causing increased API latencies and errors in the US-WEST-2 Region and continue to work toward resolution.
 2:36 AM PDT -  Between March 27 11:15 PM and March 28 1:49 AM PDT, we experienced increased API error rates and latencies in the US-WEST-2 Region. The issue has been resolved and the service is operating normally."
136,AWS Batch (Oregon),[RESOLVED] Increased Error Rates,1553757962,1,,"<div><span class=""yellowfg"">12:26 AM PDT</span>&nbsp;We are investigating increased error rates for API calls in the US-WEST-2 Region. Compute Resource connectivity and running jobs are not affected.</div><div><span class=""yellowfg"">12:57 AM PDT</span>&nbsp;We have identified the root cause of increased error rates for API calls in the US-WEST-2 Region. Compute Resource connectivity and running jobs are not affected.</div><div><span class=""yellowfg""> 2:14 AM PDT</span>&nbsp;We have identified the root cause of increased error rates for API calls in the US-WEST-2 Region impacting new API calls. Compute Resource connectivity and running jobs are not affected. We continue to work towards recovery. </div><div><span class=""yellowfg""> 2:39 AM PDT</span>&nbsp;Between March 27 11:21 PM and March 28 2:18 AM PDT, AWS Batch experienced increased error rates for API calls in the US-WEST-2 Region. Compute Resource connectivity and running jobs were not affected. The issue has been resolved and the service is operating normally. </div>",batch-us-west-2,2019-03-28 07:26:02,Batch,Oregon,11:21 PM,2:18 AM,PDT,177.0,2019,3,28,2019-03-28,increased error rates,"12:26 AM PDT -  We are investigating increased error rates for API calls in the US-WEST-2 Region. Compute Resource connectivity and running jobs are not affected.
12:57 AM PDT -  We have identified the root cause of increased error rates for API calls in the US-WEST-2 Region. Compute Resource connectivity and running jobs are not affected.
 2:14 AM PDT -  We have identified the root cause of increased error rates for API calls in the US-WEST-2 Region impacting new API calls. Compute Resource connectivity and running jobs are not affected. We continue to work towards recovery. 
 2:39 AM PDT -  Between March 27 11:21 PM and March 28 2:18 AM PDT, AWS Batch experienced increased error rates for API calls in the US-WEST-2 Region. Compute Resource connectivity and running jobs were not affected. The issue has been resolved and the service is operating normally. "
137,AWS AppSync (Oregon),[RESOLVED] Increased Error Rates,1553758789,1,,"<div><span class=""yellowfg"">12:40 AM PDT</span>&nbsp;We are investigating increased API error rates in the US-WEST-2 Region. </div><div><span class=""yellowfg""> 1:14 AM PDT</span>&nbsp;We have identified the root cause of the issue causing increased API error rates in the US-WEST-2 Region and continue to work toward resolution. </div><div><span class=""yellowfg""> 2:40 AM PDT</span>&nbsp;Between March 27 11:20 PM and March 28 1:57 AM PDT, we experienced increased API error rates for the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",appsync-us-west-2,2019-03-28 07:39:49,AppSync,Oregon,11:20 PM,1:57 AM,PDT,157.0,2019,3,28,2019-03-28,increased error rates,"12:40 AM PDT -  We are investigating increased API error rates in the US-WEST-2 Region. 
 1:14 AM PDT -  We have identified the root cause of the issue causing increased API error rates in the US-WEST-2 Region and continue to work toward resolution. 
 2:40 AM PDT -  Between March 27 11:20 PM and March 28 1:57 AM PDT, we experienced increased API error rates for the US-WEST-2 Region. The issue has been resolved and the service is operating normally."
138,AWS Certificate Manager (N. Virginia),[RESOLVED] Increased API Error Rates,1553876764,2,,"<div><span class=""yellowfg""> 9:26 AM PDT</span>&nbsp;We are investigating increased API error rates in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 9:46 AM PDT</span>&nbsp;We continue to investigate increased API error rates in the US-EAST-1 Region.</div><div><span class=""yellowfg"">10:18 AM PDT</span>&nbsp;We have identified the root cause of the increased API error rates in the US-EAST-1 Region. We continue to work toward resolution.</div><div><span class=""yellowfg"">11:28 AM PDT</span>&nbsp;We are beginning to see recovery for the increased API error rates in the US-EAST-1 Region. We continue to work toward full recovery.</div><div><span class=""yellowfg"">11:53 AM PDT</span>&nbsp;Between 8:41 AM and 11:10 AM PDT we experienced increased API error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",certificatemanager-us-east-1,2019-03-29 16:26:04,Certificate Manager,N. Virginia,8:41 AM,11:10 AM,PDT,149.0,2019,3,29,2019-03-29,increased api error rates," 9:26 AM PDT -  We are investigating increased API error rates in the US-EAST-1 Region.
 9:46 AM PDT -  We continue to investigate increased API error rates in the US-EAST-1 Region.
10:18 AM PDT -  We have identified the root cause of the increased API error rates in the US-EAST-1 Region. We continue to work toward resolution.
11:28 AM PDT -  We are beginning to see recovery for the increased API error rates in the US-EAST-1 Region. We continue to work toward full recovery.
11:53 AM PDT -  Between 8:41 AM and 11:10 AM PDT we experienced increased API error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
139,Amazon CloudFront,[RESOLVED] Change Propagation Delays,1553885279,1,,"<div><span class=""yellowfg"">11:48 AM PDT</span>&nbsp;We’re investigating longer than usual propagation times for changes to CloudFront configurations. End-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=""yellowfg"">12:52 PM PDT</span>&nbsp;We have identified the root cause of the longer than usual propagation times for changes to CloudFront configurations. We continue to work toward resolution. </div><div><span class=""yellowfg""> 1:18 PM PDT</span>&nbsp;Between 8:50 AM and 12:59 PM PDT, we experienced longer than usual propagation times for changes to CloudFront configurations. The issue has been resolved and the service is operating normally.</div>",cloudfront,2019-03-29 18:47:59,CloudFront,Global,8:50 AM,12:59 PM,PDT,249.0,2019,3,29,2019-03-29,change propagation delays,"11:48 AM PDT -  We’re investigating longer than usual propagation times for changes to CloudFront configurations. End-user requests for content from our edge locations are not affected by this issue and are being served normally.
12:52 PM PDT -  We have identified the root cause of the longer than usual propagation times for changes to CloudFront configurations. We continue to work toward resolution. 
 1:18 PM PDT -  Between 8:50 AM and 12:59 PM PDT, we experienced longer than usual propagation times for changes to CloudFront configurations. The issue has been resolved and the service is operating normally."
140,AWS Lambda (Ireland),[RESOLVED] Increased Async API Latencies,1554044749,1,,"<div><span class=""yellowfg""> 8:06 AM PDT</span>&nbsp;We are investigating increased Async API latencies in the EU-WEST-1 Region.</div><div><span class=""yellowfg""> 8:41 AM PDT</span>&nbsp;We have identified the root cause of increased Async API latencies in the EU-WEST-1 Region and continue to work toward resolution.</div><div><span class=""yellowfg""> 9:32 AM PDT</span>&nbsp;Between 6:59 AM and 8:56 AM PDT we experienced increased latencies for the Lambda Async API in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",lambda-eu-west-1,2019-03-31 15:05:49,Lambda,Ireland,6:59 AM,8:56 AM,PDT,117.0,2019,3,31,2019-03-31,increased async api latencies," 8:06 AM PDT -  We are investigating increased Async API latencies in the EU-WEST-1 Region.
 8:41 AM PDT -  We have identified the root cause of increased Async API latencies in the EU-WEST-1 Region and continue to work toward resolution.
 9:32 AM PDT -  Between 6:59 AM and 8:56 AM PDT we experienced increased latencies for the Lambda Async API in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally."
141,AWS IoT Core (Ireland),[RESOLVED] Increased API Errors and Latencies,1554201606,2,,"<div><span class=""yellowfg""> 3:40 AM PDT</span>&nbsp;We are investigating increased API error rates when establishing new connections to AWS IoT Core in the EU-WEST-1 Region.</div><div><span class=""yellowfg""> 4:23 AM PDT</span>&nbsp;Between 1:52 AM and 3:35 AM PDT we experienced increased API error rates and latencies in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",awsiot-eu-west-1,2019-04-02 10:40:06,IoT Core,Ireland,1:52 AM,3:35 AM,PDT,103.0,2019,4,2,2019-04-02,increased api errors and latencies," 3:40 AM PDT -  We are investigating increased API error rates when establishing new connections to AWS IoT Core in the EU-WEST-1 Region.
 4:23 AM PDT -  Between 1:52 AM and 3:35 AM PDT we experienced increased API error rates and latencies in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally."
142,Amazon Chime,[RESOLVED] Authentication Availability,1554263486,1,,"<div><span class=""yellowfg""> 8:51 PM PDT</span>&nbsp;We are investigating availability issues with authenticating using Active Directory in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 9:27 PM PDT</span>&nbsp;Between 6:30 PM and 9:10 PM PDT we experienced increased Active Directory authentication failures in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",chime,2019-04-03 03:51:26,Chime,Global,6:30 PM,9:10 PM,PDT,160.0,2019,4,3,2019-04-03,authentication availability," 8:51 PM PDT -  We are investigating availability issues with authenticating using Active Directory in the US-EAST-1 Region.
 9:27 PM PDT -  Between 6:30 PM and 9:10 PM PDT we experienced increased Active Directory authentication failures in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. "
143,Amazon Elastic Compute Cloud (US-East),[RESOLVED] Network Connectivity ,1554336479,1,,"<div><span class=""yellowfg""> 5:08 PM PDT</span>&nbsp;We are investigating internet connectivity issues in the US-GOV-EAST-1 Region.</div><div><span class=""yellowfg""> 5:27 PM PDT</span>&nbsp;We can confirm Internet and EC2 Public IP address connectivity issues for newly launched EC2 instances within the US-GOV-EAST-1 Region. Connectivity for existing instances is not affected.</div><div><span class=""yellowfg""> 5:59 PM PDT</span>&nbsp;Between 3:29 PM and 5:50 PM PDT some newly launched instances experienced Internet and EC2 Public IP address connectivity issues in the US-GOV-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",ec2-us-gov-east-1,2019-04-04 00:07:59,Elastic Compute Cloud,US-East,3:29 PM,5:50 PM,PDT,141.0,2019,4,4,2019-04-04,network connectivity," 5:08 PM PDT -  We are investigating internet connectivity issues in the US-GOV-EAST-1 Region.
 5:27 PM PDT -  We can confirm Internet and EC2 Public IP address connectivity issues for newly launched EC2 instances within the US-GOV-EAST-1 Region. Connectivity for existing instances is not affected.
 5:59 PM PDT -  Between 3:29 PM and 5:50 PM PDT some newly launched instances experienced Internet and EC2 Public IP address connectivity issues in the US-GOV-EAST-1 Region. The issue has been resolved and the service is operating normally."
144,AWS CodeBuild (N. Virginia),[RESOLVED] Increased Error Rates ,1554742713,1,,"<div><span class=""yellowfg""> 9:58 AM PDT</span>&nbsp;We are investigating increased API and Build error rates in the US-EAST-1 Region. </div><div><span class=""yellowfg"">10:23 AM PDT</span>&nbsp;We have identified the cause of the increased error rates for the CodeBuild Management Console and Build APIs in the US-EAST-1 Region and continue to work towards resolution. </div><div><span class=""yellowfg"">10:34 AM PDT</span>&nbsp;Between 8:35 AM and 10:05 AM PDT we experienced increased error rates for the CodeBuild Management Console and Build APIs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",codebuild-us-east-1,2019-04-08 16:58:33,CodeBuild,N. Virginia,8:35 AM,10:05 AM,PDT,90.0,2019,4,8,2019-04-08,increased error rates," 9:58 AM PDT -  We are investigating increased API and Build error rates in the US-EAST-1 Region. 
10:23 AM PDT -  We have identified the cause of the increased error rates for the CodeBuild Management Console and Build APIs in the US-EAST-1 Region and continue to work towards resolution. 
10:34 AM PDT -  Between 8:35 AM and 10:05 AM PDT we experienced increased error rates for the CodeBuild Management Console and Build APIs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. "
145,Amazon Elastic Load Balancing (N. Virginia),[RESOLVED] Increased API Error Rates,1554964842,1,,"<div><span class=""yellowfg"">11:40 PM PDT</span>&nbsp;We are investigating increased error rates and latencies for the ELB APIs in the US-EAST-1 Region.</div><div><span class=""yellowfg"">Apr 11, 12:07 AM PDT</span>&nbsp;Between 11:13 PM and 11:54 PM PDT we experienced increased error rates and latencies for the ELB APIs in the US-EAST-1 Region. Connectivity to existing load balancers was not affected. The issue has been resolved and the service is operating normally.</div>",elb-us-east-1,2019-04-11 06:40:42,Elastic Load Balancing,N. Virginia,11:13 PM,11:54 PM,PDT,41.0,2019,4,11,2019-04-11,increased api error rates,"11:40 PM PDT -  We are investigating increased error rates and latencies for the ELB APIs in the US-EAST-1 Region.
Apr 11, 12:07 AM PDT -  Between 11:13 PM and 11:54 PM PDT we experienced increased error rates and latencies for the ELB APIs in the US-EAST-1 Region. Connectivity to existing load balancers was not affected. The issue has been resolved and the service is operating normally."
146,Amazon Route 53,[RESOLVED] Elevated Route 53 API error rates ,1555178246,1,,"<div><span class=""yellowfg"">10:57 AM PDT</span>&nbsp;We are currently investigating elevated error rates for some Route 53 API calls in the US-EAST-1 Region. There is no impact to answering DNS queries and they continue to work normally.</div><div><span class=""yellowfg"">11:22 AM PDT</span>&nbsp;Between 10:13 AM and 10:58 AM PDT we experienced intermittent elevated error rates for some Route 53 API calls in the US-EAST-1 Region. There was no impact to answering DNS queries throughout this time. The issue has been resolved and the service is operating normally</div>",route53,2019-04-13 17:57:26,Route 53,Global,10:13 AM,10:58 AM,PDT,45.0,2019,4,13,2019-04-13,elevated route 53 api error rates,"10:57 AM PDT -  We are currently investigating elevated error rates for some Route 53 API calls in the US-EAST-1 Region. There is no impact to answering DNS queries and they continue to work normally.
11:22 AM PDT -  Between 10:13 AM and 10:58 AM PDT we experienced intermittent elevated error rates for some Route 53 API calls in the US-EAST-1 Region. There was no impact to answering DNS queries throughout this time. The issue has been resolved and the service is operating normally"
147,Amazon Elastic Compute Cloud (Sao Paulo),[RESOLVED] Increased API Error Rates,1555527672,1,,"<div><span class=""yellowfg"">12:01 PM PDT</span>&nbsp;We are investigating increased RunInstance API error rates in the SA-EAST-1 Region.</div><div><span class=""yellowfg"">12:21 PM PDT</span>&nbsp;Between 11:30 AM and 12:09 PM PDT we experienced increased RunInstance API error rates and latencies in the SA-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",ec2-sa-east-1,2019-04-17 19:01:12,Elastic Compute Cloud,Sao Paulo,11:30 AM,12:09 PM,PDT,39.0,2019,4,17,2019-04-17,increased api error rates,"12:01 PM PDT -  We are investigating increased RunInstance API error rates in the SA-EAST-1 Region.
12:21 PM PDT -  Between 11:30 AM and 12:09 PM PDT we experienced increased RunInstance API error rates and latencies in the SA-EAST-1 Region. The issue has been resolved and the service is operating normally."
148,AWS Billing Console,[RESOLVED] Increased Billing Console Error Rates,1555578361,1,,"<div><span class=""yellowfg""> 2:06 AM PDT</span>&nbsp;We are investigating increased error rates in the Billing Console.</div><div><span class=""yellowfg""> 2:56 AM PDT</span>&nbsp;We continue to investigate intermittent availability and increased error rates in the Billing Console.</div><div><span class=""yellowfg""> 3:53 AM PDT</span>&nbsp;We continue to investigate intermittent availability and increased error rates in the Billing Console and are working towards resolution. </div><div><span class=""yellowfg""> 4:45 AM PDT</span>&nbsp;We have identified the cause of the intermittent availability issues and increased error rates in the Billing Console and continue working towards resolution.</div><div><span class=""yellowfg""> 5:17 AM PDT</span>&nbsp;Between 2:06 AM and 5:00 AM PDT we experienced intermittent availability issues and increased error rates in the Billing Console. The issue has been resolved and the service is operating normally.</div>",billingconsole,2019-04-18 09:06:01,Billing Console,Global,2:06 AM,5:00 AM,PDT,174.0,2019,4,18,2019-04-18,increased billing console error rates," 2:06 AM PDT -  We are investigating increased error rates in the Billing Console.
 2:56 AM PDT -  We continue to investigate intermittent availability and increased error rates in the Billing Console.
 3:53 AM PDT -  We continue to investigate intermittent availability and increased error rates in the Billing Console and are working towards resolution. 
 4:45 AM PDT -  We have identified the cause of the intermittent availability issues and increased error rates in the Billing Console and continue working towards resolution.
 5:17 AM PDT -  Between 2:06 AM and 5:00 AM PDT we experienced intermittent availability issues and increased error rates in the Billing Console. The issue has been resolved and the service is operating normally."
149,AWS Management Console (US-East),Increased Console Error Rates,1556210519,1,,"<div><span class=""yellowfg""> 3:20 AM PDT</span>&nbsp;We are investigating increased error rates when loading the AWS Management Console.</div>",management-console-us-gov-east-1,2019-04-25 16:41:59,Management Console,US-East,3:20 AM,3:20 AM,PDT,0.0,2019,4,25,2019-04-25,increased console error rates, 3:20 AM PDT -  We are investigating increased error rates when loading the AWS Management Console.
150,AWS Management Console,[RESOLVED] Increased Console Error Rates,1556210754,2,,"<div><span class=""yellowfg""> 9:50 AM PDT</span>&nbsp;We are currently investigating increased errors for loading the AWS Management Console. Alternate link is available at: <a href=""https://us-west-2.console.aws.amazon.com/ec2/v2/home"">https://us-west-2.console.aws.amazon.com/ec2/v2/home</a></div><div><span class=""yellowfg"">10:10 AM PDT</span>&nbsp;We can confirm impact to the AWS Management Console in US-EAST-1 specific to the /home link. AWS services are not impacted, and access to service-specific consoles is working normally. Please use alternate link: <a href=""https://us-west-2.console.aws.amazon.com/ec2/v2/home"">https://us-west-2.console.aws.amazon.com/ec2/v2/home</a></div><div><span class=""yellowfg"">11:28 AM PDT</span>&nbsp;We continue to work towards resolution in US-EAST-1 for issues specific to the /home Management Console link. AWS services are not impacted, and access to service-specific consoles is working normally. Please use alternate link: <a href=""https://us-east-1.console.aws.amazon.com/ec2/v2/home"">https://us-east-1.console.aws.amazon.com/ec2/v2/home</a> and select a service from the Services menu above.</div><div><span class=""yellowfg"">12:18 PM PDT</span>&nbsp;Between 9:11 AM and 12:13 PM PDT we experienced increased error rates for the AWS Management Console in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",management-console,2019-04-25 16:45:54,Management Console,Global,9:11 AM,12:13 PM,PDT,182.0,2019,4,25,2019-04-25,increased console error rates," 9:50 AM PDT -  We are currently investigating increased errors for loading the AWS Management Console. Alternate link is available at: https://us-west-2.console.aws.amazon.com/ec2/v2/home
10:10 AM PDT -  We can confirm impact to the AWS Management Console in US-EAST-1 specific to the /home link. AWS services are not impacted, and access to service-specific consoles is working normally. Please use alternate link: https://us-west-2.console.aws.amazon.com/ec2/v2/home
11:28 AM PDT -  We continue to work towards resolution in US-EAST-1 for issues specific to the /home Management Console link. AWS services are not impacted, and access to service-specific consoles is working normally. Please use alternate link: https://us-east-1.console.aws.amazon.com/ec2/v2/home and select a service from the Services menu above.
12:18 PM PDT -  Between 9:11 AM and 12:13 PM PDT we experienced increased error rates for the AWS Management Console in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. "
151,AWS Single Sign-On (N. Virginia),[RESOLVED] Single Sign-On,1556215997,1,,"<div><span class=""yellowfg"">11:13 AM PDT</span>&nbsp;We can confirm impact to the AWS SSO user portal specific to single-sign on to the AWS Management Console. SAML assertions are not impacted but users may be directed to a website unavailable page when authenticating with SAML. Please use alternate link: <a href=""https://us-west-2.console.aws.amazon.com/ec2/v2/home"">https://us-west-2.console.aws.amazon.com/ec2/v2/home</a> after authenticating. There is no impact to application sign-in.</div><div><span class=""yellowfg"">11:43 AM PDT</span>&nbsp;We continue to work towards resolution for issues specific to the AWS Management Console. For SSO customers who are seeing an error message while attempting to sign into AWS console through the AWS SSO User Portal, clicking on “login again” provides access</div><div><span class=""yellowfg"">12:18 PM PDT</span>&nbsp;Between 9:11 AM and 12:13 PM PDT we experienced increased error rates for the AWS Management Console in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",sso-us-east-1,2019-04-25 18:13:17,Single Sign-On,N. Virginia,9:11 AM,12:13 PM,PDT,182.0,2019,4,25,2019-04-25,single sign-on,"11:13 AM PDT -  We can confirm impact to the AWS SSO user portal specific to single-sign on to the AWS Management Console. SAML assertions are not impacted but users may be directed to a website unavailable page when authenticating with SAML. Please use alternate link: https://us-west-2.console.aws.amazon.com/ec2/v2/home after authenticating. There is no impact to application sign-in.
11:43 AM PDT -  We continue to work towards resolution for issues specific to the AWS Management Console. For SSO customers who are seeing an error message while attempting to sign into AWS console through the AWS SSO User Portal, clicking on “login again” provides access
12:18 PM PDT -  Between 9:11 AM and 12:13 PM PDT we experienced increased error rates for the AWS Management Console in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. "
152,Amazon Simple Storage Service (N. Virginia),[RESOLVED]  Increased Error Rates ,1556557390,1,,"<div><span class=""yellowfg"">10:03 AM PDT</span>&nbsp;We are investigating increased error rates for Amazon S3 GET and PUT requests in the US-EAST-1 Region.</div><div><span class=""yellowfg"">10:32 AM PDT</span>&nbsp;We have identified the cause of the increased error rates in the US-EAST-1 Region and are working towards resolution.</div><div><span class=""yellowfg"">10:42 AM PDT</span>&nbsp;Between 9:18 AM and 10:18 AM PDT we experienced slightly increased GET and PUT error rates for Amazon S3 requests in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",s3-us-standard,2019-04-29 17:03:10,Simple Storage Service,N. Virginia,9:18 AM,10:18 AM,PDT,60.0,2019,4,29,2019-04-29,increased error rates,"10:03 AM PDT -  We are investigating increased error rates for Amazon S3 GET and PUT requests in the US-EAST-1 Region.
10:32 AM PDT -  We have identified the cause of the increased error rates in the US-EAST-1 Region and are working towards resolution.
10:42 AM PDT -  Between 9:18 AM and 10:18 AM PDT we experienced slightly increased GET and PUT error rates for Amazon S3 requests in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
153,Amazon CloudFront,[RESOLVED] Increased Error Rates,1556576555,1,,"<div><span class=""yellowfg""> 3:22 PM PDT</span>&nbsp;We are investigating increased error rates for requests served by certain edge locations in South East Asia.</div><div><span class=""yellowfg""> 3:59 PM PDT</span>&nbsp;Between 2:16 PM and 3:43 PM PDT we experienced increased error rates for requests served by edge locations in South East Asia. The issue has been resolved and the service is operating normally.</div>",cloudfront,2019-04-29 22:22:35,CloudFront,Global,2:16 PM,3:43 PM,PDT,87.0,2019,4,29,2019-04-29,increased error rates," 3:22 PM PDT -  We are investigating increased error rates for requests served by certain edge locations in South East Asia.
 3:59 PM PDT -  Between 2:16 PM and 3:43 PM PDT we experienced increased error rates for requests served by edge locations in South East Asia. The issue has been resolved and the service is operating normally."
154,Amazon Elastic Compute Cloud (Sydney),[RESOLVED] Network Connectivity,1556589414,1,,"<div><span class=""yellowfg""> 6:57 PM PDT</span>&nbsp;We are investigating network connectivity issues for some non-Nitro instances in a single Availability Zone in the AP-SOUTHEAST-2 Region.</div><div><span class=""yellowfg""> 7:35 PM PDT</span>&nbsp;We can confirm network connectivity issues for some non-Nitro instances in a single Availability Zone in the AP-SOUTHEAST-2 Region. Connectivity for affected instances can be restored using a stop/start command from the EC2 Management Console or EC2 API. We continue to work towards restoring network connectivity for the affected instances.</div><div><span class=""yellowfg""> 8:36 PM PDT</span>&nbsp;We have resolved the network connectivity issue affecting some non-Nitro instances in a single Availability Zone in the AP-SOUTHEAST-2 Region. For instances that are still experiencing connectivity issues, we recommend issuing a reboot or stop/start command from the EC2 Management Console or EC2 API. The issue has been resolved and the service is operating normally.</div>",ec2-ap-southeast-2,2019-04-30 01:56:54,Elastic Compute Cloud,Sydney,6:57 PM,8:36 PM,PDT,99.0,2019,4,30,2019-04-30,network connectivity," 6:57 PM PDT -  We are investigating network connectivity issues for some non-Nitro instances in a single Availability Zone in the AP-SOUTHEAST-2 Region.
 7:35 PM PDT -  We can confirm network connectivity issues for some non-Nitro instances in a single Availability Zone in the AP-SOUTHEAST-2 Region. Connectivity for affected instances can be restored using a stop/start command from the EC2 Management Console or EC2 API. We continue to work towards restoring network connectivity for the affected instances.
 8:36 PM PDT -  We have resolved the network connectivity issue affecting some non-Nitro instances in a single Availability Zone in the AP-SOUTHEAST-2 Region. For instances that are still experiencing connectivity issues, we recommend issuing a reboot or stop/start command from the EC2 Management Console or EC2 API. The issue has been resolved and the service is operating normally."
155,Amazon Elastic Compute Cloud (Hong Kong),[RESOLVED] EC2 Recursive DNS Resolution Issues,1556669136,1,,"<div><span class=""yellowfg""> 5:06 PM PDT</span>&nbsp;We are investigating DNS resolution issues in a single Availability Zone in the AP-EAST-1 Region.</div><div><span class=""yellowfg""> 5:23 PM PDT</span>&nbsp;We are investigating DNS resolution issues, increased API error rates, and error rates for new launches in the AP-EAST-1 Region.</div><div><span class=""yellowfg""> 6:04 PM PDT</span>&nbsp;Between 4:39 PM and 5:50 PM PDT we experienced DNS resolution issues, increased API error rates, impaired instances, and error rates for new launches in the AP-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",ec2-ap-east-1,2019-05-01 00:05:36,Elastic Compute Cloud,Hong Kong,4:39 PM,5:50 PM,PDT,71.0,2019,5,1,2019-05-01,ec2 recursive dns resolution issues," 5:06 PM PDT -  We are investigating DNS resolution issues in a single Availability Zone in the AP-EAST-1 Region.
 5:23 PM PDT -  We are investigating DNS resolution issues, increased API error rates, and error rates for new launches in the AP-EAST-1 Region.
 6:04 PM PDT -  Between 4:39 PM and 5:50 PM PDT we experienced DNS resolution issues, increased API error rates, impaired instances, and error rates for new launches in the AP-EAST-1 Region. The issue has been resolved and the service is operating normally."
156,Amazon Simple Queue Service (Ohio),[RESOLVED]  Increased Error Rates ,1556905071,1,,"<div><span class=""yellowfg"">10:38 AM PDT</span>&nbsp;We are investigating increased error rates in the US-EAST-2 Region.</div><div><span class=""yellowfg"">11:18 AM PDT</span>&nbsp;Between 10:14 AM and 11:09 AM PDT, we experienced elevated API error rates in the US-EAST-2 Region. We have resolved the issue and the service is operating normally.</div>",sqs-us-east-2,2019-05-03 17:37:51,Simple Queue Service,Ohio,10:14 AM,11:09 AM,PDT,55.0,2019,5,3,2019-05-03,increased error rates,"10:38 AM PDT -  We are investigating increased error rates in the US-EAST-2 Region.
11:18 AM PDT -  Between 10:14 AM and 11:09 AM PDT, we experienced elevated API error rates in the US-EAST-2 Region. We have resolved the issue and the service is operating normally."
157,AWS Lambda (Ohio),[RESOLVED] Increased Error Rates,1556906015,1,,"<div><span class=""yellowfg"">10:53 AM PDT</span>&nbsp;We are investigating increased error rates and latency in the US-EAST-2 Region.</div><div><span class=""yellowfg"">11:54 AM PDT</span>&nbsp;We have identified the cause of increased error rates and latency in the US-EAST-2 Region and are working towards resolution. </div><div><span class=""yellowfg"">12:28 PM PDT</span>&nbsp;Between 10:14 AM and 12:25 PM PDT, we experienced elevated API error rates in the US-EAST-2 Region. We have resolved the issue and the service is operating normally.</div>",lambda-us-east-2,2019-05-03 17:53:35,Lambda,Ohio,10:14 AM,12:25 PM,PDT,131.0,2019,5,3,2019-05-03,increased error rates,"10:53 AM PDT -  We are investigating increased error rates and latency in the US-EAST-2 Region.
11:54 AM PDT -  We have identified the cause of increased error rates and latency in the US-EAST-2 Region and are working towards resolution. 
12:28 PM PDT -  Between 10:14 AM and 12:25 PM PDT, we experienced elevated API error rates in the US-EAST-2 Region. We have resolved the issue and the service is operating normally."
158,Amazon CloudWatch (Ohio),[RESOLVED] Increased Error Rates,1556906325,1,,"<div><span class=""yellowfg"">10:58 AM PDT</span>&nbsp;We are investigating increased error rates and delays for CloudWatch Logs Insights queries in the US-EAST-2 Region.</div><div><span class=""yellowfg"">11:15 AM PDT</span>&nbsp;Between 10:14 AM and 11:09 AM PDT, we experienced elevated error rates and delays when running CloudWatch Logs Insights queries in the US-EAST-2 Region. We have resolved the issue and the service is operating normally.</div>",cloudwatch-us-east-2,2019-05-03 17:58:45,CloudWatch,Ohio,10:14 AM,11:09 AM,PDT,55.0,2019,5,3,2019-05-03,increased error rates,"10:58 AM PDT -  We are investigating increased error rates and delays for CloudWatch Logs Insights queries in the US-EAST-2 Region.
11:15 AM PDT -  Between 10:14 AM and 11:09 AM PDT, we experienced elevated error rates and delays when running CloudWatch Logs Insights queries in the US-EAST-2 Region. We have resolved the issue and the service is operating normally."
159,AWS IoT Core (Ohio),[RESOLVED] Increased Error Rates,1556906633,1,,"<div><span class=""yellowfg"">11:04 AM PDT</span>&nbsp;AWS IoT Core is investigating increased error rates and latency in the US-EAST-2 Region.</div><div><span class=""yellowfg"">11:24 AM PDT</span>&nbsp;Between 10:14 AM and 11:09 AM PDT, we experienced elevated error rates and increased latency when trying to Connect to and Publish Messages on AWS IoT Core in the US-EAST-2 Region. We have resolved the issue and the service is operating normally.</div>",awsiot-us-east-2,2019-05-03 18:03:53,IoT Core,Ohio,10:14 AM,11:09 AM,PDT,55.0,2019,5,3,2019-05-03,increased error rates,"11:04 AM PDT -  AWS IoT Core is investigating increased error rates and latency in the US-EAST-2 Region.
11:24 AM PDT -  Between 10:14 AM and 11:09 AM PDT, we experienced elevated error rates and increased latency when trying to Connect to and Publish Messages on AWS IoT Core in the US-EAST-2 Region. We have resolved the issue and the service is operating normally."
160,AWS Elastic Beanstalk (Ireland),[RESOLVED] Increased API Error Rates ,1557476843,1,,"<div><span class=""yellowfg""> 1:27 AM PDT</span>&nbsp;We are currently investigating increased error rates for the Elastic Beanstalk APIs in the EU-WEST-1 Region.</div><div><span class=""yellowfg""> 2:15 AM PDT</span>&nbsp;We have identified the root cause of increased error rates to Elastic Beanstalk APIs in the EU-WEST-1 Region and continue to work towards resolution.</div><div><span class=""yellowfg""> 2:27 AM PDT</span>&nbsp;Between 12:43 AM and 1:24 AM PDT, we experienced increased error rates in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",elasticbeanstalk-eu-west-1,2019-05-10 08:27:23,Elastic Beanstalk,Ireland,12:43 AM,1:24 AM,PDT,41.0,2019,5,10,2019-05-10,increased api error rates," 1:27 AM PDT -  We are currently investigating increased error rates for the Elastic Beanstalk APIs in the EU-WEST-1 Region.
 2:15 AM PDT -  We have identified the root cause of increased error rates to Elastic Beanstalk APIs in the EU-WEST-1 Region and continue to work towards resolution.
 2:27 AM PDT -  Between 12:43 AM and 1:24 AM PDT, we experienced increased error rates in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally."
161,Amazon Relational Database Service (Ireland),[RESOLVED] Increased API Error Rates and Connectivity Issues,1557477953,1,,"<div><span class=""yellowfg""> 1:46 AM PDT</span>&nbsp;We are investigating increased API error rates in the EU-WEST-1 Region.</div><div><span class=""yellowfg""> 2:14 AM PDT</span>&nbsp;We are investigating increased API error rates and connectivity issues for some Aurora MySQL instances in the EU-WEST-1 Region.</div><div><span class=""yellowfg""> 2:34 AM PDT</span>&nbsp;We have identified the root cause of increased API error rates and connectivity issues for some Aurora MySQL instances in the EU-WEST-1 Region and continue to work towards resolution.</div><div><span class=""yellowfg""> 4:09 AM PDT</span>&nbsp;Beginning at 12:43 AM PDT we experienced increased API error rates and connectivity issues for some Aurora MySQL instances in the EU-WEST-1 Region. Elevated API error rates were resolved at 1:33 AM PDT, at which point some existing clusters continued to experience connectivity issues. As of 3:50 AM PDT both issues have been resolved and the service is operating normally.</div>",rds-eu-west-1,2019-05-10 08:45:53,Relational Database Service,Ireland,12:43 AM,3:50 AM,PDT,187.0,2019,5,10,2019-05-10,increased api error rates and connectivity issues," 1:46 AM PDT -  We are investigating increased API error rates in the EU-WEST-1 Region.
 2:14 AM PDT -  We are investigating increased API error rates and connectivity issues for some Aurora MySQL instances in the EU-WEST-1 Region.
 2:34 AM PDT -  We have identified the root cause of increased API error rates and connectivity issues for some Aurora MySQL instances in the EU-WEST-1 Region and continue to work towards resolution.
 4:09 AM PDT -  Beginning at 12:43 AM PDT we experienced increased API error rates and connectivity issues for some Aurora MySQL instances in the EU-WEST-1 Region. Elevated API error rates were resolved at 1:33 AM PDT, at which point some existing clusters continued to experience connectivity issues. As of 3:50 AM PDT both issues have been resolved and the service is operating normally."
162,Amazon Route 53,[RESOLVED] Change Propagation Delays & API Errors,1557512464,2,,"<div><span class=""yellowfg"">11:21 AM PDT</span>&nbsp;We are investigating increased propagation times of DNS edits to public and private Route 53 hosted zones, and increased failures for Delete Hosted Zone, Associate Private Hosted Zone and Disassociate Private Hosted Zone API and Console operations. Queries to existing DNS records are not affected by this issue.</div><div><span class=""yellowfg"">11:28 AM PDT</span>&nbsp;We are investigating increased propagation times of DNS edits to public and private Route 53 hosted zones, and increased failures for Create Hosted Zone, Delete Hosted Zone, Associate Private Hosted Zone and Disassociate Private Hosted Zone API and Console operations. Queries to existing DNS records are not affected by this issue.</div><div><span class=""yellowfg"">12:06 PM PDT</span>&nbsp;We continue to investigate increased propagation times of DNS edits to public and private Route 53 hosted zones, and increased failures for Create Hosted Zone, Delete Hosted Zone, Associate Private Hosted Zone and Disassociate Private Hosted Zone API and Console operations. Queries to existing DNS records are not affected by this issue.</div><div><span class=""yellowfg"">12:35 PM PDT</span>&nbsp;We have identified the root cause and continue to work toward recovery. This may also impact provisioning workflows for other AWS services that use Route 53 for their DNS names. Queries to existing DNS records are not affected by this issue.</div><div><span class=""yellowfg""> 2:01 PM PDT</span>&nbsp;DNS updates are being accepted by the Route 53 API and Console, however an error is preventing the updates from being propagated from the Route53 API system to the live DNS servers. We have identified the root cause of this propagation error and are actively working towards resolution. This issue is impacting provisioning workflows for other AWS services that use Route 53 for their DNS names. Queries to DNS records present at the beginning of this issue are not affected and are being answered normally.</div><div><span class=""yellowfg""> 4:00 PM PDT</span>&nbsp;Some DNS changes are now propagating to the live DNS servers.  We continue to work towards full recovery.</div><div><span class=""yellowfg""> 5:18 PM PDT</span>&nbsp;We are observing steady recovery, with the oldest changes propagating first. We are continuing to work through the backlog of queued changes.</div><div><span class=""yellowfg""> 6:30 PM PDT</span>&nbsp;The backlog of queued changes continues to propagate. We have taken steps to accelerate recovery.</div><div><span class=""yellowfg""> 7:35 PM PDT</span>&nbsp;We have now propagated more than half of the backlog of queued changes and are continuing to work on the remainder.</div><div><span class=""yellowfg""> 8:23 PM PDT</span>&nbsp;We have processed the majority of the backlog of queued changes for Route 53 Public DNS. Private DNS changes are still experiencing propagation delays. Until we have fully recovered, customers may receive an error for the following APIs: CreateHostedZone, DeleteHostedZone, AssociateVPCWithHostedZone, and DisassociateVPCFromHostedZone. We continue to work towards full recovery.</div><div><span class=""yellowfg"">10:08 PM PDT</span>&nbsp;As of 8:34 PM PDT, we have resolved the issue resulting in Route 53 Public DNS propagation delays. At this time, Route 53 Private DNS changes are still experiencing propagation delays. Until we have fully recovered, customers will continue to receive errors for the following APIs: CreateHostedZone, DeleteHostedZone, AssociateVPCWithHostedZone, and DisassociateVPCFromHostedZone.</div><div><span class=""yellowfg"">11:51 PM PDT</span>&nbsp;We continue to make progress towards restoring propagation for Route 53 Private DNS changes and resolving increased API errors for CreateHostedZone, DeleteHostedZone, AssociateVPCWithHostedZone, and DisassociateVPCFromHostedZone Route 53 API calls. A majority of our hosts have processed all updates, and we are working to accelerate recovery of the remaining hosts.</div><div><span class=""yellowfg"">May 11,  1:41 AM PDT</span>&nbsp;We have restored propagation for Route 53 Private DNS changes. We continue to work on resolving increased API errors for CreateHostedZone, DeleteHostedZone, AssociateVPCWithHostedZone, and DisassociateVPCFromHostedZone Route 53 API calls.</div><div><span class=""yellowfg"">May 11,  2:39 AM PDT</span>&nbsp;Between May 10 10:50 AM and May 11 2:18 AM PDT we experienced increased change propagation latency and elevated error rates for CreateHostedZone, DeleteHostedZone, AssociateVPCWithHostedZone and DisassociateVPCFromHostedZone calls to the Route 53 API. During this entire duration there were no issues serving DNS queries. The issue has been resolved and the service is now operating normally.</div>",route53,2019-05-10 18:21:04,Route 53,Global,10:50 AM,2:18 AM,PDT,928.0,2019,5,10,2019-05-10,change propagation delays & api errors,"11:21 AM PDT -  We are investigating increased propagation times of DNS edits to public and private Route 53 hosted zones, and increased failures for Delete Hosted Zone, Associate Private Hosted Zone and Disassociate Private Hosted Zone API and Console operations. Queries to existing DNS records are not affected by this issue.
11:28 AM PDT -  We are investigating increased propagation times of DNS edits to public and private Route 53 hosted zones, and increased failures for Create Hosted Zone, Delete Hosted Zone, Associate Private Hosted Zone and Disassociate Private Hosted Zone API and Console operations. Queries to existing DNS records are not affected by this issue.
12:06 PM PDT -  We continue to investigate increased propagation times of DNS edits to public and private Route 53 hosted zones, and increased failures for Create Hosted Zone, Delete Hosted Zone, Associate Private Hosted Zone and Disassociate Private Hosted Zone API and Console operations. Queries to existing DNS records are not affected by this issue.
12:35 PM PDT -  We have identified the root cause and continue to work toward recovery. This may also impact provisioning workflows for other AWS services that use Route 53 for their DNS names. Queries to existing DNS records are not affected by this issue.
 2:01 PM PDT -  DNS updates are being accepted by the Route 53 API and Console, however an error is preventing the updates from being propagated from the Route53 API system to the live DNS servers. We have identified the root cause of this propagation error and are actively working towards resolution. This issue is impacting provisioning workflows for other AWS services that use Route 53 for their DNS names. Queries to DNS records present at the beginning of this issue are not affected and are being answered normally.
 4:00 PM PDT -  Some DNS changes are now propagating to the live DNS servers.  We continue to work towards full recovery.
 5:18 PM PDT -  We are observing steady recovery, with the oldest changes propagating first. We are continuing to work through the backlog of queued changes.
 6:30 PM PDT -  The backlog of queued changes continues to propagate. We have taken steps to accelerate recovery.
 7:35 PM PDT -  We have now propagated more than half of the backlog of queued changes and are continuing to work on the remainder.
 8:23 PM PDT -  We have processed the majority of the backlog of queued changes for Route 53 Public DNS. Private DNS changes are still experiencing propagation delays. Until we have fully recovered, customers may receive an error for the following APIs: CreateHostedZone, DeleteHostedZone, AssociateVPCWithHostedZone, and DisassociateVPCFromHostedZone. We continue to work towards full recovery.
10:08 PM PDT -  As of 8:34 PM PDT, we have resolved the issue resulting in Route 53 Public DNS propagation delays. At this time, Route 53 Private DNS changes are still experiencing propagation delays. Until we have fully recovered, customers will continue to receive errors for the following APIs: CreateHostedZone, DeleteHostedZone, AssociateVPCWithHostedZone, and DisassociateVPCFromHostedZone.
11:51 PM PDT -  We continue to make progress towards restoring propagation for Route 53 Private DNS changes and resolving increased API errors for CreateHostedZone, DeleteHostedZone, AssociateVPCWithHostedZone, and DisassociateVPCFromHostedZone Route 53 API calls. A majority of our hosts have processed all updates, and we are working to accelerate recovery of the remaining hosts.
May 11,  1:41 AM PDT -  We have restored propagation for Route 53 Private DNS changes. We continue to work on resolving increased API errors for CreateHostedZone, DeleteHostedZone, AssociateVPCWithHostedZone, and DisassociateVPCFromHostedZone Route 53 API calls.
May 11,  2:39 AM PDT -  Between May 10 10:50 AM and May 11 2:18 AM PDT we experienced increased change propagation latency and elevated error rates for CreateHostedZone, DeleteHostedZone, AssociateVPCWithHostedZone and DisassociateVPCFromHostedZone calls to the Route 53 API. During this entire duration there were no issues serving DNS queries. The issue has been resolved and the service is now operating normally."
163,Amazon Route 53,[RESOLVED] Change Propagation Delays & API Errors,1557569326,1,,"<div><span class=""yellowfg"">May 10, 11:21 AM PDT</span>&nbsp;We are investigating increased propagation times of DNS edits to public and private Route 53 hosted zones, and increased failures for Delete Hosted Zone, Associate Private Hosted Zone and Disassociate Private Hosted Zone API and Console operations. Queries to existing DNS records are not affected by this issue.</div><div><span class=""yellowfg"">May 10, 11:28 AM PDT</span>&nbsp;We are investigating increased propagation times of DNS edits to public and private Route 53 hosted zones, and increased failures for Create Hosted Zone, Delete Hosted Zone, Associate Private Hosted Zone and Disassociate Private Hosted Zone API and Console operations. Queries to existing DNS records are not affected by this issue.</div><div><span class=""yellowfg"">May 10, 12:06 PM PDT</span>&nbsp;We continue to investigate increased propagation times of DNS edits to public and private Route 53 hosted zones, and increased failures for Create Hosted Zone, Delete Hosted Zone, Associate Private Hosted Zone and Disassociate Private Hosted Zone API and Console operations. Queries to existing DNS records are not affected by this issue.</div><div><span class=""yellowfg"">May 10, 12:35 PM PDT</span>&nbsp;We have identified the root cause and continue to work toward recovery. This may also impact provisioning workflows for other AWS services that use Route 53 for their DNS names. Queries to existing DNS records are not affected by this issue.</div><div><span class=""yellowfg"">May 10,  2:01 PM PDT</span>&nbsp;DNS updates are being accepted by the Route 53 API and Console, however an error is preventing the updates from being propagated from the Route53 API system to the live DNS servers. We have identified the root cause of this propagation error and are actively working towards resolution. This issue is impacting provisioning workflows for other AWS services that use Route 53 for their DNS names. Queries to DNS records present at the beginning of this issue are not affected and are being answered normally.</div><div><span class=""yellowfg"">May 10,  4:00 PM PDT</span>&nbsp;Some DNS changes are now propagating to the live DNS servers.  We continue to work towards full recovery.</div><div><span class=""yellowfg"">May 10,  5:18 PM PDT</span>&nbsp;We are observing steady recovery, with the oldest changes propagating first. We are continuing to work through the backlog of queued changes.</div><div><span class=""yellowfg"">May 10,  6:30 PM PDT</span>&nbsp;The backlog of queued changes continues to propagate. We have taken steps to accelerate recovery.</div><div><span class=""yellowfg"">May 10,  7:35 PM PDT</span>&nbsp;We have now propagated more than half of the backlog of queued changes and are continuing to work on the remainder.</div><div><span class=""yellowfg"">May 10,  8:23 PM PDT</span>&nbsp;We have processed the majority of the backlog of queued changes for Route 53 Public DNS. Private DNS changes are still experiencing propagation delays. Until we have fully recovered, customers may receive an error for the following APIs: CreateHostedZone, DeleteHostedZone, AssociateVPCWithHostedZone, and DisassociateVPCFromHostedZone. We continue to work towards full recovery.</div><div><span class=""yellowfg"">May 10, 10:08 PM PDT</span>&nbsp;As of 8:34 PM PDT, we have resolved the issue resulting in Route 53 Public DNS propagation delays. At this time, Route 53 Private DNS changes are still experiencing propagation delays. Until we have fully recovered, customers will continue to receive errors for the following APIs: CreateHostedZone, DeleteHostedZone, AssociateVPCWithHostedZone, and DisassociateVPCFromHostedZone.</div><div><span class=""yellowfg"">May 10, 11:51 PM PDT</span>&nbsp;We continue to make progress towards restoring propagation for Route 53 Private DNS changes and resolving increased API errors for CreateHostedZone, DeleteHostedZone, AssociateVPCWithHostedZone, and DisassociateVPCFromHostedZone Route 53 API calls. A majority of our hosts have processed all updates, and we are working to accelerate recovery of the remaining hosts.</div><div><span class=""yellowfg""> 1:41 AM PDT</span>&nbsp;We have restored propagation for Route 53 Private DNS changes. We continue to work on resolving increased API errors for CreateHostedZone, DeleteHostedZone, AssociateVPCWithHostedZone, and DisassociateVPCFromHostedZone Route 53 API calls.</div><div><span class=""yellowfg""> 2:39 AM PDT</span>&nbsp;Between May 10 10:50 AM and May 11 2:18 AM PDT we experienced increased change propagation latency and elevated error rates for CreateHostedZone, DeleteHostedZone, AssociateVPCWithHostedZone and DisassociateVPCFromHostedZone calls to the Route 53 API. During this entire duration there were no issues serving DNS queries. The issue has been resolved and the service is now operating normally.</div>",route53,2019-05-11 10:08:46,Route 53,Global,10:50 AM,2:18 AM,PDT,928.0,2019,5,11,2019-05-11,change propagation delays & api errors,"May 10, 11:21 AM PDT -  We are investigating increased propagation times of DNS edits to public and private Route 53 hosted zones, and increased failures for Delete Hosted Zone, Associate Private Hosted Zone and Disassociate Private Hosted Zone API and Console operations. Queries to existing DNS records are not affected by this issue.
May 10, 11:28 AM PDT -  We are investigating increased propagation times of DNS edits to public and private Route 53 hosted zones, and increased failures for Create Hosted Zone, Delete Hosted Zone, Associate Private Hosted Zone and Disassociate Private Hosted Zone API and Console operations. Queries to existing DNS records are not affected by this issue.
May 10, 12:06 PM PDT -  We continue to investigate increased propagation times of DNS edits to public and private Route 53 hosted zones, and increased failures for Create Hosted Zone, Delete Hosted Zone, Associate Private Hosted Zone and Disassociate Private Hosted Zone API and Console operations. Queries to existing DNS records are not affected by this issue.
May 10, 12:35 PM PDT -  We have identified the root cause and continue to work toward recovery. This may also impact provisioning workflows for other AWS services that use Route 53 for their DNS names. Queries to existing DNS records are not affected by this issue.
May 10,  2:01 PM PDT -  DNS updates are being accepted by the Route 53 API and Console, however an error is preventing the updates from being propagated from the Route53 API system to the live DNS servers. We have identified the root cause of this propagation error and are actively working towards resolution. This issue is impacting provisioning workflows for other AWS services that use Route 53 for their DNS names. Queries to DNS records present at the beginning of this issue are not affected and are being answered normally.
May 10,  4:00 PM PDT -  Some DNS changes are now propagating to the live DNS servers.  We continue to work towards full recovery.
May 10,  5:18 PM PDT -  We are observing steady recovery, with the oldest changes propagating first. We are continuing to work through the backlog of queued changes.
May 10,  6:30 PM PDT -  The backlog of queued changes continues to propagate. We have taken steps to accelerate recovery.
May 10,  7:35 PM PDT -  We have now propagated more than half of the backlog of queued changes and are continuing to work on the remainder.
May 10,  8:23 PM PDT -  We have processed the majority of the backlog of queued changes for Route 53 Public DNS. Private DNS changes are still experiencing propagation delays. Until we have fully recovered, customers may receive an error for the following APIs: CreateHostedZone, DeleteHostedZone, AssociateVPCWithHostedZone, and DisassociateVPCFromHostedZone. We continue to work towards full recovery.
May 10, 10:08 PM PDT -  As of 8:34 PM PDT, we have resolved the issue resulting in Route 53 Public DNS propagation delays. At this time, Route 53 Private DNS changes are still experiencing propagation delays. Until we have fully recovered, customers will continue to receive errors for the following APIs: CreateHostedZone, DeleteHostedZone, AssociateVPCWithHostedZone, and DisassociateVPCFromHostedZone.
May 10, 11:51 PM PDT -  We continue to make progress towards restoring propagation for Route 53 Private DNS changes and resolving increased API errors for CreateHostedZone, DeleteHostedZone, AssociateVPCWithHostedZone, and DisassociateVPCFromHostedZone Route 53 API calls. A majority of our hosts have processed all updates, and we are working to accelerate recovery of the remaining hosts.
 1:41 AM PDT -  We have restored propagation for Route 53 Private DNS changes. We continue to work on resolving increased API errors for CreateHostedZone, DeleteHostedZone, AssociateVPCWithHostedZone, and DisassociateVPCFromHostedZone Route 53 API calls.
 2:39 AM PDT -  Between May 10 10:50 AM and May 11 2:18 AM PDT we experienced increased change propagation latency and elevated error rates for CreateHostedZone, DeleteHostedZone, AssociateVPCWithHostedZone and DisassociateVPCFromHostedZone calls to the Route 53 API. During this entire duration there were no issues serving DNS queries. The issue has been resolved and the service is now operating normally."
164,Amazon Connect (N. Virginia),[RESOLVED] Degraded Call Handling Experience,1558724798,1,,"<div><span class=""yellowfg"">12:06 PM PDT</span>&nbsp;Between 10:40 AM and 11:04 AM PDT we experienced failures in call handling operations in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",connect-us-east-1,2019-05-24 19:06:38,Connect,N. Virginia,10:40 AM,11:04 AM,PDT,24.0,2019,5,24,2019-05-24,degraded call handling experience,12:06 PM PDT -  Between 10:40 AM and 11:04 AM PDT we experienced failures in call handling operations in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.
165,Amazon Relational Database Service (N. Virginia),[RESOLVED] Increased API Latencies and Management Console Error Rates,1560790279,1,,"<div><span class=""yellowfg""> 9:51 AM PDT</span>&nbsp;We are investigating increased API latencies and increased error rates for the RDS Management Console in the US-EAST-1 Region.</div><div><span class=""yellowfg"">10:09 AM PDT</span>&nbsp;Between 8:21 AM and 10:01 AM PDT, we experienced increased API latencies and increased error rates for the RDS Management Console in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",rds-us-east-1,2019-06-17 16:51:19,Relational Database Service,N. Virginia,8:21 AM,10:01 AM,PDT,100.0,2019,6,17,2019-06-17,increased api latencies and management console error rates," 9:51 AM PDT -  We are investigating increased API latencies and increased error rates for the RDS Management Console in the US-EAST-1 Region.
10:09 AM PDT -  Between 8:21 AM and 10:01 AM PDT, we experienced increased API latencies and increased error rates for the RDS Management Console in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
166,Amazon Virtual Private Cloud (Singapore),[RESOLVED] AWS Site-to-Site VPN Connectivity Issues,1561413948,2,,"<div><span class=""yellowfg""> 3:05 PM PDT</span>&nbsp;We are investigating network connectivity issues affecting some AWS Site-to-Site VPN customers in the AP-SOUTHEAST-1 Region.</div><div><span class=""yellowfg""> 3:24 PM PDT</span>&nbsp;We have identified the root cause of the issue affecting network connectivity for some AWS Site-to-Site VPN connections within the AP-SOUTHEAST-1 Region. Connectivity to and from the Internet, within Amazon VPC, and over DirectConnect is not affected by this issue.</div><div><span class=""yellowfg""> 4:11 PM PDT</span>&nbsp;We can confirm network connectivity issues for some AWS Site-to-Site VPN connections within the AP-SOUTHEAST-1 Region and continue to work towards resolution. Connectivity to and from the Internet, within Amazon VPC, and over DirectConnect is not affected by this issue.</div><div><span class=""yellowfg""> 4:37 PM PDT</span>&nbsp;We can confirm network connectivity issues for existing AWS Site-to-Site VPN connections within the AP-SOUTHEAST-1 Region and are starting to see recovery. Newly created AWS Site-to-Site VPN connections within the AP-SOUTHEAST-1 Region are not affected by this issue. Connectivity to and from the Internet, within Amazon VPC, and over DirectConnect is not affected by this issue.</div><div><span class=""yellowfg""> 5:52 PM PDT</span>&nbsp;We continue to work towards full recovery of the network connectivity issues affecting VPN connections in the AP-SOUTHEAST-1 Region. Connectivity to and from the Internet, within Amazon VPC, and over DirectConnect is not affected by this issue.</div><div><span class=""yellowfg""> 7:07 PM PDT</span>&nbsp;We continue to work towards full recovery of the network connectivity issues affecting VPN connections in the AP-SOUTHEAST-1 Region. Connectivity to and from the Internet, within Amazon VPC, and over DirectConnect is not affected by this issue.</div><div><span class=""yellowfg""> 9:08 PM PDT</span>&nbsp;Some VPN connections have been restored. We continue to work towards full recovery.</div><div><span class=""yellowfg""> 9:54 PM PDT</span>&nbsp;We have now restored connectivity for the majority of the primary tunnels associated with all affected VPN connections in the AP-SOUTHEAST-1 Region. At this stage, VPN connections should no longer be experiencing connectivity issues. We continue to work on restoring connectivity for all secondary tunnels.</div><div><span class=""yellowfg"">10:35 PM PDT</span>&nbsp;While the majority of VPN tunnels are once again able to establish a connection, route updates are not yet propagating for all affected VPN connections in the AP-SOUTHEAST-1 Region. For affected tunnels, this will result in network connectivity issues which we continue to work on resolving. </div><div><span class=""yellowfg"">Jun 25,  1:34 AM PDT</span>&nbsp;We continue to make progress in restoring network connectivity for VPN connections in the AP-SOUTHEAST-1 Region. We are also seeing an improvement in route propagation times as we work towards full recovery.</div><div><span class=""yellowfg"">Jun 25,  6:32 AM PDT</span>&nbsp;We have resolved the connectivity issues affecting VPN connections in the AP-SOUTHEAST-1 Region. Connectivity to and from the Internet, within Amazon VPC, and over DirectConnect was not affected by this issue. The issue has been resolved and the service is operating normally.</div>",vpc-ap-southeast-1,2019-06-24 22:05:48,Virtual Private Cloud,Singapore,3:05 PM,6:32 AM,PDT,927.0,2019,6,24,2019-06-24,aws site-to-site vpn connectivity issues," 3:05 PM PDT -  We are investigating network connectivity issues affecting some AWS Site-to-Site VPN customers in the AP-SOUTHEAST-1 Region.
 3:24 PM PDT -  We have identified the root cause of the issue affecting network connectivity for some AWS Site-to-Site VPN connections within the AP-SOUTHEAST-1 Region. Connectivity to and from the Internet, within Amazon VPC, and over DirectConnect is not affected by this issue.
 4:11 PM PDT -  We can confirm network connectivity issues for some AWS Site-to-Site VPN connections within the AP-SOUTHEAST-1 Region and continue to work towards resolution. Connectivity to and from the Internet, within Amazon VPC, and over DirectConnect is not affected by this issue.
 4:37 PM PDT -  We can confirm network connectivity issues for existing AWS Site-to-Site VPN connections within the AP-SOUTHEAST-1 Region and are starting to see recovery. Newly created AWS Site-to-Site VPN connections within the AP-SOUTHEAST-1 Region are not affected by this issue. Connectivity to and from the Internet, within Amazon VPC, and over DirectConnect is not affected by this issue.
 5:52 PM PDT -  We continue to work towards full recovery of the network connectivity issues affecting VPN connections in the AP-SOUTHEAST-1 Region. Connectivity to and from the Internet, within Amazon VPC, and over DirectConnect is not affected by this issue.
 7:07 PM PDT -  We continue to work towards full recovery of the network connectivity issues affecting VPN connections in the AP-SOUTHEAST-1 Region. Connectivity to and from the Internet, within Amazon VPC, and over DirectConnect is not affected by this issue.
 9:08 PM PDT -  Some VPN connections have been restored. We continue to work towards full recovery.
 9:54 PM PDT -  We have now restored connectivity for the majority of the primary tunnels associated with all affected VPN connections in the AP-SOUTHEAST-1 Region. At this stage, VPN connections should no longer be experiencing connectivity issues. We continue to work on restoring connectivity for all secondary tunnels.
10:35 PM PDT -  While the majority of VPN tunnels are once again able to establish a connection, route updates are not yet propagating for all affected VPN connections in the AP-SOUTHEAST-1 Region. For affected tunnels, this will result in network connectivity issues which we continue to work on resolving. 
Jun 25,  1:34 AM PDT -  We continue to make progress in restoring network connectivity for VPN connections in the AP-SOUTHEAST-1 Region. We are also seeing an improvement in route propagation times as we work towards full recovery.
Jun 25,  6:32 AM PDT -  We have resolved the connectivity issues affecting VPN connections in the AP-SOUTHEAST-1 Region. Connectivity to and from the Internet, within Amazon VPC, and over DirectConnect was not affected by this issue. The issue has been resolved and the service is operating normally."
167,AWS IoT Analytics (N. Virginia),[RESOLVED] Increased API Error Rates and Latency,1561652460,2,,"<div><span class=""yellowfg""> 1:32 PM PDT</span>&nbsp;We can confirm increased error rates and latencies in the US-EAST-1 Region. We have identified the root cause and continue to work toward resolution.</div><div><span class=""yellowfg""> 2:33 PM PDT</span>&nbsp;We are seeing significant recovery and continue to work on restoring all operations.</div><div><span class=""yellowfg""> 2:45 PM PDT</span>&nbsp;Between 9:21 AM and 2:27 PM PDT we experienced increased API error rates and latencies in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",iotanalytics-us-east-1,2019-06-27 16:21:00,IoT Analytics,N. Virginia,9:21 AM,2:27 PM,PDT,306.0,2019,6,27,2019-06-27,increased api error rates and latency," 1:32 PM PDT -  We can confirm increased error rates and latencies in the US-EAST-1 Region. We have identified the root cause and continue to work toward resolution.
 2:33 PM PDT -  We are seeing significant recovery and continue to work on restoring all operations.
 2:45 PM PDT -  Between 9:21 AM and 2:27 PM PDT we experienced increased API error rates and latencies in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
168,AWS Glue (N. Virginia),[RESOLVED] Increased Error Rates,1561655416,3,,"<div><span class=""yellowfg"">10:10 AM PDT</span>&nbsp;We are investigating increased API error rates in the US-EAST-1 Region.</div><div><span class=""yellowfg"">10:34 AM PDT</span>&nbsp;We can confirm increased API error rates in the US-EAST-1 Region. We have identified the root cause and continue to work toward resolution. </div><div><span class=""yellowfg"">11:52 AM PDT</span>&nbsp;We are beginning to see intermittent recovery for Glue and continue to work toward full recovery.</div><div><span class=""yellowfg""> 1:04 PM PDT</span>&nbsp;We want to give you more information on the issue affecting AWS Glue. Glue Workflow APIs, Orchestration APIs, and ETL jobs that do not require the AWS Glue Data Catalog APIs continue to operate normally. The issue with the Data Catalog APIs started with a software update in the US-EAST-1 Region that completed at 9:21 AM PDT. The software update was immediately rolled back, and we are now working towards stabilizing the Data Catalog API subsystem. We continue to work towards a full recovery.</div><div><span class=""yellowfg""> 1:17 PM PDT</span>&nbsp;As we work toward resolution we are temporarily suspending traffic for the Data Catalog APIs in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 2:06 PM PDT</span>&nbsp;We are beginning to work on removing throttles and restoring API availability but are proceeding cautiously.</div><div><span class=""yellowfg""> 2:23 PM PDT</span>&nbsp;We are seeing significant recovery and continue to work on restoring all operations.</div><div><span class=""yellowfg""> 2:42 PM PDT</span>&nbsp;Between 9:21 AM and 2:06 PM PDT we experienced increased API error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",glue-us-east-1,2019-06-27 17:10:16,Glue,N. Virginia,9:21 AM,2:06 PM,PDT,285.0,2019,6,27,2019-06-27,increased error rates,"10:10 AM PDT -  We are investigating increased API error rates in the US-EAST-1 Region.
10:34 AM PDT -  We can confirm increased API error rates in the US-EAST-1 Region. We have identified the root cause and continue to work toward resolution. 
11:52 AM PDT -  We are beginning to see intermittent recovery for Glue and continue to work toward full recovery.
 1:04 PM PDT -  We want to give you more information on the issue affecting AWS Glue. Glue Workflow APIs, Orchestration APIs, and ETL jobs that do not require the AWS Glue Data Catalog APIs continue to operate normally. The issue with the Data Catalog APIs started with a software update in the US-EAST-1 Region that completed at 9:21 AM PDT. The software update was immediately rolled back, and we are now working towards stabilizing the Data Catalog API subsystem. We continue to work towards a full recovery.
 1:17 PM PDT -  As we work toward resolution we are temporarily suspending traffic for the Data Catalog APIs in the US-EAST-1 Region.
 2:06 PM PDT -  We are beginning to work on removing throttles and restoring API availability but are proceeding cautiously.
 2:23 PM PDT -  We are seeing significant recovery and continue to work on restoring all operations.
 2:42 PM PDT -  Between 9:21 AM and 2:06 PM PDT we experienced increased API error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
169,Amazon Athena (N. Virginia),[RESOLVED] Increased Query Failures and Latency,1561655746,3,,"<div><span class=""yellowfg"">10:15 AM PDT</span>&nbsp;We are investigating increased query failures and latency in the US-EAST-1 Region.</div><div><span class=""yellowfg"">10:37 AM PDT</span>&nbsp;We can confirm increased query failures and latency in the US-EAST-1 Region.</div><div><span class=""yellowfg"">11:01 AM PDT</span>&nbsp;We have identified the root cause and continue to work toward resolution.</div><div><span class=""yellowfg"">11:52 AM PDT</span>&nbsp;We are beginning to see signs of recovery for Athena and continue to work toward full recovery.</div><div><span class=""yellowfg"">12:32 PM PDT</span>&nbsp;We continue to experience increased error rates, as Athena depends on the Glue Data Catalog APIs. We will be throttling the Athena APIs as we work toward recovery. Customers may receive a ""Rate Exceeded"" error as recovery progresses.</div><div><span class=""yellowfg""> 1:35 PM PDT</span>&nbsp;We continue to work toward resolution.</div><div><span class=""yellowfg""> 2:14 PM PDT</span>&nbsp;We are beginning to work on removing throttles and restoring query availability but are proceeding cautiously.</div><div><span class=""yellowfg""> 2:32 PM PDT</span>&nbsp;We are seeing significant recovery and continue to work on restoring all operations.</div><div><span class=""yellowfg""> 2:43 PM PDT</span>&nbsp;Between 9:21 AM and 2:36 PM PDT we experienced increased query failures and latency in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",athena-us-east-1,2019-06-27 17:15:46,Athena,N. Virginia,9:21 AM,2:36 PM,PDT,315.0,2019,6,27,2019-06-27,increased query failures and latency,"10:15 AM PDT -  We are investigating increased query failures and latency in the US-EAST-1 Region.
10:37 AM PDT -  We can confirm increased query failures and latency in the US-EAST-1 Region.
11:01 AM PDT -  We have identified the root cause and continue to work toward resolution.
11:52 AM PDT -  We are beginning to see signs of recovery for Athena and continue to work toward full recovery.
12:32 PM PDT -  We continue to experience increased error rates, as Athena depends on the Glue Data Catalog APIs. We will be throttling the Athena APIs as we work toward recovery. Customers may receive a ""Rate Exceeded"" error as recovery progresses.
 1:35 PM PDT -  We continue to work toward resolution.
 2:14 PM PDT -  We are beginning to work on removing throttles and restoring query availability but are proceeding cautiously.
 2:32 PM PDT -  We are seeing significant recovery and continue to work on restoring all operations.
 2:43 PM PDT -  Between 9:21 AM and 2:36 PM PDT we experienced increased query failures and latency in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
170,Amazon Redshift (N. Virginia),[RESOLVED] Increased API Error Rates,1561658605,1,,"<div><span class=""yellowfg"">11:03 AM PDT</span>&nbsp;We are investigating increased error rates for Redshift Spectrum queries accessing the Glue catalog in the US-EAST-1 Region.</div><div><span class=""yellowfg"">12:04 PM PDT</span>&nbsp;We can confirm increased error rates for Redshift Spectrum queries accessing the Glue catalog in the US-EAST-1 Region. We have identified the root cause and continue to work toward resolution.</div><div><span class=""yellowfg""> 1:09 PM PDT</span>&nbsp;We continue to work toward resolution.</div><div><span class=""yellowfg""> 2:33 PM PDT</span>&nbsp;We are seeing significant recovery and continue to work on restoring all operations.</div><div><span class=""yellowfg""> 2:45 PM PDT</span>&nbsp;Between 9:21 AM and 2:06 PM PDT we experienced increased error rates for Redshift Spectrum queries accessing the Glue catalog in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",redshift-us-east-1,2019-06-27 18:03:25,Redshift,N. Virginia,9:21 AM,2:06 PM,PDT,285.0,2019,6,27,2019-06-27,increased api error rates,"11:03 AM PDT -  We are investigating increased error rates for Redshift Spectrum queries accessing the Glue catalog in the US-EAST-1 Region.
12:04 PM PDT -  We can confirm increased error rates for Redshift Spectrum queries accessing the Glue catalog in the US-EAST-1 Region. We have identified the root cause and continue to work toward resolution.
 1:09 PM PDT -  We continue to work toward resolution.
 2:33 PM PDT -  We are seeing significant recovery and continue to work on restoring all operations.
 2:45 PM PDT -  Between 9:21 AM and 2:06 PM PDT we experienced increased error rates for Redshift Spectrum queries accessing the Glue catalog in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. "
171,Amazon Elastic Compute Cloud (London),[RESOLVED] Increased API Error Rates,1561864621,1,,"<div><span class=""yellowfg""> 8:17 PM PDT</span>&nbsp;We are investigating increased API error rates and latencies in a single Availability Zone in the EU-WEST-2 Region.</div><div><span class=""yellowfg""> 9:23 PM PDT</span>&nbsp;We continue to investigate the increased error rates and latencies for the EC2 APIs in the EU-WEST-2 Region. The health of existing EC2 instances is not affected.</div><div><span class=""yellowfg"">10:00 PM PDT</span>&nbsp;Between 7:40 PM and 9:35 PM PDT we experienced increased API error rates and latencies in the EU-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",ec2-eu-west-2,2019-06-30 03:17:01,Elastic Compute Cloud,London,7:40 PM,9:35 PM,PDT,115.0,2019,6,30,2019-06-30,increased api error rates," 8:17 PM PDT -  We are investigating increased API error rates and latencies in a single Availability Zone in the EU-WEST-2 Region.
 9:23 PM PDT -  We continue to investigate the increased error rates and latencies for the EC2 APIs in the EU-WEST-2 Region. The health of existing EC2 instances is not affected.
10:00 PM PDT -  Between 7:40 PM and 9:35 PM PDT we experienced increased API error rates and latencies in the EU-WEST-2 Region. The issue has been resolved and the service is operating normally."
172,Amazon Elastic Load Balancing (London),[RESOLVED] Increased API Error Rates,1561866166,1,,"<div><span class=""yellowfg""> 8:42 PM PDT</span>&nbsp;We are investigating increased API error rates and latencies in the EU-WEST-2 Region.</div><div><span class=""yellowfg""> 9:51 PM PDT</span>&nbsp;We have identified the root cause of the increase in error rates and latencies for the ELB APIs in the EU-WEST-2 Region and continue to work towards full recovery.</div><div><span class=""yellowfg"">10:45 PM PDT</span>&nbsp;Between 8:25 PM and 10:15 PM PDT, we experienced increased API error rates and latencies in the EU-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",elb-eu-west-2,2019-06-30 03:42:46,Elastic Load Balancing,London,8:25 PM,10:15 PM,PDT,110.0,2019,6,30,2019-06-30,increased api error rates," 8:42 PM PDT -  We are investigating increased API error rates and latencies in the EU-WEST-2 Region.
 9:51 PM PDT -  We have identified the root cause of the increase in error rates and latencies for the ELB APIs in the EU-WEST-2 Region and continue to work towards full recovery.
10:45 PM PDT -  Between 8:25 PM and 10:15 PM PDT, we experienced increased API error rates and latencies in the EU-WEST-2 Region. The issue has been resolved and the service is operating normally."
173,Amazon CloudWatch (London),[RESOLVED] Elevated API faults and alarm delays,1561867337,1,,"<div><span class=""yellowfg""> 9:02 PM PDT</span>&nbsp;We are investigating increased faults for CloudWatch alarms APIs and delays in processing some alarms in the EU-WEST-2 Region.</div><div><span class=""yellowfg""> 9:55 PM PDT</span>&nbsp;We can confirm an elevated rate of CloudWatch Alarms API faults and delays in processing some alarms in EU-WEST-2 Region and continue to work towards full recovery.</div><div><span class=""yellowfg"">10:04 PM PDT</span>&nbsp;Between 8:27 PM and 9:55 PM PDT, customers may have experienced elevated alarms API faults and delayed alarms in the EU-WEST-2 Region. We have resolved the issue and the service is operating normally.</div>",cloudwatch-eu-west-2,2019-06-30 04:02:17,CloudWatch,London,8:27 PM,9:55 PM,PDT,88.0,2019,6,30,2019-06-30,elevated api faults and alarm delays," 9:02 PM PDT -  We are investigating increased faults for CloudWatch alarms APIs and delays in processing some alarms in the EU-WEST-2 Region.
 9:55 PM PDT -  We can confirm an elevated rate of CloudWatch Alarms API faults and delays in processing some alarms in EU-WEST-2 Region and continue to work towards full recovery.
10:04 PM PDT -  Between 8:27 PM and 9:55 PM PDT, customers may have experienced elevated alarms API faults and delayed alarms in the EU-WEST-2 Region. We have resolved the issue and the service is operating normally."
174,Amazon Simple Workflow Service (London),[RESOLVED] Elevated error rates for Simple Workflow,1561871879,1,,"<div><span class=""yellowfg"">10:18 PM PDT</span>&nbsp;We are experiencing elevated API failures for AWS Simple Workflow in the EU-WEST-2 Region and continue to work towards full recovery.</div><div><span class=""yellowfg"">10:55 PM PDT</span>&nbsp;Between 9:15 PM and 10:30 PM PDT we experienced elevated API failures for AWS Simple Workflow in the EU-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",swf-eu-west-2,2019-06-30 05:17:59,Simple Workflow Service,London,9:15 PM,10:30 PM,PDT,75.0,2019,6,30,2019-06-30,elevated error rates for simple workflow,"10:18 PM PDT -  We are experiencing elevated API failures for AWS Simple Workflow in the EU-WEST-2 Region and continue to work towards full recovery.
10:55 PM PDT -  Between 9:15 PM and 10:30 PM PDT we experienced elevated API failures for AWS Simple Workflow in the EU-WEST-2 Region. The issue has been resolved and the service is operating normally."
175,Amazon Elastic Compute Cloud (US-East),[RESOLVED] Increased API Error Rates,1562900564,1,,"<div><span class=""yellowfg""> 8:02 PM PDT</span>&nbsp;We are investigating increased error rates for the EC2 APIs and new instance launches in the US-GOV-EAST-1 Region. Existing instances are unaffected. </div><div><span class=""yellowfg""> 8:35 PM PDT</span>&nbsp;We can confirm increased error rates for the CreateVolume and CreateSnapshot EBS APIs in the US-GOV-EAST-1 Region. This may impact the launching of new EBS backed EC2 instances. Existing instances are unaffected. </div><div><span class=""yellowfg""> 9:00 PM PDT</span>&nbsp;Between 7:07 PM and 8:47 PM PDT we experienced increased error rates for the CreateVolume and CreateSnapshot EBS APIs in the US-GOV-EAST-1 Region, which also impacted the launching of new EBS backed EC2 instances. Existing instances were unaffected. The issue has been resolved and the service is operating normally.</div>",ec2-us-gov-east-1,2019-07-12 03:02:44,Elastic Compute Cloud,US-East,7:07 PM,8:47 PM,PDT,100.0,2019,7,12,2019-07-12,increased api error rates," 8:02 PM PDT -  We are investigating increased error rates for the EC2 APIs and new instance launches in the US-GOV-EAST-1 Region. Existing instances are unaffected. 
 8:35 PM PDT -  We can confirm increased error rates for the CreateVolume and CreateSnapshot EBS APIs in the US-GOV-EAST-1 Region. This may impact the launching of new EBS backed EC2 instances. Existing instances are unaffected. 
 9:00 PM PDT -  Between 7:07 PM and 8:47 PM PDT we experienced increased error rates for the CreateVolume and CreateSnapshot EBS APIs in the US-GOV-EAST-1 Region, which also impacted the launching of new EBS backed EC2 instances. Existing instances were unaffected. The issue has been resolved and the service is operating normally."
176,Amazon Elastic Compute Cloud (London),[RESOLVED] EBS Volumes Degraded Performance,1563902591,1,,"<div><span class=""yellowfg"">10:23 AM PDT</span>&nbsp;We are investigating degraded performance for EBS volumes in a single Availability Zone in the EU-WEST-2 Region.</div><div><span class=""yellowfg"">11:11 AM PDT</span>&nbsp;We have determined root cause of the degraded performance for EBS volumes in a single Availability Zone in the EU-WEST-2 Region. The degraded performance has been resolved for some of the affected volumes and we continue to work towards full recovery.</div><div><span class=""yellowfg"">11:44 AM PDT</span>&nbsp;Between 9:30 AM and 11:37 AM PDT we experienced degraded performance for some EBS volumes in a single Availability Zone in the EU-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",ec2-eu-west-2,2019-07-23 17:23:11,Elastic Compute Cloud,London,9:30 AM,11:37 AM,PDT,127.0,2019,7,23,2019-07-23,ebs volumes degraded performance,"10:23 AM PDT -  We are investigating degraded performance for EBS volumes in a single Availability Zone in the EU-WEST-2 Region.
11:11 AM PDT -  We have determined root cause of the degraded performance for EBS volumes in a single Availability Zone in the EU-WEST-2 Region. The degraded performance has been resolved for some of the affected volumes and we continue to work towards full recovery.
11:44 AM PDT -  Between 9:30 AM and 11:37 AM PDT we experienced degraded performance for some EBS volumes in a single Availability Zone in the EU-WEST-2 Region. The issue has been resolved and the service is operating normally."
177,Amazon Relational Database Service (London),[RESOLVED] Connectivity Issues,1563904324,1,,"<div><span class=""yellowfg"">10:52 AM PDT</span>&nbsp;We are investigating connectivity issues affecting some database instances in a single Availability Zone in the EU-WEST-2 Region.</div><div><span class=""yellowfg"">11:26 AM PDT</span>&nbsp;We have determined root cause for the connectivity issues affecting some database instances in a single Availability Zone in the EU-WEST-2 Region. The connectivity issues have been resolved for some database instances and we continue to work towards full recovery.</div><div><span class=""yellowfg"">11:51 AM PDT</span>&nbsp;Between 9:30 AM and 11:40 AM PDT we experienced connectivity issues affecting some database instances in a single Availability Zone in the EU-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",rds-eu-west-2,2019-07-23 17:52:04,Relational Database Service,London,9:30 AM,11:40 AM,PDT,130.0,2019,7,23,2019-07-23,connectivity issues,"10:52 AM PDT -  We are investigating connectivity issues affecting some database instances in a single Availability Zone in the EU-WEST-2 Region.
11:26 AM PDT -  We have determined root cause for the connectivity issues affecting some database instances in a single Availability Zone in the EU-WEST-2 Region. The connectivity issues have been resolved for some database instances and we continue to work towards full recovery.
11:51 AM PDT -  Between 9:30 AM and 11:40 AM PDT we experienced connectivity issues affecting some database instances in a single Availability Zone in the EU-WEST-2 Region. The issue has been resolved and the service is operating normally."
178,Amazon Connect (N. Virginia),[RESOLVED] Increased Call Failures,1566235755,2,,"<div><span class=""yellowfg"">10:29 AM PDT</span>&nbsp;We are investigating call failures and issues accessing Amazon Connect in the US-EAST-1 Region.</div><div><span class=""yellowfg"">10:58 AM PDT</span>&nbsp;Call handling has recovered for agents who can access Amazon Connect in the US-EAST-1 Region. We continue to investigate problems accessing the Amazon Connect Console.</div><div><span class=""yellowfg"">11:46 AM PDT</span>&nbsp;Between 10:02 AM and 11:13 AM PDT, some Amazon Connect users experienced issues logging in or performing actions in the Connect application in the US-EAST-1 Region. Some calls may have failed during this time. The issue has been resolved and the service is operating normally.</div>",connect-us-east-1,2019-08-19 17:29:15,Connect,N. Virginia,10:02 AM,11:13 AM,PDT,71.0,2019,8,19,2019-08-19,increased call failures,"10:29 AM PDT -  We are investigating call failures and issues accessing Amazon Connect in the US-EAST-1 Region.
10:58 AM PDT -  Call handling has recovered for agents who can access Amazon Connect in the US-EAST-1 Region. We continue to investigate problems accessing the Amazon Connect Console.
11:46 AM PDT -  Between 10:02 AM and 11:13 AM PDT, some Amazon Connect users experienced issues logging in or performing actions in the Connect application in the US-EAST-1 Region. Some calls may have failed during this time. The issue has been resolved and the service is operating normally."
179,Amazon Elastic Compute Cloud (N. Virginia),[RESOLVED] Increased API Error Rates and Latencies ,1566264383,1,,"<div><span class=""yellowfg""> 6:26 PM PDT</span>&nbsp;We are investigating increased API error rates and latencies for the EC2 APIs in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 6:48 PM PDT</span>&nbsp;Between 6:00 PM and 6:41 PM PDT we experienced increased API error rates and latencies for the EC2 APIs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",ec2-us-east-1,2019-08-20 01:26:23,Elastic Compute Cloud,N. Virginia,6:00 PM,6:41 PM,PDT,41.0,2019,8,20,2019-08-20,increased api error rates and latencies," 6:26 PM PDT -  We are investigating increased API error rates and latencies for the EC2 APIs in the US-EAST-1 Region.
 6:48 PM PDT -  Between 6:00 PM and 6:41 PM PDT we experienced increased API error rates and latencies for the EC2 APIs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
180,Amazon Elastic Compute Cloud (Tokyo),インスタンスの接続性について | Instance Availability,1566533746,2,,"<div><span class=""yellowfg""> 9:18 PM PDT</span>&nbsp;We are investigating connectivity issues affecting some instances in a single Availability Zone in the AP-NORTHEAST-1 Region.</div><div><span class=""yellowfg""> 9:47 PM PDT</span>&nbsp;We can confirm that some instances are impaired and some EBS volumes are experiencing degraded performance within a single Availability Zone in the AP-NORTHEAST-1 Region. Some EC2 APIs are also experiencing increased error rates and latencies. We are working to resolve the issue.</div><div><span class=""yellowfg"">10:27 PM PDT</span>&nbsp;We have identified the root cause and are working toward recovery for the instance impairments and degraded EBS volume performance within a single Availability Zone in the AP-NORTHEAST-1 Region.</div><div><span class=""yellowfg"">11:40 PM PDT</span>&nbsp;We are starting to see recovery for instance impairments and degraded EBS volume performance within a single Availability Zone in the AP-NORTHEAST-1 Region. We continue to work towards recovery for all affected instances and EBS volumes.</div><div><span class=""yellowfg"">Aug 23,  1:54 AM PDT</span>&nbsp;Recovery is in progress for instance impairments and degraded EBS volume performance within a single Availability Zone in the AP-NORTHEAST-1 Region. We continue to work towards recovery for all affected instances and EBS volumes.</div><div><span class=""yellowfg"">Aug 23,  2:39 AM PDT</span>&nbsp;The majority of impaired EC2 instances and EBS volumes experiencing degraded performance have now recovered. We continue to work on recovery for the remaining EC2 instances and EBS volumes that are affected by this issue. This issue affects EC2 instances and EBS volumes in a single Availability Zone in the AP-NORTHEAST-1 region.</div><div><span class=""yellowfg"">Aug 23,  4:18 AM PDT</span>&nbsp;日本時間 2019年8月23日 12:36 より、AP-NORTHEAST-1 の単一のアベイラビリティゾーンで、一定の割合の EC2 サーバのオーバーヒートが発生しました。この結果、当該アベイラビリティゾーンの EC2 インスタンス及び EBS ボリュームのパフォーマンスの劣化が発生しました。このオーバーヒートは、影響を受けたアベイラビリティゾーン中の一部の冗長化された空調設備の管理システム障害が原因です。日本時間 15:21 に冷却装置は復旧し、室温が通常状態に戻り始めました。温度が通常状態に戻ったことで、影響を受けたインスタンスの電源が回復しました。日本時間 18:30 より大部分の EC2 インスタンスと EBS ボリュームは回復しました。 我々は残りの EC2 インスタンスと EBS ボリュームの回復に取り組んでいます。少数の EC2 インスタンスと EBS ボリュームが電源が落ちたハードウェア ホスト上に残されています。我々は影響をうけた全ての EC2 インスタンスと EBS ボリュームの回復のための作業を継続しています。 早期回復の為、可能な場合残された影響を受けている EC2 インスタンスと EBS ボリュームのリプレースを推奨します。いくつかの影響をうけた EC2 インスタンスはお客様側での作業が必要になる可能性がある為、 後ほどお客様個別にお知らせすることを予定しています。<br><br>詳細は <a href=""https://aws.amazon.com/message/56489/"">こちら</a> をご参照ください。追加のご質問がある場合は、<a href=""https://aws.amazon.com/support"">AWS サポート</a>までご連絡ください。 | Beginning at 8:36 PM PDT a small percentage of EC2 servers in a single Availability Zone in the AP-NORTHEAST-1 Region shutdown due to overheating. This resulted in impaired EC2 instances and degraded EBS volume performance for resources in the affected area of the Availability Zone. The overheating was caused by a control system failure that caused multiple, redundant cooling systems to fail in parts of the affected Availability Zone. The chillers were restored at 11:21 PM PDT and temperatures in the affected areas began to return to normal. As temperatures returned to normal, power was restored to the affected instances. By 2:30 AM PDT, the vast majority of instances and volumes had recovered. We have been working to recover the remaining instances and volumes. A small number of remaining instances and volumes are hosted on hardware which was adversely affected by the loss of power. We continue to work to recover all affected instances and volumes. For immediate recovery, we recommend replacing any remaining affected instances or volumes if possible. Some of the affected instances may require action from customers and we will be reaching out to those customers with next steps.<br><br>Additional details are available <a href=""https://aws.amazon.com/message/56489/"">here</a>. If you have additional questions, please contact <a href=""https://aws.amazon.com/support"">AWS Support</a>.</div>",ec2-ap-northeast-1,2019-08-23 04:15:46,Elastic Compute Cloud,Tokyo,8:36 PM,2:30 AM,PDT,354.0,2019,8,23,2019-08-23,instance availability," 9:18 PM PDT -  We are investigating connectivity issues affecting some instances in a single Availability Zone in the AP-NORTHEAST-1 Region.
 9:47 PM PDT -  We can confirm that some instances are impaired and some EBS volumes are experiencing degraded performance within a single Availability Zone in the AP-NORTHEAST-1 Region. Some EC2 APIs are also experiencing increased error rates and latencies. We are working to resolve the issue.
10:27 PM PDT -  We have identified the root cause and are working toward recovery for the instance impairments and degraded EBS volume performance within a single Availability Zone in the AP-NORTHEAST-1 Region.
11:40 PM PDT -  We are starting to see recovery for instance impairments and degraded EBS volume performance within a single Availability Zone in the AP-NORTHEAST-1 Region. We continue to work towards recovery for all affected instances and EBS volumes.
Aug 23,  1:54 AM PDT -  Recovery is in progress for instance impairments and degraded EBS volume performance within a single Availability Zone in the AP-NORTHEAST-1 Region. We continue to work towards recovery for all affected instances and EBS volumes.
Aug 23,  2:39 AM PDT -  The majority of impaired EC2 instances and EBS volumes experiencing degraded performance have now recovered. We continue to work on recovery for the remaining EC2 instances and EBS volumes that are affected by this issue. This issue affects EC2 instances and EBS volumes in a single Availability Zone in the AP-NORTHEAST-1 region.
Aug 23,  4:18 AM PDT -  日本時間 2019年8月23日 12:36 より、AP-NORTHEAST-1 の単一のアベイラビリティゾーンで、一定の割合の EC2 サーバのオーバーヒートが発生しました。この結果、当該アベイラビリティゾーンの EC2 インスタンス及び EBS ボリュームのパフォーマンスの劣化が発生しました。このオーバーヒートは、影響を受けたアベイラビリティゾーン中の一部の冗長化された空調設備の管理システム障害が原因です。日本時間 15:21 に冷却装置は復旧し、室温が通常状態に戻り始めました。温度が通常状態に戻ったことで、影響を受けたインスタンスの電源が回復しました。日本時間 18:30 より大部分の EC2 インスタンスと EBS ボリュームは回復しました。 我々は残りの EC2 インスタンスと EBS ボリュームの回復に取り組んでいます。少数の EC2 インスタンスと EBS ボリュームが電源が落ちたハードウェア ホスト上に残されています。我々は影響をうけた全ての EC2 インスタンスと EBS ボリュームの回復のための作業を継続しています。 早期回復の為、可能な場合残された影響を受けている EC2 インスタンスと EBS ボリュームのリプレースを推奨します。いくつかの影響をうけた EC2 インスタンスはお客様側での作業が必要になる可能性がある為、 後ほどお客様個別にお知らせすることを予定しています。詳細は こちら をご参照ください。追加のご質問がある場合は、AWS サポートまでご連絡ください。 | Beginning at 8:36 PM PDT a small percentage of EC2 servers in a single Availability Zone in the AP-NORTHEAST-1 Region shutdown due to overheating. This resulted in impaired EC2 instances and degraded EBS volume performance for resources in the affected area of the Availability Zone. The overheating was caused by a control system failure that caused multiple, redundant cooling systems to fail in parts of the affected Availability Zone. The chillers were restored at 11:21 PM PDT and temperatures in the affected areas began to return to normal. As temperatures returned to normal, power was restored to the affected instances. By 2:30 AM PDT, the vast majority of instances and volumes had recovered. We have been working to recover the remaining instances and volumes. A small number of remaining instances and volumes are hosted on hardware which was adversely affected by the loss of power. We continue to work to recover all affected instances and volumes. For immediate recovery, we recommend replacing any remaining affected instances or volumes if possible. Some of the affected instances may require action from customers and we will be reaching out to those customers with next steps.Additional details are available here. If you have additional questions, please contact AWS Support."
181,Amazon Relational Database Service (Tokyo),インスタンスの接続性について | Instance Availability,1566537770,1,,"<div><span class=""yellowfg"">10:22 PM PDT</span>&nbsp;AWSでは、現在、東京リージョンの1つのアベイラビリティゾーンで発生している、複数インスタンスに対する接続性の問題について調査を進めております。| We are investigating connectivity issues affecting some instances in a single Availability Zone in the AP-NORTHEAST-1 Region.</div><div><span class=""yellowfg"">11:25 PM PDT</span>&nbsp;AWSでは、東京リージョンの1つのアベイラビリティゾーンで発生しているインスタンスの接続性の問題について原因を特定し、現在復旧に向けて対応を進めております。| We have identified the root cause of instance connectivity issues within a single Availability Zone in the AP-NORTHEAST-1 Region and are working toward recovery.</div><div><span class=""yellowfg"">Aug 23, 12:01 AM PDT</span>&nbsp;AWSでは、現在、東京リージョンの1つのアベイラビリティゾーンで発生しているインスタンスの接続性の問題ついて、復旧を開始しております。影響を受けている全てのインスタンスの復旧に向け、対応を継続いたします。| We are starting to see recovery for instance connectivity issues within a single Availability Zone in the AP-NORTHEAST-1 Region. We continue to work towards recovery for all affected instances.</div><div><span class=""yellowfg"">Aug 23,  2:16 AM PDT</span>&nbsp;AWSでは、現在、東京リージョンの1つのアベイラビリティゾーンで接続性の問題が生じている全てのインスタンスの復旧に向け、対応を進めております。| We continue to see recovery for instance connectivity issues within a single Availability Zone in the AP-NORTHEAST-1 Region and are working towards recovery for all affected instances.</div><div><span class=""yellowfg"">Aug 23,  6:19 AM PDT</span>&nbsp;日本時間 2019年8月23日 12:36 から 22:05 にかけて、東京リージョンの単一のアベイラビリティゾーンで一部の RDS インスタンスに接続性の問題が発生しました。現在、この問題は解消しており、サービスは正常稼働しております。<br><br>詳細は <a href=""https://aws.amazon.com/message/56489/"">こちら</a> をご参照ください。追加のご質問がある場合は、<a href=""https://aws.amazon.com/support"">AWS サポート</a>までご連絡ください。| Between August 22 8:36 PM and August 23 6:05 AM PDT, some RDS instances experienced connectivity issues within a single Availability Zone in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally. <br><br>Additional details are available <a href=""https://aws.amazon.com/message/56489/"">here</a>. If you have additional questions, please contact <a href=""https://aws.amazon.com/support"">AWS Support</a>.</div>",rds-ap-northeast-1,2019-08-23 05:22:50,Relational Database Service,Tokyo,8:36 PM,6:05 AM,PDT,569.0,2019,8,23,2019-08-23,instance availability,"10:22 PM PDT -  AWSでは、現在、東京リージョンの1つのアベイラビリティゾーンで発生している、複数インスタンスに対する接続性の問題について調査を進めております。| We are investigating connectivity issues affecting some instances in a single Availability Zone in the AP-NORTHEAST-1 Region.
11:25 PM PDT -  AWSでは、東京リージョンの1つのアベイラビリティゾーンで発生しているインスタンスの接続性の問題について原因を特定し、現在復旧に向けて対応を進めております。| We have identified the root cause of instance connectivity issues within a single Availability Zone in the AP-NORTHEAST-1 Region and are working toward recovery.
Aug 23, 12:01 AM PDT -  AWSでは、現在、東京リージョンの1つのアベイラビリティゾーンで発生しているインスタンスの接続性の問題ついて、復旧を開始しております。影響を受けている全てのインスタンスの復旧に向け、対応を継続いたします。| We are starting to see recovery for instance connectivity issues within a single Availability Zone in the AP-NORTHEAST-1 Region. We continue to work towards recovery for all affected instances.
Aug 23,  2:16 AM PDT -  AWSでは、現在、東京リージョンの1つのアベイラビリティゾーンで接続性の問題が生じている全てのインスタンスの復旧に向け、対応を進めております。| We continue to see recovery for instance connectivity issues within a single Availability Zone in the AP-NORTHEAST-1 Region and are working towards recovery for all affected instances.
Aug 23,  6:19 AM PDT -  日本時間 2019年8月23日 12:36 から 22:05 にかけて、東京リージョンの単一のアベイラビリティゾーンで一部の RDS インスタンスに接続性の問題が発生しました。現在、この問題は解消しており、サービスは正常稼働しております。詳細は こちら をご参照ください。追加のご質問がある場合は、AWS サポートまでご連絡ください。| Between August 22 8:36 PM and August 23 6:05 AM PDT, some RDS instances experienced connectivity issues within a single Availability Zone in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally. Additional details are available here. If you have additional questions, please contact AWS Support."
182,Amazon Elastic Compute Cloud (N. Virginia),[RESOLVED] Instances Unavailable,1567257765,1,,"<div><span class=""yellowfg""> 6:22 AM PDT</span>&nbsp;We are investigating connectivity issues affecting some instances in a single Availability Zone in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 6:54 AM PDT</span>&nbsp;We can confirm that some instances are impaired and some EBS volumes are experiencing degraded performance within a single Availability Zone in the US-EAST-1 Region. Some EC2 APIs are also experiencing increased error rates and latencies. We are working to resolve the issue.</div><div><span class=""yellowfg""> 7:37 AM PDT</span>&nbsp;We can confirm that some instances are impaired and some EBS volumes are experiencing degraded performance within a single Availability Zone in the US-EAST-1 Region. We are investigating increased error rates for new launches within the same Availability Zone. We are working to resolve the issue.</div><div><span class=""yellowfg""> 8:06 AM PDT</span>&nbsp;We are starting to see recovery for instance impairments and degraded EBS volume performance within a single Availability Zone in the US-EAST-1 Region. We are also starting to see recovery of EC2 APIs. We continue to work towards recovery for all affected EC2 instances and EBS volumes.</div><div><span class=""yellowfg""> 9:04 AM PDT</span>&nbsp;Recovery is in progress for instance impairments and degraded EBS volume performance within a single Availability Zone in the US-EAST-1 Region. We continue to work towards recovery for all remaining affected instances and EBS volumes. </div><div><span class=""yellowfg"">10:47 AM PDT</span>&nbsp;We want to give you more information on progress at this point, and what we know about the event. At 4:33 AM PDT one of 10 datacenters in one of the 6 Availability Zones in the US-EAST-1 Region saw a failure of utility power. Backup generators came online immediately, but for reasons we are still investigating, began quickly failing at around 6:00 AM PDT. This resulted in 7.5% of all instances in that Availability Zone failing by 6:10 AM PDT. Over the last few hours we have recovered most instances but still have 1.5% of the instances in that Availability Zone remaining to be recovered. Similar impact existed to EBS and we continue to recover volumes within EBS. New instance launches in this zone continue to work without issue.</div><div><span class=""yellowfg""> 1:30 PM PDT</span>&nbsp;At 4:33 AM PDT one of ten data centers in one of the six Availability Zones in the US-EAST-1 Region saw a failure of utility power. Our backup generators came online immediately but began failing at around 6:00 AM PDT. This impacted 7.5% of EC2 instances and EBS volumes in the Availability Zone. Power was fully restored to the impacted data center at 7:45 AM PDT. By 10:45 AM PDT, all but 1% of instances had been recovered, and by 12:30 PM PDT only 0.5% of instances remained impaired. Since the beginning of the impact, we have been working to recover the remaining instances and volumes. A small number of remaining instances and volumes are hosted on hardware which was adversely affected by the loss of power. We continue to work to recover all affected instances and volumes and will be communicating to the remaining impacted customers via the Personal Health Dashboard. For immediate recovery, we recommend replacing any remaining affected instances or volumes if possible. </div>",ec2-us-east-1,2019-08-31 13:22:45,Elastic Compute Cloud,N. Virginia,6:22 AM,12:30 PM,PDT,368.0,2019,8,31,2019-08-31,instances unavailable," 6:22 AM PDT -  We are investigating connectivity issues affecting some instances in a single Availability Zone in the US-EAST-1 Region.
 6:54 AM PDT -  We can confirm that some instances are impaired and some EBS volumes are experiencing degraded performance within a single Availability Zone in the US-EAST-1 Region. Some EC2 APIs are also experiencing increased error rates and latencies. We are working to resolve the issue.
 7:37 AM PDT -  We can confirm that some instances are impaired and some EBS volumes are experiencing degraded performance within a single Availability Zone in the US-EAST-1 Region. We are investigating increased error rates for new launches within the same Availability Zone. We are working to resolve the issue.
 8:06 AM PDT -  We are starting to see recovery for instance impairments and degraded EBS volume performance within a single Availability Zone in the US-EAST-1 Region. We are also starting to see recovery of EC2 APIs. We continue to work towards recovery for all affected EC2 instances and EBS volumes.
 9:04 AM PDT -  Recovery is in progress for instance impairments and degraded EBS volume performance within a single Availability Zone in the US-EAST-1 Region. We continue to work towards recovery for all remaining affected instances and EBS volumes. 
10:47 AM PDT -  We want to give you more information on progress at this point, and what we know about the event. At 4:33 AM PDT one of 10 datacenters in one of the 6 Availability Zones in the US-EAST-1 Region saw a failure of utility power. Backup generators came online immediately, but for reasons we are still investigating, began quickly failing at around 6:00 AM PDT. This resulted in 7.5% of all instances in that Availability Zone failing by 6:10 AM PDT. Over the last few hours we have recovered most instances but still have 1.5% of the instances in that Availability Zone remaining to be recovered. Similar impact existed to EBS and we continue to recover volumes within EBS. New instance launches in this zone continue to work without issue.
 1:30 PM PDT -  At 4:33 AM PDT one of ten data centers in one of the six Availability Zones in the US-EAST-1 Region saw a failure of utility power. Our backup generators came online immediately but began failing at around 6:00 AM PDT. This impacted 7.5% of EC2 instances and EBS volumes in the Availability Zone. Power was fully restored to the impacted data center at 7:45 AM PDT. By 10:45 AM PDT, all but 1% of instances had been recovered, and by 12:30 PM PDT only 0.5% of instances remained impaired. Since the beginning of the impact, we have been working to recover the remaining instances and volumes. A small number of remaining instances and volumes are hosted on hardware which was adversely affected by the loss of power. We continue to work to recover all affected instances and volumes and will be communicating to the remaining impacted customers via the Personal Health Dashboard. For immediate recovery, we recommend replacing any remaining affected instances or volumes if possible. "
183,Amazon WorkSpaces (N. Virginia),[RESOLVED] Connectivity Issues,1567259752,1,,"<div><span class=""yellowfg""> 6:55 AM PDT</span>&nbsp;We are investigating connectivity issues in a single Availability Zone in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 7:50 AM PDT</span>&nbsp;We can confirm that some WorkSpaces instances are impaired within a single Availability Zone in the US-EAST-1 Region. We are working to resolve the issue.</div><div><span class=""yellowfg""> 8:33 AM PDT</span>&nbsp;We are starting to see recovery of WorkSpaces instance impairments within a single Availability Zone in the US-EAST-1 Region. We continue to work towards recovery for all affected WorkSpaces instances.</div><div><span class=""yellowfg""> 2:02 PM PDT</span>&nbsp;Beginning at 6:15 AM PDT, we experienced connectivity issues affecting some WorkSpaces in a single Availability Zone in the US-EAST-1 Region. By 12:30 PM, the majority of the impaired WorkSpaces had been recovered. A small number of the impaired WorkSpaces were hosted on hardware which may have been adversely affected by the loss of power. We continue to work to recover all affected WorkSpaces. Some of the remaining impacted WorkSpaces may require action from customers and we will be communicating to the remaining impacted customers via the Personal Health Dashboard. </div>",workspaces-us-east-1,2019-08-31 13:55:52,WorkSpaces,N. Virginia,6:15 AM,12:30 PM,PDT,375.0,2019,8,31,2019-08-31,connectivity issues," 6:55 AM PDT -  We are investigating connectivity issues in a single Availability Zone in the US-EAST-1 Region.
 7:50 AM PDT -  We can confirm that some WorkSpaces instances are impaired within a single Availability Zone in the US-EAST-1 Region. We are working to resolve the issue.
 8:33 AM PDT -  We are starting to see recovery of WorkSpaces instance impairments within a single Availability Zone in the US-EAST-1 Region. We continue to work towards recovery for all affected WorkSpaces instances.
 2:02 PM PDT -  Beginning at 6:15 AM PDT, we experienced connectivity issues affecting some WorkSpaces in a single Availability Zone in the US-EAST-1 Region. By 12:30 PM, the majority of the impaired WorkSpaces had been recovered. A small number of the impaired WorkSpaces were hosted on hardware which may have been adversely affected by the loss of power. We continue to work to recover all affected WorkSpaces. Some of the remaining impacted WorkSpaces may require action from customers and we will be communicating to the remaining impacted customers via the Personal Health Dashboard. "
184,Amazon Relational Database Service (N. Virginia),[RESOLVED] Connectivity Issues,1567260966,1,,"<div><span class=""yellowfg""> 7:16 AM PDT</span>&nbsp;We are investigating connectivity issues affecting some single-AZ RDS instances in a single Availability Zone in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 8:25 AM PDT</span>&nbsp;We are starting to see recovery for connectivity issues impacting some single-AZ instances in a single Availability Zone in the US-EAST-1 Region. We continue to work towards recovery for all impacted instances. </div><div><span class=""yellowfg""> 2:15 PM PDT</span>&nbsp;Beginning at 5:40 AM PDT, some RDS single-AZ instances experienced connectivity issues within a single Availability Zone in the US-EAST-1 Region. By 11:55 AM, 1% of instances in the affected AZ were still experiencing connectivity issues. We continue to work to recover all affected instances and volumes and will be communicating to the remaining impacted customers via the Personal Health Dashboard. For customers who need immediate recovery, we recommend performing a <a href=""https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIT.html"">point in time</a> or <a href=""https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_RestoreFromSnapshot.html"">snapshot recovery</a> of your DB instance from the AWS Management Console or RDS APIs.</div>",rds-us-east-1,2019-08-31 14:16:06,Relational Database Service,N. Virginia,5:40 AM,11:55 AM,PDT,375.0,2019,8,31,2019-08-31,connectivity issues," 7:16 AM PDT -  We are investigating connectivity issues affecting some single-AZ RDS instances in a single Availability Zone in the US-EAST-1 Region.
 8:25 AM PDT -  We are starting to see recovery for connectivity issues impacting some single-AZ instances in a single Availability Zone in the US-EAST-1 Region. We continue to work towards recovery for all impacted instances. 
 2:15 PM PDT -  Beginning at 5:40 AM PDT, some RDS single-AZ instances experienced connectivity issues within a single Availability Zone in the US-EAST-1 Region. By 11:55 AM, 1% of instances in the affected AZ were still experiencing connectivity issues. We continue to work to recover all affected instances and volumes and will be communicating to the remaining impacted customers via the Personal Health Dashboard. For customers who need immediate recovery, we recommend performing a point in time or snapshot recovery of your DB instance from the AWS Management Console or RDS APIs."
185,Amazon WorkSpaces (Ireland),[RESOLVED] WorkSpaces Launch Errors,1568280002,1,,"<div><span class=""yellowfg""> 2:20 AM PDT</span>&nbsp;We are investigating increased errors when launching new WorkSpaces in the EU-WEST-1 Region.</div><div><span class=""yellowfg""> 2:49 AM PDT</span>&nbsp;Between September 11 6:35 PM and September 12 2:24 AM PDT we experienced increased errors when launching new WorkSpaces in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",workspaces-eu-west-1,2019-09-12 09:20:02,WorkSpaces,Ireland,6:35 PM,2:24 AM,PDT,469.0,2019,9,12,2019-09-12,workspaces launch errors," 2:20 AM PDT -  We are investigating increased errors when launching new WorkSpaces in the EU-WEST-1 Region.
 2:49 AM PDT -  Between September 11 6:35 PM and September 12 2:24 AM PDT we experienced increased errors when launching new WorkSpaces in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally."
186,Amazon Elastic Compute Cloud (N. Virginia),[RESOLVED] Increased API Error Rates and Latencies,1568389951,1,,"<div><span class=""yellowfg""> 8:52 AM PDT</span>&nbsp;Between 7:49AM PDT and 8:27AM PDT EC2 experienced elevated API errors for instance related APIs in a single Availability Zone in the US-EAST-1 region. Existing instances were not affected. The issue has been resolved and the service is operating normally.</div>",ec2-us-east-1,2019-09-13 15:52:31,Elastic Compute Cloud,N. Virginia,7:49AM,8:27AM PDT,PDT,38.0,2019,9,13,2019-09-13,increased api error rates and latencies, 8:52 AM PDT -  Between 7:49AM PDT and 8:27AM PDT EC2 experienced elevated API errors for instance related APIs in a single Availability Zone in the US-EAST-1 region. Existing instances were not affected. The issue has been resolved and the service is operating normally.
187,Amazon Elastic Compute Cloud (N. Virginia),[RESOLVED] Increased API Error Rates and Latencies,1568742586,1,,"<div><span class=""yellowfg"">10:49 AM PDT</span>&nbsp;Between 9:45 AM and 10:25 AM PDT we experienced increased API error rates and latencies in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",ec2-us-east-1,2019-09-17 17:49:46,Elastic Compute Cloud,N. Virginia,9:45 AM,10:25 AM,PDT,40.0,2019,9,17,2019-09-17,increased api error rates and latencies,10:49 AM PDT -  Between 9:45 AM and 10:25 AM PDT we experienced increased API error rates and latencies in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.
188,Amazon CloudFront,[RESOLVED] Increased Invalidation Error Rates,1568887640,1,,"<div><span class=""yellowfg""> 3:07 AM PDT</span>&nbsp;We are investigating elevated error rates for content invalidation. Some customers may also receive ""Rate exceeded"" exceptions when attempting to invalidate content. End-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=""yellowfg""> 3:36 AM PDT</span>&nbsp;Between September 18 10:58 PM and September 19 2:44 AM PDT, we experienced increased API error rates impacting CreateInvalidation API. Some customers may have also received ""Rate exceeded"" exceptions when attempting to invalidate content. End-user requests for content were not affected by this issue, and content from our edge locations were not affected and continue to be served normally. The issue has been resolved and the service is operating normally.</div>",cloudfront,2019-09-19 10:07:20,CloudFront,Global,10:58 PM,2:44 AM,PDT,226.0,2019,9,19,2019-09-19,increased invalidation error rates," 3:07 AM PDT -  We are investigating elevated error rates for content invalidation. Some customers may also receive ""Rate exceeded"" exceptions when attempting to invalidate content. End-user requests for content from our edge locations are not affected by this issue and are being served normally.
 3:36 AM PDT -  Between September 18 10:58 PM and September 19 2:44 AM PDT, we experienced increased API error rates impacting CreateInvalidation API. Some customers may have also received ""Rate exceeded"" exceptions when attempting to invalidate content. End-user requests for content were not affected by this issue, and content from our edge locations were not affected and continue to be served normally. The issue has been resolved and the service is operating normally."
189,Amazon Relational Database Service (N. Virginia),[RESOLVED] Increased Error Rates,1569184140,1,,"<div><span class=""yellowfg""> 2:32 PM PDT</span>&nbsp;We are investigating increased API error rates in the US-EAST-1 Region</div><div><span class=""yellowfg""> 3:34 PM PDT</span>&nbsp;We continue to investigate increased error rates with Console and API calls in the US-EAST-1 Region. Connectivity to currently running instances is not affected. </div><div><span class=""yellowfg""> 4:39 PM PDT</span>&nbsp;We continue to see increased error rates with Console and API calls in the US-EAST-1 Region and are working towards recovery. Connectivity to currently running instances is not affected.</div><div><span class=""yellowfg""> 7:07 PM PDT</span>&nbsp;We have identified the issue causing increased error rates with RDS Console and API calls in the US-EAST-1 Region and are working towards recovery.</div><div><span class=""yellowfg""> 8:06 PM PDT</span>&nbsp;Between 1:29 PM and 7:34 PM PDT we experienced increased error rates for the RDS Management Console and API calls. The issue is resolved and the service is operating normally.</div>",rds-us-east-1,2019-09-22 20:29:00,Relational Database Service,N. Virginia,1:29 PM,7:34 PM,PDT,365.0,2019,9,22,2019-09-22,increased error rates," 2:32 PM PDT -  We are investigating increased API error rates in the US-EAST-1 Region
 3:34 PM PDT -  We continue to investigate increased error rates with Console and API calls in the US-EAST-1 Region. Connectivity to currently running instances is not affected. 
 4:39 PM PDT -  We continue to see increased error rates with Console and API calls in the US-EAST-1 Region and are working towards recovery. Connectivity to currently running instances is not affected.
 7:07 PM PDT -  We have identified the issue causing increased error rates with RDS Console and API calls in the US-EAST-1 Region and are working towards recovery.
 8:06 PM PDT -  Between 1:29 PM and 7:34 PM PDT we experienced increased error rates for the RDS Management Console and API calls. The issue is resolved and the service is operating normally."
190,AWS Management Console,[RESOLVED] Increased Error Rates,1569187417,1,,"<div><span class=""yellowfg""> 2:23 PM PDT</span>&nbsp;We are investigating increased error rates in the US-EAST-1 Region for RDS and Autoscaling Management Consoles.</div><div><span class=""yellowfg""> 3:33 PM PDT</span>&nbsp;We are continuing to investigate elevated error rates with the RDS Management Console in the US-EAST-1 Region, Autoscaling Console has recovered and is operating normally.</div><div><span class=""yellowfg""> 4:43 PM PDT</span>&nbsp;We are continuing to investigate elevated error rates with the RDS Management Console in the US-EAST-1 Region. All other consoles are operating normally.</div><div><span class=""yellowfg""> 6:32 PM PDT</span>&nbsp;We continue to see increased error rates with the RDS Console in the US-EAST-1 Region and are working towards recovery</div><div><span class=""yellowfg""> 7:10 PM PDT</span>&nbsp;We have identified the issue causing increased error rates with the RDS Management Console in the US-EAST-1 Region and are working towards recovery. </div><div><span class=""yellowfg""> 8:12 PM PDT</span>&nbsp;Between 1:29 PM and 7:34 PM PDT, we experienced increased error rates when working with the RDS Management Console in the US-EAST-1 region. The issue has been resolved and the Management Console is operating normally.</div>",management-console,2019-09-22 21:23:37,Management Console,Global,1:29 PM,7:34 PM,PDT,365.0,2019,9,22,2019-09-22,increased error rates," 2:23 PM PDT -  We are investigating increased error rates in the US-EAST-1 Region for RDS and Autoscaling Management Consoles.
 3:33 PM PDT -  We are continuing to investigate elevated error rates with the RDS Management Console in the US-EAST-1 Region, Autoscaling Console has recovered and is operating normally.
 4:43 PM PDT -  We are continuing to investigate elevated error rates with the RDS Management Console in the US-EAST-1 Region. All other consoles are operating normally.
 6:32 PM PDT -  We continue to see increased error rates with the RDS Console in the US-EAST-1 Region and are working towards recovery
 7:10 PM PDT -  We have identified the issue causing increased error rates with the RDS Management Console in the US-EAST-1 Region and are working towards recovery. 
 8:12 PM PDT -  Between 1:29 PM and 7:34 PM PDT, we experienced increased error rates when working with the RDS Management Console in the US-EAST-1 region. The issue has been resolved and the Management Console is operating normally."
191,Amazon API Gateway (N. Virginia),[RESOLVED] Increased Error Rates,1569529335,1,,"<div><span class=""yellowfg""> 1:22 PM PDT</span>&nbsp;We are investigating increased error rates for API Management calls via the console, CLI, and CloudFormation in the US-EAST-1 Region. There is no impact to invocations of deployed APIs.</div><div><span class=""yellowfg""> 2:04 PM PDT</span>&nbsp;Between 12:22 PM and 1:47 PM PDT we experienced increased error rates for API Management calls via the console, CLI, and CloudFormation in the US-EAST-1 Region. There was no impact to invocations of deployed APIs. The issue is resolved, and the service is operating normally.</div>",apigateway-us-east-1,2019-09-26 20:22:15,API Gateway,N. Virginia,12:22 PM,1:47 PM,PDT,85.0,2019,9,26,2019-09-26,increased error rates," 1:22 PM PDT -  We are investigating increased error rates for API Management calls via the console, CLI, and CloudFormation in the US-EAST-1 Region. There is no impact to invocations of deployed APIs.
 2:04 PM PDT -  Between 12:22 PM and 1:47 PM PDT we experienced increased error rates for API Management calls via the console, CLI, and CloudFormation in the US-EAST-1 Region. There was no impact to invocations of deployed APIs. The issue is resolved, and the service is operating normally."
192,AWS Internet Connectivity (Singapore),[RESOLVED] Network Connectivity,1569965515,1,,"<div><span class=""yellowfg""> 2:31 PM PDT</span>&nbsp;We are investigating network connectivity issues for some AWS services in the AP-SOUTHEAST-1 Region.</div><div><span class=""yellowfg""> 3:07 PM PDT</span>&nbsp; Between 1:51 PM  and 2:41 PM PDT we experienced network connectivity issues for some AWS APIs in the AP-SOUTHEAST-1 Region. The issue has been resolved and the service is operating normally.</div>",internetconnectivity-ap-southeast-1,2019-10-01 21:31:55,Internet Connectivity,Singapore,1:51 PM,2:41 PM,PDT,50.0,2019,10,1,2019-10-01,network connectivity," 2:31 PM PDT -  We are investigating network connectivity issues for some AWS services in the AP-SOUTHEAST-1 Region.
 3:07 PM PDT -   Between 1:51 PM  and 2:41 PM PDT we experienced network connectivity issues for some AWS APIs in the AP-SOUTHEAST-1 Region. The issue has been resolved and the service is operating normally."
193,Amazon Elastic Compute Cloud (Singapore),[RESOLVED] Network Connectivity and API Error Rates,1569966223,1,,"<div><span class=""yellowfg""> 2:43 PM PDT</span>&nbsp;We are investigating connectivity issues and increased API error rates in the AP-SOUTHEAST-1 Region.</div><div><span class=""yellowfg""> 3:17 PM PDT</span>&nbsp;Between 1:51 PM and 2:41 PM PDT we experienced API error rates for the EC2 APIs and the EC2 Management Console in the AP-SOUTHEAST-1 Region. Connectivity to EC2 instances was not impacted during this event. The issue has been resolved and the service is operating normally.</div>",ec2-ap-southeast-1,2019-10-01 21:43:43,Elastic Compute Cloud,Singapore,1:51 PM,2:41 PM,PDT,50.0,2019,10,1,2019-10-01,network connectivity and api error rates," 2:43 PM PDT -  We are investigating connectivity issues and increased API error rates in the AP-SOUTHEAST-1 Region.
 3:17 PM PDT -  Between 1:51 PM and 2:41 PM PDT we experienced API error rates for the EC2 APIs and the EC2 Management Console in the AP-SOUTHEAST-1 Region. Connectivity to EC2 instances was not impacted during this event. The issue has been resolved and the service is operating normally."
194,AWS Batch (Ireland),[RESOLVED] Increased API error rates and latencies,1570041989,2,,"<div><span class=""yellowfg"">11:46 AM PDT</span>&nbsp;We are investigating increased API error rates and latencies for the AWS Batch APIs in the EU-WEST-1 Region. Compute Resource connectivity and running jobs are not affected. </div><div><span class=""yellowfg"">12:15 PM PDT</span>&nbsp;Between 10:35 AM and 11:59 AM PDT we experienced increased error rates and latencies for all AWS Batch APIs in the EU-WEST-1 Region. Compute Resource connectivity and running jobs were not affected. The issue has been resolved and the service is operating normally.</div>",batch-eu-west-1,2019-10-02 18:46:29,Batch,Ireland,10:35 AM,11:59 AM,PDT,84.0,2019,10,2,2019-10-02,increased api error rates and latencies,"11:46 AM PDT -  We are investigating increased API error rates and latencies for the AWS Batch APIs in the EU-WEST-1 Region. Compute Resource connectivity and running jobs are not affected. 
12:15 PM PDT -  Between 10:35 AM and 11:59 AM PDT we experienced increased error rates and latencies for all AWS Batch APIs in the EU-WEST-1 Region. Compute Resource connectivity and running jobs were not affected. The issue has been resolved and the service is operating normally."
195,AWS IoT Core (Ireland),[RESOLVED] Increased API error rates,1570042403,1,,"<div><span class=""yellowfg"">11:53 AM PDT</span>&nbsp;We are investigating increased API error rates for AWS IoT Core APIs for managing Identities, Policies, Things and Rules in the EU-WEST-1 region. Device connections, messaging and rules executions are not impacted.</div><div><span class=""yellowfg"">12:18 PM PDT</span>&nbsp;Between 10:52 AM and 11:57 AM PDT, we experienced elevated API error rates for managing Identities, Policies, Things and Rules in the EU-WEST-1 region. We have resolved the issue and the service is operating normally. </div>",awsiot-eu-west-1,2019-10-02 18:53:23,IoT Core,Ireland,10:52 AM,11:57 AM,PDT,65.0,2019,10,2,2019-10-02,increased api error rates,"11:53 AM PDT -  We are investigating increased API error rates for AWS IoT Core APIs for managing Identities, Policies, Things and Rules in the EU-WEST-1 region. Device connections, messaging and rules executions are not impacted.
12:18 PM PDT -  Between 10:52 AM and 11:57 AM PDT, we experienced elevated API error rates for managing Identities, Policies, Things and Rules in the EU-WEST-1 region. We have resolved the issue and the service is operating normally. "
196,Amazon API Gateway (Ireland),[RESOLVED] Increase API error rates,1570043240,1,,"<div><span class=""yellowfg"">12:07 PM PDT</span>&nbsp;We are investigating increased error rates for API invokes in the EU-WEST-1 Region.</div><div><span class=""yellowfg""> 1:19 PM PDT</span>&nbsp;We have identified the root cause of the issue causing increased error rates in the EU-WEST-1 Region and are working on a resolution.</div><div><span class=""yellowfg""> 1:32 PM PDT</span>&nbsp;Between 10:23 AM and 1:11 PM PDT we experienced increased error rates in the EU-WEST-1 Region. The issue is resolved and the service is operating normally.</div>",apigateway-eu-west-1,2019-10-02 19:07:20,API Gateway,Ireland,10:23 AM,1:11 PM,PDT,168.0,2019,10,2,2019-10-02,increase api error rates,"12:07 PM PDT -  We are investigating increased error rates for API invokes in the EU-WEST-1 Region.
 1:19 PM PDT -  We have identified the root cause of the issue causing increased error rates in the EU-WEST-1 Region and are working on a resolution.
 1:32 PM PDT -  Between 10:23 AM and 1:11 PM PDT we experienced increased error rates in the EU-WEST-1 Region. The issue is resolved and the service is operating normally."
197,Amazon CloudWatch (N. Virginia),[RESOLVED] Elevated Error Rates,1570238568,1,,"<div><span class=""yellowfg""> 6:22 PM PDT</span>&nbsp;We are investigating elevated rate of API faults and delays in processing some alarms in the US-EAST-1 Region. We are actively working to resolve the issue.</div><div><span class=""yellowfg""> 6:42 PM PDT</span>&nbsp;We can confirm that scheduled events are not being delivered via CloudWatch Events in the US-EAST-1 Region. We are actively working to resolve the issue. </div><div><span class=""yellowfg""> 6:49 PM PDT</span>&nbsp;We can confirm that scheduled events are not being delivered via CloudWatch Events in the US-EAST-1 Region. AWS Lambda functions that are invoked in response to CloudWatch events may not be getting invoked successfully. We are actively working to resolve the issue.</div><div><span class=""yellowfg""> 7:34 PM PDT</span>&nbsp;We have identified the root cause of the errors in scheduled event creation. We can confirm that AWS Lambda functions that are invoked in response to scheduled CloudWatch events are not getting invoked successfully. We are actively working towards recovery.</div><div><span class=""yellowfg""> 8:49 PM PDT</span>&nbsp;We are starting to see recovery of scheduled CloudWatch event delivery. We are also starting to see recovery of AWS Lambda functions that were set up to be invoked from scheduled CloudWatch events. </div><div><span class=""yellowfg""> 9:48 PM PDT</span>&nbsp;Starting at 5:27 PM PDT, there were delays delivering scheduled CloudWatch events. Recovery began at 7:31 PM PDT, and by 9:28 PM PDT the backlog of all previously scheduled CloudWatch events was successfully delivered. AWS Lambda functions that were configured to be triggered from scheduled CloudWatch events are now being invoked normally. During this entire duration all other kinds of CloudWatch events and other Lambda function invocations continued to operate normally. All issues are fully resolved and the services are operating normally.</div>",cloudwatch-us-east-1,2019-10-05 01:22:48,CloudWatch,N. Virginia,5:27 PM,9:28 PM,PDT,241.0,2019,10,5,2019-10-05,elevated error rates," 6:22 PM PDT -  We are investigating elevated rate of API faults and delays in processing some alarms in the US-EAST-1 Region. We are actively working to resolve the issue.
 6:42 PM PDT -  We can confirm that scheduled events are not being delivered via CloudWatch Events in the US-EAST-1 Region. We are actively working to resolve the issue. 
 6:49 PM PDT -  We can confirm that scheduled events are not being delivered via CloudWatch Events in the US-EAST-1 Region. AWS Lambda functions that are invoked in response to CloudWatch events may not be getting invoked successfully. We are actively working to resolve the issue.
 7:34 PM PDT -  We have identified the root cause of the errors in scheduled event creation. We can confirm that AWS Lambda functions that are invoked in response to scheduled CloudWatch events are not getting invoked successfully. We are actively working towards recovery.
 8:49 PM PDT -  We are starting to see recovery of scheduled CloudWatch event delivery. We are also starting to see recovery of AWS Lambda functions that were set up to be invoked from scheduled CloudWatch events. 
 9:48 PM PDT -  Starting at 5:27 PM PDT, there were delays delivering scheduled CloudWatch events. Recovery began at 7:31 PM PDT, and by 9:28 PM PDT the backlog of all previously scheduled CloudWatch events was successfully delivered. AWS Lambda functions that were configured to be triggered from scheduled CloudWatch events are now being invoked normally. During this entire duration all other kinds of CloudWatch events and other Lambda function invocations continued to operate normally. All issues are fully resolved and the services are operating normally."
198,Amazon CloudWatch (Ireland),[RESOLVED] Elevated Error Rates ,1570239838,1,,"<div><span class=""yellowfg""> 6:43 PM PDT</span>&nbsp;We can confirm that scheduled events are not being delivered via CloudWatch Events in the EU-WEST-1 Region. AWS Lambda functions that are invoked in response to CloudWatch events may not be getting invoked successfully. We are actively working to resolve the issue.</div><div><span class=""yellowfg""> 7:36 PM PDT</span>&nbsp;We have identified the root cause of the errors in scheduled event creation. We can confirm that AWS Lambda functions that are invoked in response to scheduled CloudWatch events are not getting invoked successfully. We are actively working towards recovery. </div><div><span class=""yellowfg""> 8:46 PM PDT</span>&nbsp;We are starting to see recovery of scheduled CloudWatch event delivery. We are also starting to see recovery of AWS Lambda functions that were set up to be invoked from scheduled CloudWatch events. </div><div><span class=""yellowfg""> 9:44 PM PDT</span>&nbsp;Starting at 5:27 PM PDT, there were delays delivering scheduled CloudWatch events. Recovery began at 8:55 PM PDT, and we are currently working to deliver the backlog of all previously scheduled CloudWatch events. All other kinds of CloudWatch events and other Lambda function invocations continue to operate normally.</div><div><span class=""yellowfg"">10:19 PM PDT</span>&nbsp;Starting at 5:27 PM PDT, there were delays delivering scheduled CloudWatch events. Recovery began at 8:55 PM PDT, and by 10:15 PM PDT the backlog of all previously scheduled CloudWatch events was successfully delivered. AWS Lambda functions that were configured to be triggered from scheduled CloudWatch events are now being invoked normally. During this entire duration all other kinds of CloudWatch events and other Lambda function invocations continued to operate normally. All issues are fully resolved and the services are operating normally.</div>",cloudwatch-eu-west-1,2019-10-05 01:43:58,CloudWatch,Ireland,5:27 PM,10:15 PM,PDT,288.0,2019,10,5,2019-10-05,elevated error rates," 6:43 PM PDT -  We can confirm that scheduled events are not being delivered via CloudWatch Events in the EU-WEST-1 Region. AWS Lambda functions that are invoked in response to CloudWatch events may not be getting invoked successfully. We are actively working to resolve the issue.
 7:36 PM PDT -  We have identified the root cause of the errors in scheduled event creation. We can confirm that AWS Lambda functions that are invoked in response to scheduled CloudWatch events are not getting invoked successfully. We are actively working towards recovery. 
 8:46 PM PDT -  We are starting to see recovery of scheduled CloudWatch event delivery. We are also starting to see recovery of AWS Lambda functions that were set up to be invoked from scheduled CloudWatch events. 
 9:44 PM PDT -  Starting at 5:27 PM PDT, there were delays delivering scheduled CloudWatch events. Recovery began at 8:55 PM PDT, and we are currently working to deliver the backlog of all previously scheduled CloudWatch events. All other kinds of CloudWatch events and other Lambda function invocations continue to operate normally.
10:19 PM PDT -  Starting at 5:27 PM PDT, there were delays delivering scheduled CloudWatch events. Recovery began at 8:55 PM PDT, and by 10:15 PM PDT the backlog of all previously scheduled CloudWatch events was successfully delivered. AWS Lambda functions that were configured to be triggered from scheduled CloudWatch events are now being invoked normally. During this entire duration all other kinds of CloudWatch events and other Lambda function invocations continued to operate normally. All issues are fully resolved and the services are operating normally."
199,Amazon CloudWatch (Oregon),[RESOLVED] Elevated Error Rates,1570239912,1,,"<div><span class=""yellowfg""> 6:45 PM PDT</span>&nbsp;We can confirm that scheduled events are not being delivered via CloudWatch Events in the US-WEST-2 Region. We are actively working to resolve the issue. </div><div><span class=""yellowfg""> 6:51 PM PDT</span>&nbsp;We can confirm that scheduled events are not being delivered via CloudWatch Events in the US-WEST-2. AWS Lambda functions that are invoked in response to CloudWatch events may not be getting invoked successfully. We are actively working to resolve the issue.

</div><div><span class=""yellowfg""> 7:35 PM PDT</span>&nbsp;We have identified the root cause of the errors in scheduled event creation. We can confirm that AWS Lambda functions that are invoked in response to scheduled CloudWatch events are not getting invoked successfully. We are actively working towards recovery. </div><div><span class=""yellowfg""> 8:46 PM PDT</span>&nbsp;We are starting to see recovery of scheduled CloudWatch event delivery. We are also starting to see recovery of AWS Lambda functions that were set up to be invoked from scheduled CloudWatch events. </div><div><span class=""yellowfg""> 9:37 PM PDT</span>&nbsp;Starting at 5:27 PM PDT, there were delays delivering scheduled CloudWatch events. Recovery began at 7:36 PM PDT, and by 9:22 PM PDT the backlog of all previously scheduled CloudWatch events was successfully delivered. AWS Lambda functions that were configured to be triggered from scheduled CloudWatch events are now being invoked normally. During this entire duration all other kinds of CloudWatch events and other Lambda function invocations continued to operate normally. All issues are fully resolved and the services are operating normally.</div>",cloudwatch-us-west-2,2019-10-05 01:45:12,CloudWatch,Oregon,5:27 PM,9:22 PM,PDT,235.0,2019,10,5,2019-10-05,elevated error rates," 6:45 PM PDT -  We can confirm that scheduled events are not being delivered via CloudWatch Events in the US-WEST-2 Region. We are actively working to resolve the issue. 
 6:51 PM PDT -  We can confirm that scheduled events are not being delivered via CloudWatch Events in the US-WEST-2. AWS Lambda functions that are invoked in response to CloudWatch events may not be getting invoked successfully. We are actively working to resolve the issue.
 7:35 PM PDT -  We have identified the root cause of the errors in scheduled event creation. We can confirm that AWS Lambda functions that are invoked in response to scheduled CloudWatch events are not getting invoked successfully. We are actively working towards recovery. 
 8:46 PM PDT -  We are starting to see recovery of scheduled CloudWatch event delivery. We are also starting to see recovery of AWS Lambda functions that were set up to be invoked from scheduled CloudWatch events. 
 9:37 PM PDT -  Starting at 5:27 PM PDT, there were delays delivering scheduled CloudWatch events. Recovery began at 7:36 PM PDT, and by 9:22 PM PDT the backlog of all previously scheduled CloudWatch events was successfully delivered. AWS Lambda functions that were configured to be triggered from scheduled CloudWatch events are now being invoked normally. During this entire duration all other kinds of CloudWatch events and other Lambda function invocations continued to operate normally. All issues are fully resolved and the services are operating normally."
200,AWS Backup (Frankfurt),[RESOLVED] Increased API Error Rates,1570493040,1,,"<div><span class=""yellowfg""> 7:00 PM PDT</span>&nbsp;We are investigating increased API error rates in the EU-CENTRAL-1 Region.</div><div><span class=""yellowfg""> 7:36 PM PDT</span>&nbsp;We continue to investigate increased API error rates in the EU-CENTRAL-1 Region.</div><div><span class=""yellowfg""> 9:32 PM PDT</span>&nbsp;We have identified the cause of the increased API error rates in the EU-CENTRAL-1 Region and continue working towards resolution.</div><div><span class=""yellowfg"">10:09 PM PDT</span>&nbsp;Between 5:04 PM and 9:17 PM PDT we experienced increased API error rates in the EU-CENTRAL-1 Region. The issue has been resolved and the service is operating normally.</div>",backup-eu-central-1,2019-10-08 00:04:00,Backup,Frankfurt,5:04 PM,9:17 PM,PDT,253.0,2019,10,8,2019-10-08,increased api error rates," 7:00 PM PDT -  We are investigating increased API error rates in the EU-CENTRAL-1 Region.
 7:36 PM PDT -  We continue to investigate increased API error rates in the EU-CENTRAL-1 Region.
 9:32 PM PDT -  We have identified the cause of the increased API error rates in the EU-CENTRAL-1 Region and continue working towards resolution.
10:09 PM PDT -  Between 5:04 PM and 9:17 PM PDT we experienced increased API error rates in the EU-CENTRAL-1 Region. The issue has been resolved and the service is operating normally."
201,Amazon Simple Storage Service (N. Virginia),[RESOLVED] Increased Error Rates for Bucket Operations,1570551545,1,,"<div><span class=""yellowfg""> 9:19 AM PDT</span>&nbsp;We have identified the cause of the increased error rates for customers creating and managing Amazon S3 buckets and are working towards resolution. Only a small subset of requests are affected and retries are working.</div><div><span class=""yellowfg""> 9:30 AM PDT</span>&nbsp;Between 7:54 AM and 9:18 AM PDT, S3 customers saw elevated errors and latency for a subset of API requests to create and manage Amazon S3 buckets. The issue has been resolved and the service is operating normally.</div>",s3-us-standard,2019-10-08 16:19:05,Simple Storage Service,N. Virginia,7:54 AM,9:18 AM,PDT,84.0,2019,10,8,2019-10-08,increased error rates for bucket operations," 9:19 AM PDT -  We have identified the cause of the increased error rates for customers creating and managing Amazon S3 buckets and are working towards resolution. Only a small subset of requests are affected and retries are working.
 9:30 AM PDT -  Between 7:54 AM and 9:18 AM PDT, S3 customers saw elevated errors and latency for a subset of API requests to create and manage Amazon S3 buckets. The issue has been resolved and the service is operating normally."
202,AWS Backup (Frankfurt),[RESOLVED] Increased API Error Rates,1570579440,1,,"<div><span class=""yellowfg"">Oct 7,  7:00 PM PDT</span>&nbsp;We are investigating increased API error rates in the EU-CENTRAL-1 Region.</div><div><span class=""yellowfg"">Oct 7,  7:36 PM PDT</span>&nbsp;We continue to investigate increased API error rates in the EU-CENTRAL-1 Region.</div><div><span class=""yellowfg"">Oct 7,  9:32 PM PDT</span>&nbsp;We have identified the cause of the increased API error rates in the EU-CENTRAL-1 Region and continue working towards resolution.</div><div><span class=""yellowfg"">Oct 7, 10:09 PM PDT</span>&nbsp;Between 5:04 PM and 9:17 PM PDT we experienced increased API error rates in the EU-CENTRAL-1 Region. The issue has been resolved and the service is operating normally.</div>",backup-eu-central-1,2019-10-09 00:04:00,Backup,Frankfurt,5:04 PM,9:17 PM,PDT,253.0,2019,10,9,2019-10-09,increased api error rates,"Oct 7,  7:00 PM PDT -  We are investigating increased API error rates in the EU-CENTRAL-1 Region.
Oct 7,  7:36 PM PDT -  We continue to investigate increased API error rates in the EU-CENTRAL-1 Region.
Oct 7,  9:32 PM PDT -  We have identified the cause of the increased API error rates in the EU-CENTRAL-1 Region and continue working towards resolution.
Oct 7, 10:09 PM PDT -  Between 5:04 PM and 9:17 PM PDT we experienced increased API error rates in the EU-CENTRAL-1 Region. The issue has been resolved and the service is operating normally."
203,Amazon Route 53,[RESOLVED] Increased API error rates,1571588700,1,,"<div><span class=""yellowfg"">10:16 AM PDT</span>&nbsp;We are investigating increased errors when making changes to some hosted zones, and creation of new hosted zones using the Route 53 API. DNS queries are not affected by this issue, which are all being answered as normal.</div><div><span class=""yellowfg"">10:46 AM PDT</span>&nbsp;We are continuing to investigate elevated API errors when making changes to some hosted zones, and creation of new hosted zones using the Route 53 API. DNS queries are not affected by this issue, which are all being answered normally.</div><div><span class=""yellowfg"">11:26 AM PDT</span>&nbsp;We have identified root cause and are working towards recovery of elevated error rates using the Route 53 API. DNS queries are not affected by this issue, which are all being answered normally.</div><div><span class=""yellowfg"">11:33 AM PDT</span>&nbsp;Between 9:25 AM and 11:30 AM PDT, we experienced increased errors when making changes to some hosted zones, and creating new hosted zones using the Route 53 API and Console. The issue has been recovered, and all services are operating normally. DNS queries were not affected by this issue, which were all answered normally.</div>",route53,2019-10-20 16:25:00,Route 53,Global,9:25 AM,11:30 AM,PDT,125.0,2019,10,20,2019-10-20,increased api error rates,"10:16 AM PDT -  We are investigating increased errors when making changes to some hosted zones, and creation of new hosted zones using the Route 53 API. DNS queries are not affected by this issue, which are all being answered as normal.
10:46 AM PDT -  We are continuing to investigate elevated API errors when making changes to some hosted zones, and creation of new hosted zones using the Route 53 API. DNS queries are not affected by this issue, which are all being answered normally.
11:26 AM PDT -  We have identified root cause and are working towards recovery of elevated error rates using the Route 53 API. DNS queries are not affected by this issue, which are all being answered normally.
11:33 AM PDT -  Between 9:25 AM and 11:30 AM PDT, we experienced increased errors when making changes to some hosted zones, and creating new hosted zones using the Route 53 API and Console. The issue has been recovered, and all services are operating normally. DNS queries were not affected by this issue, which were all answered normally."
204,Amazon Route 53,[RESOLVED] Increased API error rates,1571591808,1,,"<div><span class=""yellowfg"">10:16 AM PDT</span>&nbsp;We are investigating increased errors when making changes to some hosted zones, and creation of new hosted zones using the Route 53 API. DNS queries are not affected by this issue, which are all being answered as normal.</div><div><span class=""yellowfg"">10:46 AM PDT</span>&nbsp;We are continuing to investigate elevated API errors when making changes to some hosted zones, and creation of new hosted zones using the Route 53 API. DNS queries are not affected by this issue, which are all being answered normally.</div><div><span class=""yellowfg"">11:26 AM PDT</span>&nbsp;We have identified root cause and are working towards recovery of elevated error rates using the Route 53 API. DNS queries are not affected by this issue, which are all being answered normally.</div><div><span class=""yellowfg"">11:33 AM PDT</span>&nbsp;Between 9:25 AM and 11:30 AM PDT, we experienced increased errors when making changes to some hosted zones, and creating new hosted zones using the Route 53 API and Console. The issue has been recovered, and all services are operating normally. DNS queries were not affected by this issue, which were all answered normally.</div>",route53,2019-10-20 17:16:48,Route 53,Global,9:25 AM,11:30 AM,PDT,125.0,2019,10,20,2019-10-20,increased api error rates,"10:16 AM PDT -  We are investigating increased errors when making changes to some hosted zones, and creation of new hosted zones using the Route 53 API. DNS queries are not affected by this issue, which are all being answered as normal.
10:46 AM PDT -  We are continuing to investigate elevated API errors when making changes to some hosted zones, and creation of new hosted zones using the Route 53 API. DNS queries are not affected by this issue, which are all being answered normally.
11:26 AM PDT -  We have identified root cause and are working towards recovery of elevated error rates using the Route 53 API. DNS queries are not affected by this issue, which are all being answered normally.
11:33 AM PDT -  Between 9:25 AM and 11:30 AM PDT, we experienced increased errors when making changes to some hosted zones, and creating new hosted zones using the Route 53 API and Console. The issue has been recovered, and all services are operating normally. DNS queries were not affected by this issue, which were all answered normally."
205,Amazon Route 53,[RESOLVED] Further information regarding DNS resolution,1571877848,1,,"<div><span class=""yellowfg""> 5:44 PM PDT</span>&nbsp;On October 22, 2019, we detected and then mitigated a DDoS (Distributed Denial of Service) attack against Route 53. Due to the way that DNS queries are processed, this attack was first experienced by many other DNS server operators as the queries made their way through DNS resolvers on the internet to Route 53. The attack targeted specific DNS names and paths, notably those used to access the global names for S3 buckets.

Because this attack was widely distributed, a small number of ISPs operating affected DNS resolvers implemented mitigation strategies of their own in an attempt to control the traffic. This is causing DNS lookups through these resolvers for a small number of AWS names to fail. We are doing our best to identify and contact these operators, as quickly as possible, and working with them to enhance their mitigations so that they do not cause impact to valid requests. If you are experiencing issues, please contact us so we can work with your operator to help resolve.</div>",route53,2019-10-24 00:44:08,Route 53,Global,5:44 PM,5:44 PM,PDT,0.0,2019,10,24,2019-10-24,further information regarding dns resolution," 5:44 PM PDT -  On October 22, 2019, we detected and then mitigated a DDoS (Distributed Denial of Service) attack against Route 53. Due to the way that DNS queries are processed, this attack was first experienced by many other DNS server operators as the queries made their way through DNS resolvers on the internet to Route 53. The attack targeted specific DNS names and paths, notably those used to access the global names for S3 buckets.Because this attack was widely distributed, a small number of ISPs operating affected DNS resolvers implemented mitigation strategies of their own in an attempt to control the traffic. This is causing DNS lookups through these resolvers for a small number of AWS names to fail. We are doing our best to identify and contact these operators, as quickly as possible, and working with them to enhance their mitigations so that they do not cause impact to valid requests. If you are experiencing issues, please contact us so we can work with your operator to help resolve."
206,Amazon Connect (N. Virginia),[RESOLVED] Degraded Call Handling,1572019433,2,,"<div><span class=""yellowfg""> 9:03 AM PDT</span>&nbsp;We are investigating degraded call handling by agents using the Amazon Connect softphone in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 9:48 AM PDT</span>&nbsp;Between 8:17 AM and 9:30 AM PDT we experienced degraded call handling by agents using the softphone in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.
</div>",connect-us-east-1,2019-10-25 16:03:53,Connect,N. Virginia,8:17 AM,9:30 AM,PDT,73.0,2019,10,25,2019-10-25,degraded call handling," 9:03 AM PDT -  We are investigating degraded call handling by agents using the Amazon Connect softphone in the US-EAST-1 Region.
 9:48 AM PDT -  Between 8:17 AM and 9:30 AM PDT we experienced degraded call handling by agents using the softphone in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
207,Amazon Kinesis Data Streams (N. Virginia),[RESOLVED] Increased API Error Rates,1572377331,1,,"<div><span class=""yellowfg"">12:28 PM PDT</span>&nbsp;We are investigating increased Create, Update, and Delete Stream API error rates for Amazon Kinesis Data Streams in the US-EAST-1 Region. </div><div><span class=""yellowfg""> 1:06 PM PDT</span>&nbsp;Between 10:45 AM and 12:52 PM PDT, we experienced increased error rates for Create, Update, and Delete Stream API operations for Amazon Kinesis Data Streams in the US-EAST-1 Region. The issue has been resolved and the service is now operating normally.</div>",kinesis-us-east-1,2019-10-29 19:28:51,Kinesis Data Streams,N. Virginia,10:45 AM,12:52 PM,PDT,127.0,2019,10,29,2019-10-29,increased api error rates,"12:28 PM PDT -  We are investigating increased Create, Update, and Delete Stream API error rates for Amazon Kinesis Data Streams in the US-EAST-1 Region. 
 1:06 PM PDT -  Between 10:45 AM and 12:52 PM PDT, we experienced increased error rates for Create, Update, and Delete Stream API operations for Amazon Kinesis Data Streams in the US-EAST-1 Region. The issue has been resolved and the service is now operating normally."
208,AWS Billing Console,[RESOLVED] Increased Console Error Rates and latencies,1572963155,1,,"<div><span class=""yellowfg""> 6:12 AM PST</span>&nbsp;We are investigating increased AWS Billing Console error rates and latencies. We have identified the root-cause and are working on remediation.</div><div><span class=""yellowfg""> 6:42 AM PST</span>&nbsp;We continue to investigate increased AWS Billing Console error rates and latencies. We have identified the root-cause and are working on remediation.</div><div><span class=""yellowfg""> 7:41 AM PST</span>&nbsp;Remediation work for increased AWS Billing Console error rates and latencies continues. The issue affects accessing and reviewing billing information on the Billing Dashboard and Bills pages on the console.
</div><div><span class=""yellowfg""> 8:48 AM PST</span>&nbsp;We continue to work on the remediation for elevated Billing Dashboard and Bills page errors. As a work-around to retrieve your invoices, please navigate to Orders and Invoices page on the AWS Billing Console and, in order to review your cost and usage information, please navigate to Cost Explorer Console.</div><div><span class=""yellowfg"">10:17 AM PST</span>&nbsp;As of 9:50 am, we are seeing recovery of the service on Billing Dashboard and Bills pages as we continue to work through the mitigation activities. </div><div><span class=""yellowfg"">10:44 AM PST</span>&nbsp;Between 5:06 AM and 9:50 AM PST we experienced elevated error rates and latencies affecting Billing Dashboard and Bills pages on the AWS Billing Console. The issue has been resolved and the service is operating normally.</div>",billingconsole,2019-11-05 14:12:35,Billing Console,Global,5:06 AM,9:50 AM,PST,284.0,2019,11,5,2019-11-05,increased console error rates and latencies," 6:12 AM PST -  We are investigating increased AWS Billing Console error rates and latencies. We have identified the root-cause and are working on remediation.
 6:42 AM PST -  We continue to investigate increased AWS Billing Console error rates and latencies. We have identified the root-cause and are working on remediation.
 7:41 AM PST -  Remediation work for increased AWS Billing Console error rates and latencies continues. The issue affects accessing and reviewing billing information on the Billing Dashboard and Bills pages on the console.
 8:48 AM PST -  We continue to work on the remediation for elevated Billing Dashboard and Bills page errors. As a work-around to retrieve your invoices, please navigate to Orders and Invoices page on the AWS Billing Console and, in order to review your cost and usage information, please navigate to Cost Explorer Console.
10:17 AM PST -  As of 9:50 am, we are seeing recovery of the service on Billing Dashboard and Bills pages as we continue to work through the mitigation activities. 
10:44 AM PST -  Between 5:06 AM and 9:50 AM PST we experienced elevated error rates and latencies affecting Billing Dashboard and Bills pages on the AWS Billing Console. The issue has been resolved and the service is operating normally."
209,Amazon CloudFront,[RESOLVED] Change Propagation Delays,1573033383,1,,"<div><span class=""yellowfg""> 1:43 AM PST</span>&nbsp;We are investigating longer than usual propagation times to propagate certain configuration changes made either via console, CloudFront APIs, or AWS CloudFormation templates to some of our edge locations. End-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=""yellowfg""> 2:39 AM PST</span>&nbsp;We can confirm longer than usual propagation times to propagate configuration changes made either via console, CloudFront APIs, or AWS CloudFormation templates to some of our edge locations, and continue to work toward resolution. End-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=""yellowfg""> 4:03 AM PST</span>&nbsp;We continue to work on mitigating the cause of longer than usual propagation times to propagate configuration changes made either via console, CloudFront APIs, or AWS CloudFormation templates to some of our edge locations. End-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=""yellowfg""> 6:23 AM PST</span>&nbsp;Between November 5 9:52 PM and November 6 6:00 AM PST, we experienced intermittent delays in propagating CloudFront distribution changes to some edge locations. End-user requests for content from our edge locations were not affected. The issue has been resolved and the service is operating normally. </div>",cloudfront,2019-11-06 09:43:03,CloudFront,Global,9:52 PM,6:00 AM,PST,488.0,2019,11,6,2019-11-06,change propagation delays," 1:43 AM PST -  We are investigating longer than usual propagation times to propagate certain configuration changes made either via console, CloudFront APIs, or AWS CloudFormation templates to some of our edge locations. End-user requests for content from our edge locations are not affected by this issue and are being served normally.
 2:39 AM PST -  We can confirm longer than usual propagation times to propagate configuration changes made either via console, CloudFront APIs, or AWS CloudFormation templates to some of our edge locations, and continue to work toward resolution. End-user requests for content from our edge locations are not affected by this issue and are being served normally.
 4:03 AM PST -  We continue to work on mitigating the cause of longer than usual propagation times to propagate configuration changes made either via console, CloudFront APIs, or AWS CloudFormation templates to some of our edge locations. End-user requests for content from our edge locations are not affected by this issue and are being served normally.
 6:23 AM PST -  Between November 5 9:52 PM and November 6 6:00 AM PST, we experienced intermittent delays in propagating CloudFront distribution changes to some edge locations. End-user requests for content from our edge locations were not affected. The issue has been resolved and the service is operating normally. "
210,Auto Scaling (US-West),[RESOLVED] Increased Auto Scaling API errors,1573088299,1,,"<div><span class=""yellowfg""> 4:58 PM PST</span>&nbsp;We are investigating increased API error rates in the US-GOV-WEST-1 Region.</div><div><span class=""yellowfg""> 5:31 PM PST</span>&nbsp;We continue to investigate increased API error rates in the US-GOV-WEST-1 Region.</div><div><span class=""yellowfg""> 6:06 PM PST</span>&nbsp;We can confirm increased API error rates in the US-GOV-WEST-1 Region and continue to work towards resolution.</div><div><span class=""yellowfg""> 6:20 PM PST</span>&nbsp;Between 4:15 PM and 6:11 PM PST we experienced increased API error rates in the US-GOV-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",autoscaling-us-gov-west-1,2019-11-07 00:58:19,Auto Scaling,US-West,4:15 PM,6:11 PM,PST,116.0,2019,11,7,2019-11-07,increased auto scaling api errors," 4:58 PM PST -  We are investigating increased API error rates in the US-GOV-WEST-1 Region.
 5:31 PM PST -  We continue to investigate increased API error rates in the US-GOV-WEST-1 Region.
 6:06 PM PST -  We can confirm increased API error rates in the US-GOV-WEST-1 Region and continue to work towards resolution.
 6:20 PM PST -  Between 4:15 PM and 6:11 PM PST we experienced increased API error rates in the US-GOV-WEST-1 Region. The issue has been resolved and the service is operating normally."
211,Amazon Kinesis Data Streams (Hong Kong),[RESOLVED] Increased Error Rates,1573150026,1,,"<div><span class=""yellowfg"">10:07 AM PST</span>&nbsp;We are investigating increased error rates for Kinesis Data Streams APIs in the AP-EAST-1 Region.</div><div><span class=""yellowfg"">10:38 AM PST</span>&nbsp;We can confirm Kinesis Data Streams is experiencing increased error rates on all APIs in the AP-EAST-1 Region.</div><div><span class=""yellowfg"">11:06 AM PST</span>&nbsp;Between 9:19 AM and 10:52 AM PST Kinesis Data Streams customers experienced increased error rates on all APIs in the AP-EAST-1 Region. The issue is resolved and the service is operating normally.
</div>",kinesis-ap-east-1,2019-11-07 18:07:06,Kinesis Data Streams,Hong Kong,9:19 AM,10:52 AM,PST,93.0,2019,11,7,2019-11-07,increased error rates,"10:07 AM PST -  We are investigating increased error rates for Kinesis Data Streams APIs in the AP-EAST-1 Region.
10:38 AM PST -  We can confirm Kinesis Data Streams is experiencing increased error rates on all APIs in the AP-EAST-1 Region.
11:06 AM PST -  Between 9:19 AM and 10:52 AM PST Kinesis Data Streams customers experienced increased error rates on all APIs in the AP-EAST-1 Region. The issue is resolved and the service is operating normally."
212,Amazon CloudWatch (Hong Kong),[RESOLVED] Increased Faults and Latencies,1573150195,1,,"<div><span class=""yellowfg"">10:09 AM PST</span>&nbsp;We are investigating increased faults and latencies for CloudWatch Metrics PutMetricData and CloudWatch Logs PutLogEvents APIs in the AP-EAST-1 Region. CloudWatch alarms may transition into ""INSUFFICIENT_DATA"" state if set on delayed metrics.</div><div><span class=""yellowfg"">10:45 AM PST</span>&nbsp;We can confirm increased faults and latencies for CloudWatch Metrics PutMetricData and CloudWatch Logs PutLogEvents APIs in the AP-EAST-1 Region, we are actively working to resolve the issue.</div><div><span class=""yellowfg"">10:52 AM PST</span>&nbsp;We can confirm recovery for the CloudWatch Logs PutLogEvents API and continue to work towards recovery for the Metrics PutMetricsData API in the AP-EAST-1 Region. </div><div><span class=""yellowfg"">11:04 AM PST</span>&nbsp;Between 09:15 AM and 10:51 AM PST, some customers experienced elevated faults when calling CloudWatch Metrics PutMetricData and CloudWatch Logs PutLogEvents APIs in the AP-EAST-1 Region. Some metrics were delayed, CloudWatch alarms on delayed metrics transitioned into INSUFFICIENT_DATA state, and some metric data may be missing. The issue is resolved and the service is operating normally.</div>",cloudwatch-ap-east-1,2019-11-07 18:09:55,CloudWatch,Hong Kong,09:15 AM,10:51 AM,PST,96.0,2019,11,7,2019-11-07,increased faults and latencies,"10:09 AM PST -  We are investigating increased faults and latencies for CloudWatch Metrics PutMetricData and CloudWatch Logs PutLogEvents APIs in the AP-EAST-1 Region. CloudWatch alarms may transition into ""INSUFFICIENT_DATA"" state if set on delayed metrics.
10:45 AM PST -  We can confirm increased faults and latencies for CloudWatch Metrics PutMetricData and CloudWatch Logs PutLogEvents APIs in the AP-EAST-1 Region, we are actively working to resolve the issue.
10:52 AM PST -  We can confirm recovery for the CloudWatch Logs PutLogEvents API and continue to work towards recovery for the Metrics PutMetricsData API in the AP-EAST-1 Region. 
11:04 AM PST -  Between 09:15 AM and 10:51 AM PST, some customers experienced elevated faults when calling CloudWatch Metrics PutMetricData and CloudWatch Logs PutLogEvents APIs in the AP-EAST-1 Region. Some metrics were delayed, CloudWatch alarms on delayed metrics transitioned into INSUFFICIENT_DATA state, and some metric data may be missing. The issue is resolved and the service is operating normally."
213,Amazon Kinesis Firehose (Hong Kong),[RESOLVED] Increased Error Rates,1573150212,1,,"<div><span class=""yellowfg"">10:10 AM PST</span>&nbsp;We are investigating increased error rates for Kinesis Data Firehose APIs in the AP-EAST-1 Region.</div><div><span class=""yellowfg"">10:41 AM PST</span>&nbsp;We can confirm Kinesis Data Firehose is experiencing increased error rates on all APIs in the AP-EAST-1 Region.</div><div><span class=""yellowfg"">11:09 AM PST</span>&nbsp;Between 9:19 AM and 10:52 AM PST Kinesis Data Firehose customers experienced increased error rates on all APIs in the AP-EAST-1 Region. The issue is resolved and the service is operating normally.</div>",firehose-ap-east-1,2019-11-07 18:10:12,Kinesis Firehose,Hong Kong,9:19 AM,10:52 AM,PST,93.0,2019,11,7,2019-11-07,increased error rates,"10:10 AM PST -  We are investigating increased error rates for Kinesis Data Firehose APIs in the AP-EAST-1 Region.
10:41 AM PST -  We can confirm Kinesis Data Firehose is experiencing increased error rates on all APIs in the AP-EAST-1 Region.
11:09 AM PST -  Between 9:19 AM and 10:52 AM PST Kinesis Data Firehose customers experienced increased error rates on all APIs in the AP-EAST-1 Region. The issue is resolved and the service is operating normally."
214,Amazon DynamoDB (N. Virginia),[RESOLVED] Increased Update Table Processing Times,1573196668,1,,"<div><span class=""yellowfg"">11:04 PM PST</span>&nbsp;We are investigating increased processing times for update table with Amazon DynamoDB in the US-EAST-1 Region.</div><div><span class=""yellowfg"">11:44 PM PST</span>&nbsp;We have identified the cause of the increased processing times for update table with Amazon DynamoDB in the US-EAST-1 Region and continue working towards resolution.</div><div><span class=""yellowfg"">Nov 8, 12:03 AM PST</span>&nbsp;Between 8:45 PM and 11:55 PM PST, we experienced increased processing times for update table with Amazon DynamoDB in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",dynamodb-us-east-1,2019-11-08 07:04:28,DynamoDB,N. Virginia,8:45 PM,11:55 PM,PST,190.0,2019,11,8,2019-11-08,increased update table processing times,"11:04 PM PST -  We are investigating increased processing times for update table with Amazon DynamoDB in the US-EAST-1 Region.
11:44 PM PST -  We have identified the cause of the increased processing times for update table with Amazon DynamoDB in the US-EAST-1 Region and continue working towards resolution.
Nov 8, 12:03 AM PST -  Between 8:45 PM and 11:55 PM PST, we experienced increased processing times for update table with Amazon DynamoDB in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
215,Amazon Elastic Compute Cloud (Frankfurt),[RESOLVED] Network Connectivity,1573546089,2,,"<div><span class=""yellowfg"">12:08 AM PST</span>&nbsp;We are investigating increased network connectivity errors to instances in a single Availability Zone in the EU-CENTRAL-1 Region.</div><div><span class=""yellowfg"">12:28 AM PST</span>&nbsp;We are experiencing elevated API error rates and network connectivity errors in a single Availability Zone. We have identified the root cause and are working to resolve the issue. </div><div><span class=""yellowfg""> 1:04 AM PST</span>&nbsp;Most network connectivity to instances within the impacted Availability Zone has been restored, and we are completing recovery for the remaining connectivity errors. We are continuing to experience elevated API error rates and are working to resolve this issue.</div><div><span class=""yellowfg""> 2:13 AM PST</span>&nbsp;Some EBS volumes are experiencing degraded performance in a single Availability Zone in the EU-CENTRAL-1 Region. We are working to resolve this issue. Network connectivity errors and elevated API error rates in the Availability Zone have been resolved. </div><div><span class=""yellowfg""> 6:06 AM PST</span>&nbsp;Between November 11 11:38 PM and November 12 12:48 AM PST we experienced API errors, network connectivity errors, and degraded EBS volume performance in a single Availability Zone in the EU-CENTRAL-1 Region. The network connectivity errors were resolved at 12:48 AM PST, the API errors were resolved at 1:26 AM PST. We have recovered the majority of the degraded volumes. A small number of volumes remain degraded. We continue to work to recover all affected volumes and will notify customers with impacted volumes via their Personal Health Dashboard. For immediate recovery, we recommend replacing any remaining affected volumes if possible. The issue has been resolved and the service is operating normally.</div>",ec2-eu-central-1,2019-11-12 08:08:09,Elastic Compute Cloud,Frankfurt,11:38 PM,12:48 AM,PST,70.0,2019,11,12,2019-11-12,network connectivity,"12:08 AM PST -  We are investigating increased network connectivity errors to instances in a single Availability Zone in the EU-CENTRAL-1 Region.
12:28 AM PST -  We are experiencing elevated API error rates and network connectivity errors in a single Availability Zone. We have identified the root cause and are working to resolve the issue. 
 1:04 AM PST -  Most network connectivity to instances within the impacted Availability Zone has been restored, and we are completing recovery for the remaining connectivity errors. We are continuing to experience elevated API error rates and are working to resolve this issue.
 2:13 AM PST -  Some EBS volumes are experiencing degraded performance in a single Availability Zone in the EU-CENTRAL-1 Region. We are working to resolve this issue. Network connectivity errors and elevated API error rates in the Availability Zone have been resolved. 
 6:06 AM PST -  Between November 11 11:38 PM and November 12 12:48 AM PST we experienced API errors, network connectivity errors, and degraded EBS volume performance in a single Availability Zone in the EU-CENTRAL-1 Region. The network connectivity errors were resolved at 12:48 AM PST, the API errors were resolved at 1:26 AM PST. We have recovered the majority of the degraded volumes. A small number of volumes remain degraded. We continue to work to recover all affected volumes and will notify customers with impacted volumes via their Personal Health Dashboard. For immediate recovery, we recommend replacing any remaining affected volumes if possible. The issue has been resolved and the service is operating normally."
216,Amazon Relational Database Service (Frankfurt),[RESOLVED] Network Connectivity,1573549998,1,,"<div><span class=""yellowfg""> 1:13 AM PST</span>&nbsp;We are continuing to investigate connectivity issues affecting some instances in a single Availability Zone in the EU-CENTRAL-1 Region. 
</div><div><span class=""yellowfg""> 2:23 AM PST</span>&nbsp;We are starting to see recovery for the connectivity issues affecting a small number of instances in a single Availability Zone in the EU-CENTRAL-1 Region.</div><div><span class=""yellowfg""> 5:09 AM PST</span>&nbsp;Between November 11 11:39 PM and November 12 5:00 AM PST, a small number of instances experienced connectivity issues in a single Availability Zone in the EU-CENTRAL-1 Region. The issue has been resolved and the service is operating normally.</div>",rds-eu-central-1,2019-11-12 09:13:18,Relational Database Service,Frankfurt,11:39 PM,5:00 AM,PST,321.0,2019,11,12,2019-11-12,network connectivity," 1:13 AM PST -  We are continuing to investigate connectivity issues affecting some instances in a single Availability Zone in the EU-CENTRAL-1 Region. 
 2:23 AM PST -  We are starting to see recovery for the connectivity issues affecting a small number of instances in a single Availability Zone in the EU-CENTRAL-1 Region.
 5:09 AM PST -  Between November 11 11:39 PM and November 12 5:00 AM PST, a small number of instances experienced connectivity issues in a single Availability Zone in the EU-CENTRAL-1 Region. The issue has been resolved and the service is operating normally."
217,AWS CloudFormation (Frankfurt),"[RESOLVED] Increased Stack Create, Delete and Update Times. ",1573552926,1,,"<div><span class=""yellowfg""> 2:02 AM PST</span>&nbsp;We are investigating increased latencies for creating, deleting and updating stacks in the EU-CENTRAL-1 Region. </div><div><span class=""yellowfg""> 3:29 AM PST</span>&nbsp;Between November 11 11:35 PM and November 12 3:25 AM PST we experienced increased latencies for creating, deleting and updating stacks in the EU-CENTRAL-1 Region. The issue has been resolved and the service is operating normally. </div>",cloudformation-eu-central-1,2019-11-12 10:02:06,CloudFormation,Frankfurt,11:35 PM,3:25 AM,PST,230.0,2019,11,12,2019-11-12,"increased stack create, delete and update times."," 2:02 AM PST -  We are investigating increased latencies for creating, deleting and updating stacks in the EU-CENTRAL-1 Region. 
 3:29 AM PST -  Between November 11 11:35 PM and November 12 3:25 AM PST we experienced increased latencies for creating, deleting and updating stacks in the EU-CENTRAL-1 Region. The issue has been resolved and the service is operating normally. "
218,Auto Scaling (Frankfurt),[RESOLVED] Increased Auto Scaling API errors,1573556991,1,,"<div><span class=""yellowfg""> 3:09 AM PST</span>&nbsp;We are investigating elevated API fault rate and scaling delays for EC2 Auto Scaling in the EU-CENTRAL-1 Region.</div><div><span class=""yellowfg""> 4:10 AM PST</span>&nbsp;Between November 11 11:44 PM PST and November 12 3:35 AM PST we experienced elevated API fault rates and scaling delays for EC2 Auto Scaling in the EU-CENTRAL-1 Region. The issue has been resolved and the service is operating normally. </div>",autoscaling-eu-central-1,2019-11-12 11:09:51,Auto Scaling,Frankfurt,11:44 PM,3:35 AM,PST,231.0,2019,11,12,2019-11-12,increased auto scaling api errors," 3:09 AM PST -  We are investigating elevated API fault rate and scaling delays for EC2 Auto Scaling in the EU-CENTRAL-1 Region.
 4:10 AM PST -  Between November 11 11:44 PM PST and November 12 3:35 AM PST we experienced elevated API fault rates and scaling delays for EC2 Auto Scaling in the EU-CENTRAL-1 Region. The issue has been resolved and the service is operating normally. "
219,Amazon Elastic Compute Cloud (N. Virginia),[RESOLVED] Increased API Error Rates,1573583181,1,,"<div><span class=""yellowfg"">10:26 AM PST</span>&nbsp;We are investigating increased API error rates and latencies in the US-EAST-1 Region. Connectivity to running instances is not affected.</div><div><span class=""yellowfg"">10:50 AM PST</span>&nbsp;We continue to work towards resolution of the issue affecting API error rates and latencies in the US-EAST-1 Region. Connectivity to running instances remains unaffected. </div><div><span class=""yellowfg"">11:31 AM PST</span>&nbsp;We are beginning to see recovery for the increased API error rates and latencies in the US-EAST-1 Region and continue to work toward full recovery. </div><div><span class=""yellowfg""> 1:18 PM PST</span>&nbsp;Between 8:57 AM and 10:52 AM PST, we experienced increased API error rates and latencies in the US-EAST-1 Region. Connectivity to running instances was not affected during this event. The issue has been resolved and the service is operating normally.</div>",ec2-us-east-1,2019-11-12 18:26:21,Elastic Compute Cloud,N. Virginia,8:57 AM,10:52 AM,PST,115.0,2019,11,12,2019-11-12,increased api error rates,"10:26 AM PST -  We are investigating increased API error rates and latencies in the US-EAST-1 Region. Connectivity to running instances is not affected.
10:50 AM PST -  We continue to work towards resolution of the issue affecting API error rates and latencies in the US-EAST-1 Region. Connectivity to running instances remains unaffected. 
11:31 AM PST -  We are beginning to see recovery for the increased API error rates and latencies in the US-EAST-1 Region and continue to work toward full recovery. 
 1:18 PM PST -  Between 8:57 AM and 10:52 AM PST, we experienced increased API error rates and latencies in the US-EAST-1 Region. Connectivity to running instances was not affected during this event. The issue has been resolved and the service is operating normally."
220,AWS Identity and Access Management,[RESOLVED] Increased Error Rates,1573586011,1,,"<div><span class=""yellowfg"">11:13 AM PST</span>&nbsp;We are investigating increased API error rates and latency for some IAM APIs.</div><div><span class=""yellowfg"">11:47 AM PST</span>&nbsp;We are beginning to see recovery for the increased API error rates and latencies and continue to work toward full recovery. </div><div><span class=""yellowfg""> 1:33 PM PST</span>&nbsp;Between 8:57 AM and 10:55 AM PST we experienced increased API error rates and latencies. The issue has been resolved and the service is operating normally. </div>",iam,2019-11-12 19:13:31,Identity and Access Management,Global,8:57 AM,10:55 AM,PST,118.0,2019,11,12,2019-11-12,increased error rates,"11:13 AM PST -  We are investigating increased API error rates and latency for some IAM APIs.
11:47 AM PST -  We are beginning to see recovery for the increased API error rates and latencies and continue to work toward full recovery. 
 1:33 PM PST -  Between 8:57 AM and 10:55 AM PST we experienced increased API error rates and latencies. The issue has been resolved and the service is operating normally. "
221,Amazon Connect (N. Virginia),[RESOLVED] Increased Error Rates,1573587595,1,,"<div><span class=""yellowfg"">10:31 AM PST</span>&nbsp;We are investigating increased API error rates and call handling issues the US-EAST-1 Region. </div><div><span class=""yellowfg"">12:19 PM PST</span>&nbsp;We are beginning to see recovery for the increased API error rates and call handling issues in the US-EAST-1 Region and continue to work toward full recovery. </div><div><span class=""yellowfg""> 1:24 PM PST</span>&nbsp;Between 8:57 AM and 12:09 PM PST we experienced increased API error rates and call handling issues in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",connect-us-east-1,2019-11-12 19:39:55,Connect,N. Virginia,8:57 AM,12:09 PM,PST,192.0,2019,11,12,2019-11-12,increased error rates,"10:31 AM PST -  We are investigating increased API error rates and call handling issues the US-EAST-1 Region. 
12:19 PM PST -  We are beginning to see recovery for the increased API error rates and call handling issues in the US-EAST-1 Region and continue to work toward full recovery. 
 1:24 PM PST -  Between 8:57 AM and 12:09 PM PST we experienced increased API error rates and call handling issues in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
222,AWS Lambda (N. Virginia),[RESOLVED] Increased Error Rates,1573591030,1,,"<div><span class=""yellowfg"">12:37 PM PST</span>&nbsp;We are experiencing increased error rates and latencies for actions on the AWS Lambda Management Console in the US-EAST-1 Region. AWS Lambda API calls via the SDK and CLI remain unaffected. Some invocations of scheduled Lambda function are also experiencing error rates in the US-EAST-1 Region. The root cause of these issues has been identified and we are working towards full resolution.</div><div><span class=""yellowfg""> 1:21 PM PST</span>&nbsp;Between 8:56 AM and 12:15 PM PST, some customers experienced increased error rates and latencies for actions on the AWS Lambda Management Console in the US-EAST-1 Region. AWS Lambda API calls via the SDK and CLI were unaffected. Between 11:22 AM and 12:40 PM PST, some invocations of scheduled Lambda functions also experienced elevated error rates in the US-EAST-1 Region. Any unprocessed invocations for affected functions have been reprocessed based on the retry policy on the function. The issue has been resolved and the service is operating normally.</div>",lambda-us-east-1,2019-11-12 20:37:10,Lambda,N. Virginia,11:22 AM,12:40 PM,PST,78.0,2019,11,12,2019-11-12,increased error rates,"12:37 PM PST -  We are experiencing increased error rates and latencies for actions on the AWS Lambda Management Console in the US-EAST-1 Region. AWS Lambda API calls via the SDK and CLI remain unaffected. Some invocations of scheduled Lambda function are also experiencing error rates in the US-EAST-1 Region. The root cause of these issues has been identified and we are working towards full resolution.
 1:21 PM PST -  Between 8:56 AM and 12:15 PM PST, some customers experienced increased error rates and latencies for actions on the AWS Lambda Management Console in the US-EAST-1 Region. AWS Lambda API calls via the SDK and CLI were unaffected. Between 11:22 AM and 12:40 PM PST, some invocations of scheduled Lambda functions also experienced elevated error rates in the US-EAST-1 Region. Any unprocessed invocations for affected functions have been reprocessed based on the retry policy on the function. The issue has been resolved and the service is operating normally."
223,Amazon Kinesis Analytics (N. Virginia),[RESOLVED] Applications not processing data,1573673937,1,,"<div><span class=""yellowfg"">11:38 AM PST</span>&nbsp;We are currently investigating issues with Amazon Kinesis Data Analytics applications that are not processing data in the US-EAST-1 Region.</div><div><span class=""yellowfg"">12:12 PM PST</span>&nbsp;Between 10:08 AM PST and 12:10 PM PST a subset of Amazon Kinesis Data Analytics applications were not processing data in US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",kinesisanalytics-us-east-1,2019-11-13 19:38:57,Kinesis Analytics,N. Virginia,10:08 AM,12:10 PM,PST,122.0,2019,11,13,2019-11-13,applications not processing data,"11:38 AM PST -  We are currently investigating issues with Amazon Kinesis Data Analytics applications that are not processing data in the US-EAST-1 Region.
12:12 PM PST -  Between 10:08 AM PST and 12:10 PM PST a subset of Amazon Kinesis Data Analytics applications were not processing data in US-EAST-1 Region. The issue has been resolved and the service is operating normally."
224,Amazon Elastic Compute Cloud (N. Virginia),[RESOLVED] Increased API Error Rates for EC2 Spot,1573890252,1,,"<div><span class=""yellowfg"">11:44 PM PST</span>&nbsp;We are investigating increased API error rates for EC2 Spot-related APIs in the US-EAST-1 Region. Affected customers will not be able to launch new EC2 Spot instances, but existing instances are unaffected. Launching new EC2 On-Demand Instances is unaffected.</div><div><span class=""yellowfg"">Nov 16, 12:03 AM PST</span>&nbsp;We are investigating increased API error rates for EC2 Spot-related APIs in the US-EAST-1 Region. Affected customers will not be able to launch new EC2 Spot instances, but existing instances are unaffected. Launching new EC2 On-Demand Instances is unaffected. We have identified the root cause and are working towards resolution.</div><div><span class=""yellowfg"">Nov 16,  1:41 AM PST</span>&nbsp;Between November 15 10:30 PM and November 16 1:19 AM PST, we experienced increased API latencies and error rates for EC2 Spot APIs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",ec2-us-east-1,2019-11-16 07:44:12,Elastic Compute Cloud,N. Virginia,10:30 PM,1:19 AM,PST,169.0,2019,11,16,2019-11-16,increased api error rates for ec2 spot,"11:44 PM PST -  We are investigating increased API error rates for EC2 Spot-related APIs in the US-EAST-1 Region. Affected customers will not be able to launch new EC2 Spot instances, but existing instances are unaffected. Launching new EC2 On-Demand Instances is unaffected.
Nov 16, 12:03 AM PST -  We are investigating increased API error rates for EC2 Spot-related APIs in the US-EAST-1 Region. Affected customers will not be able to launch new EC2 Spot instances, but existing instances are unaffected. Launching new EC2 On-Demand Instances is unaffected. We have identified the root cause and are working towards resolution.
Nov 16,  1:41 AM PST -  Between November 15 10:30 PM and November 16 1:19 AM PST, we experienced increased API latencies and error rates for EC2 Spot APIs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
225,AWS Elastic Beanstalk (N. Virginia),Increased API Error Rates ,1574595711,1,,"<div><span class=""yellowfg""> 3:41 AM PST</span>&nbsp;We are currently investigating increased error rates for the Elastic Beanstalk Health APIs in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 4:07 AM PST</span>&nbsp;Between 1:20 AM and 3:50 AM PST, we experienced increased error rates for the Elastic Beanstalk Health APIs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",elasticbeanstalk-us-east-1,2019-11-24 11:41:51,Elastic Beanstalk,N. Virginia,1:20 AM,3:50 AM,PST,150.0,2019,11,24,2019-11-24,increased api error rates," 3:41 AM PST -  We are currently investigating increased error rates for the Elastic Beanstalk Health APIs in the US-EAST-1 Region.
 4:07 AM PST -  Between 1:20 AM and 3:50 AM PST, we experienced increased error rates for the Elastic Beanstalk Health APIs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
226,Amazon Elastic Compute Cloud (Sydney),[RESOLVED] Increased API Error Rates,1574731819,1,,"<div><span class=""yellowfg""> 5:30 PM PST</span>&nbsp;We are investigating increased Console and API error rates in the AP-SOUTHEAST-2 Region.</div><div><span class=""yellowfg""> 5:55 PM PST</span>&nbsp;We have identified the root cause of the issue causing increased error rates and latencies for the EC2 and EBS APIs in the AP-SOUTHEAST-2 Region. Existing instances are not affected by this issue. We are working towards resolution of the issue.</div><div><span class=""yellowfg""> 6:28 PM PST</span>&nbsp;We are seeing recovery for the increased error rates and latencies for the EC2 and EBS APIs in the AP-SOUTHEAST-2 Region. We continue to monitor error rates and latencies as we work towards full recovery.</div><div><span class=""yellowfg""> 7:33 PM PST</span>&nbsp;Between 4:52 PM and 6:22 PM PST we experienced increased error rates and latencies for the EC2 and EBS APIs in the AP-SOUTHEAST-2 Region. The issue has been resolved and the service is operating normally.</div>",ec2-ap-southeast-2,2019-11-26 01:30:19,Elastic Compute Cloud,Sydney,4:52 PM,6:22 PM,PST,90.0,2019,11,26,2019-11-26,increased api error rates," 5:30 PM PST -  We are investigating increased Console and API error rates in the AP-SOUTHEAST-2 Region.
 5:55 PM PST -  We have identified the root cause of the issue causing increased error rates and latencies for the EC2 and EBS APIs in the AP-SOUTHEAST-2 Region. Existing instances are not affected by this issue. We are working towards resolution of the issue.
 6:28 PM PST -  We are seeing recovery for the increased error rates and latencies for the EC2 and EBS APIs in the AP-SOUTHEAST-2 Region. We continue to monitor error rates and latencies as we work towards full recovery.
 7:33 PM PST -  Between 4:52 PM and 6:22 PM PST we experienced increased error rates and latencies for the EC2 and EBS APIs in the AP-SOUTHEAST-2 Region. The issue has been resolved and the service is operating normally."
227,Auto Scaling (Sydney),[RESOLVED] Increased Launch Times ,1574732926,1,,"<div><span class=""yellowfg""> 5:48 PM PST</span>&nbsp;We are investigating increased launch times for EC2 instances managed by Auto Scaling in the AP-SOUTHEAST-2 Region.</div><div><span class=""yellowfg""> 6:13 PM PST</span>&nbsp;We continue to work towards resolving the issue causing increased launch times for Auto Scaling in the AP-SOUTHEAST-2 Region. All affected launches will complete successfully once the increased error rates and latencies for the EC2 APIs have been resolved.</div><div><span class=""yellowfg""> 6:29 PM PST</span>&nbsp;With the improvement in error rates and latencies for the EC2 APIs, Auto Scaling launches are once again succeeding. We're now working through the backlog of pending launches.</div><div><span class=""yellowfg""> 7:33 PM PST</span>&nbsp;Between 4:52 PM and 6:22 PM PST we experienced increased launch latencies for Auto Scaling in the AP-SOUTHEAST-2 Region. The issue has been resolved and the service is operating normally.</div>",autoscaling-ap-southeast-2,2019-11-26 01:48:46,Auto Scaling,Sydney,4:52 PM,6:22 PM,PST,90.0,2019,11,26,2019-11-26,increased launch times," 5:48 PM PST -  We are investigating increased launch times for EC2 instances managed by Auto Scaling in the AP-SOUTHEAST-2 Region.
 6:13 PM PST -  We continue to work towards resolving the issue causing increased launch times for Auto Scaling in the AP-SOUTHEAST-2 Region. All affected launches will complete successfully once the increased error rates and latencies for the EC2 APIs have been resolved.
 6:29 PM PST -  With the improvement in error rates and latencies for the EC2 APIs, Auto Scaling launches are once again succeeding. We're now working through the backlog of pending launches.
 7:33 PM PST -  Between 4:52 PM and 6:22 PM PST we experienced increased launch latencies for Auto Scaling in the AP-SOUTHEAST-2 Region. The issue has been resolved and the service is operating normally."
228,AWS Identity and Access Management,[RESOLVED] Increased SAML Sign-In Error Rates,1576790068,1,,"<div><span class=""yellowfg""> 1:14 PM PST</span>&nbsp;We are investigating increased error rates for SAML-based sign-in requests. Sign-in requests for other Roles and users are not affected.</div><div><span class=""yellowfg""> 2:14 PM PST</span>&nbsp;We can confirm increased error rates for SAML-based sign-in requests. Sign-in requests for other Roles and users are not affected. We continue to investigate increased error rates for SAML-based sign-in requests.</div><div><span class=""yellowfg""> 2:37 PM PST</span>&nbsp;We are beginning to see recovery for the increased error rates impacting SAML-based sign-in requests. We continue to work toward full recovery. </div><div><span class=""yellowfg""> 3:21 PM PST</span>&nbsp;Between 11:23 AM and 2:42 PM PST we experienced increased error rates impacting SAML-based sign-in requests. During this time customers may have been unable to authenticate to the AWS Console when using SAML for sign-in requests. The issue has been resolved and the service is operating normally. </div>",iam,2019-12-19 21:14:28,Identity and Access Management,Global,11:23 AM,2:42 PM,PST,199.0,2019,12,19,2019-12-19,increased saml sign-in error rates," 1:14 PM PST -  We are investigating increased error rates for SAML-based sign-in requests. Sign-in requests for other Roles and users are not affected.
 2:14 PM PST -  We can confirm increased error rates for SAML-based sign-in requests. Sign-in requests for other Roles and users are not affected. We continue to investigate increased error rates for SAML-based sign-in requests.
 2:37 PM PST -  We are beginning to see recovery for the increased error rates impacting SAML-based sign-in requests. We continue to work toward full recovery. 
 3:21 PM PST -  Between 11:23 AM and 2:42 PM PST we experienced increased error rates impacting SAML-based sign-in requests. During this time customers may have been unable to authenticate to the AWS Console when using SAML for sign-in requests. The issue has been resolved and the service is operating normally. "
229,Amazon CloudFront,[RESOLVED] Elevated CloudFront API errors,1577471528,1,,"<div><span class=""yellowfg"">10:32 AM PST</span>&nbsp;We are investigating increased CloudFront API errors and longer than usual propagation times while making changes to CloudFront configurations. End-user requests for content from edge locations are not affected.</div><div><span class=""yellowfg"">11:16 AM PST</span>&nbsp;We have resolved the issues related to CloudFront APIs. We have identified the root cause of the longer than usual change propagation delays for invalidations and CloudFront configurations and are actively working towards resolution. End-user requests for content from edge locations are not affected and continue to be served normally.</div><div><span class=""yellowfg"">12:04 PM PST</span>&nbsp;Between 9:30 AM and 11:01 AM PST, customers may have seen elevated CloudFront API errors. These have now been resolved. Due to these API errors there was a backlog of changes that resulted in longer than usual change propagation delays for CloudFront configurations and invalidations. This backlog of changes is actively being processed. End-user requests for content from edge locations are not affected and continue to be served normally.</div><div><span class=""yellowfg""> 2:01 PM PST</span>&nbsp;Between 9:30 AM and 11:01 AM PST, customers may have seen elevated CloudFront API errors. Due to these API errors there was a backlog of changes that resulted in longer than usual change propagation delays for CloudFront configurations and invalidations. The backlog of Invalidation changes were fully processed by 11:35 AM. The backlog of CloudFront configuration changes was fully processed by 2:00 PM PST. All issues have been fully resolved and the system is operating normally.</div>",cloudfront,2019-12-27 18:32:08,CloudFront,Global,9:30 AM,11:01 AM,PST,91.0,2019,12,27,2019-12-27,elevated cloudfront api errors,"10:32 AM PST -  We are investigating increased CloudFront API errors and longer than usual propagation times while making changes to CloudFront configurations. End-user requests for content from edge locations are not affected.
11:16 AM PST -  We have resolved the issues related to CloudFront APIs. We have identified the root cause of the longer than usual change propagation delays for invalidations and CloudFront configurations and are actively working towards resolution. End-user requests for content from edge locations are not affected and continue to be served normally.
12:04 PM PST -  Between 9:30 AM and 11:01 AM PST, customers may have seen elevated CloudFront API errors. These have now been resolved. Due to these API errors there was a backlog of changes that resulted in longer than usual change propagation delays for CloudFront configurations and invalidations. This backlog of changes is actively being processed. End-user requests for content from edge locations are not affected and continue to be served normally.
 2:01 PM PST -  Between 9:30 AM and 11:01 AM PST, customers may have seen elevated CloudFront API errors. Due to these API errors there was a backlog of changes that resulted in longer than usual change propagation delays for CloudFront configurations and invalidations. The backlog of Invalidation changes were fully processed by 11:35 AM. The backlog of CloudFront configuration changes was fully processed by 2:00 PM PST. All issues have been fully resolved and the system is operating normally."
230,AWS Lambda (Ireland),[RESOLVED] Increased delays in event processing from Streams ,1578446772,1,,"<div><span class=""yellowfg""> 5:26 PM PST</span>&nbsp;Between 4:05 PM PST and 5:20 PM PST, customers using Lambda functions to process events from Kinesis Data Streams and DynamoDB Streams experienced significant delays in event processing for a subset of functions in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally. The backlogged events will be processed by the function over the next few hours as per the retry policy on the event source mapping on the affected functions.</div>",lambda-eu-west-1,2020-01-08 01:26:12,Lambda,Ireland,4:05 PM,5:20 PM,PST,75.0,2020,1,8,2020-01-08,increased delays in event processing from streams," 5:26 PM PST -  Between 4:05 PM PST and 5:20 PM PST, customers using Lambda functions to process events from Kinesis Data Streams and DynamoDB Streams experienced significant delays in event processing for a subset of functions in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally. The backlogged events will be processed by the function over the next few hours as per the retry policy on the event source mapping on the affected functions."
231,Amazon Chime,[RESOLVED] Availability,1579119985,2,,"<div><span class=""yellowfg"">12:26 PM PST</span>&nbsp;We are currently investigating increased application faults and decreased availability for Amazon Chime.</div><div><span class=""yellowfg"">12:45 PM PST</span>&nbsp;We have identified the root cause of the increased application faults and decreased availability for Amazon Chime. Customers who are already signed into the Chime Client are not advised to logout, or select the re-connection option.</div><div><span class=""yellowfg""> 1:36 PM PST</span>&nbsp;We have identified the root cause of the increased application faults and decreased availability for Amazon Chime, and can confirm recovery for some users. Customers who are already signed into the Chime Client are not advised to logout, or select the re-connection option, as we continue to work towards resolution.</div><div><span class=""yellowfg""> 2:30 PM PST</span>&nbsp;We have identified the root cause of the increased application faults and decreased availability for Amazon Chime, and can confirm recovery for audio meeting and chat features. We continue to work towards resolving decreased video conferencing availability.</div><div><span class=""yellowfg""> 3:11 PM PST</span>&nbsp;Between 11:49 AM and 3:08 PM PST we experienced increased application faults and decreased availability for Amazon Chime. The issue has been resolved and the service is operating normally.</div>",chime,2020-01-15 20:26:25,Chime,Global,11:49 AM,3:08 PM,PST,199.0,2020,1,15,2020-01-15,availability,"12:26 PM PST -  We are currently investigating increased application faults and decreased availability for Amazon Chime.
12:45 PM PST -  We have identified the root cause of the increased application faults and decreased availability for Amazon Chime. Customers who are already signed into the Chime Client are not advised to logout, or select the re-connection option.
 1:36 PM PST -  We have identified the root cause of the increased application faults and decreased availability for Amazon Chime, and can confirm recovery for some users. Customers who are already signed into the Chime Client are not advised to logout, or select the re-connection option, as we continue to work towards resolution.
 2:30 PM PST -  We have identified the root cause of the increased application faults and decreased availability for Amazon Chime, and can confirm recovery for audio meeting and chat features. We continue to work towards resolving decreased video conferencing availability.
 3:11 PM PST -  Between 11:49 AM and 3:08 PM PST we experienced increased application faults and decreased availability for Amazon Chime. The issue has been resolved and the service is operating normally."
232,AWS Certificate Manager (N. Virginia),[RESOLVED] Certificate Issuance Delays,1579134535,1,,"<div><span class=""yellowfg""> 4:28 PM PST</span>&nbsp;We have identified the cause of the increased delays in validation and issuance for DNS validated domains in the US-EAST-1 Region and are working to mitigate the issue. Certificates can be requested but will be delayed in issuance.</div><div><span class=""yellowfg""> 4:58 PM PST</span>&nbsp;We continue to work towards mitigating the increased delays in validation and issuance for DNS validated domains in the US-EAST-1 Region. Certificates that are DNS validated can be requested but will be delayed in issuance.</div><div><span class=""yellowfg""> 6:20 PM PST</span>&nbsp;Between 3:05 PM and 6:11 PM PST we experienced increased delays in validation and issuance for DNS validated domains in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. Certificates waiting for DNS validation which have proper CNAME entries in DNS will validate within the next 3 hours.</div>",certificatemanager-us-east-1,2020-01-16 00:28:55,Certificate Manager,N. Virginia,3:05 PM,6:11 PM,PST,186.0,2020,1,16,2020-01-16,certificate issuance delays," 4:28 PM PST -  We have identified the cause of the increased delays in validation and issuance for DNS validated domains in the US-EAST-1 Region and are working to mitigate the issue. Certificates can be requested but will be delayed in issuance.
 4:58 PM PST -  We continue to work towards mitigating the increased delays in validation and issuance for DNS validated domains in the US-EAST-1 Region. Certificates that are DNS validated can be requested but will be delayed in issuance.
 6:20 PM PST -  Between 3:05 PM and 6:11 PM PST we experienced increased delays in validation and issuance for DNS validated domains in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. Certificates waiting for DNS validation which have proper CNAME entries in DNS will validate within the next 3 hours."
233,AWS Identity and Access Management,Increased Error Rates and Latencies,1579369796,1,,"<div><span class=""yellowfg""> 9:49 AM PST</span>&nbsp;Starting at 7:30 AM PST, we have been experiencing increased error rates due to higher latency for IAM requests. We are actively investigating the issue. </div><div><span class=""yellowfg"">10:05 AM PST</span>&nbsp;Between 7:30 AM and 9:45 AM PST, we experienced increased error rates due to higher latency for IAM requests. The system has recovered and is operating normally.</div>",iam,2020-01-18 17:49:56,Identity and Access Management,Global,7:30 AM,9:45 AM,PST,135.0,2020,1,18,2020-01-18,increased error rates and latencies," 9:49 AM PST -  Starting at 7:30 AM PST, we have been experiencing increased error rates due to higher latency for IAM requests. We are actively investigating the issue. 
10:05 AM PST -  Between 7:30 AM and 9:45 AM PST, we experienced increased error rates due to higher latency for IAM requests. The system has recovered and is operating normally."
234,Amazon Relational Database Service (Mumbai),[RESOLVED] Increased API Error Rates,1579421028,1,,"<div><span class=""yellowfg"">12:03 AM PST</span>&nbsp;We are investigating increased API error rates in the AP-SOUTH-1 Region.</div><div><span class=""yellowfg"">12:35 AM PST</span>&nbsp;Between January 18 11:18 PM and January 19 12:14 AM PST we experienced increased API error rate in the AP-SOUTH-1 Region. The issue has been resolved and the service is operating normally.</div>",rds-ap-south-1,2020-01-19 08:03:48,Relational Database Service,Mumbai,11:18 PM,12:14 AM,PST,56.0,2020,1,19,2020-01-19,increased api error rates,"12:03 AM PST -  We are investigating increased API error rates in the AP-SOUTH-1 Region.
12:35 AM PST -  Between January 18 11:18 PM and January 19 12:14 AM PST we experienced increased API error rate in the AP-SOUTH-1 Region. The issue has been resolved and the service is operating normally."
235,AWS Internet Connectivity (Paris),[RESOLVED] Network Connectivity,1579717555,1,,"<div><span class=""yellowfg"">10:25 AM PST</span>&nbsp;We are investigating an issue which is affecting internet connectivity to a single availability zone in EU-WEST-3 Region.</div><div><span class=""yellowfg"">11:05 AM PST</span>&nbsp;We have identified the root cause of the issue that is affecting connectivity to a single availability zone in EU-WEST-3 Region and continue to work towards resolution.</div><div><span class=""yellowfg"">11:45 AM PST</span>&nbsp;Between 10:00 AM and 11:28 AM PST we experienced an issue affecting network connectivity to AWS services in a single Availability Zone in EU-WEST-3 Region. The issue has been resolved and connectivity has been restored.</div>",internetconnectivity-eu-west-3,2020-01-22 18:25:55,Internet Connectivity,Paris,10:00 AM,11:28 AM,PST,88.0,2020,1,22,2020-01-22,network connectivity,"10:25 AM PST -  We are investigating an issue which is affecting internet connectivity to a single availability zone in EU-WEST-3 Region.
11:05 AM PST -  We have identified the root cause of the issue that is affecting connectivity to a single availability zone in EU-WEST-3 Region and continue to work towards resolution.
11:45 AM PST -  Between 10:00 AM and 11:28 AM PST we experienced an issue affecting network connectivity to AWS services in a single Availability Zone in EU-WEST-3 Region. The issue has been resolved and connectivity has been restored."
236,Amazon Elastic Compute Cloud (Paris),[RESOLVED] Network Connectivity,1579723478,1,,"<div><span class=""yellowfg"">12:04 PM PST</span>&nbsp;Between 10:00 AM and 11:28 AM PST we experienced network connectivity issues affecting EC2 instances in a single Availability Zone in the EU-WEST-3 Region. Instances in the affected Availability Zone were able to connect to the Internet but were unable to resolve DNS records during this time. New instance launches into the affected Availability Zone were also affected by the event. The issue has been resolved and the service is operating normally.</div>",ec2-eu-west-3,2020-01-22 20:04:38,Elastic Compute Cloud,Paris,10:00 AM,11:28 AM,PST,88.0,2020,1,22,2020-01-22,network connectivity,12:04 PM PST -  Between 10:00 AM and 11:28 AM PST we experienced network connectivity issues affecting EC2 instances in a single Availability Zone in the EU-WEST-3 Region. Instances in the affected Availability Zone were able to connect to the Internet but were unable to resolve DNS records during this time. New instance launches into the affected Availability Zone were also affected by the event. The issue has been resolved and the service is operating normally.
237,Amazon Elastic Compute Cloud (Sydney),[RESOLVED] Increased API Error Rates,1579740075,2,,"<div><span class=""yellowfg""> 4:41 PM PST</span>&nbsp;We are investigating increased API error rates and latencies in the AP-SOUTHEAST-2 Region. Connectivity to existing instances is not impacted.</div><div><span class=""yellowfg""> 5:18 PM PST</span>&nbsp;We have identified the root cause of the issue causing increased API error rates and latencies in the AP-SOUTHEAST-2 Region and continue working towards resolution. This issue mainly affects EC2 RunInstances and VPC related API requests. Customer using the EC2 Management Console will also experience error rates for instance and network related functions. Connectivity to existing instances remains unaffected.</div><div><span class=""yellowfg""> 6:25 PM PST</span>&nbsp;We continue to experience increased API error rates for the EC2 APIs in the AP-SOUTHEAST-2 Region. We have confirmed the root cause, and are working on multiple paths toward recovering the subsytem that is impaired, which is responsible for networking related API calls. This issue mainly affects EC2 RunInstance, and VPC related API requests. Customers using the EC2 Management Console may experience errors Describing Resources, as well as making mutating API requests. Connectivity to existing instances in the AP-SOUTHEAST-2 remains unaffected.</div><div><span class=""yellowfg""> 8:49 PM PST</span>&nbsp;We wanted to provide you with more details on the issue causing increased API error rates and latencies in the AP-SOUTHEAST-2 Region. A data store used by a subsystem responsible for the configuration of Virtual Private Cloud (VPC) networks is currently offline and the engineering team are working to restore it. While the investigation into the issue was started immediately, it took us longer to understand the full extent of the issue and determine a path to recovery. We determined that the data store needed to be restored to a point before the issue began. In order to do this restore, we needed to disable writes. Error rates and latencies for the networking-related APIs will continue until the restore has been completed and writes re-enabled. We are working through the recovery process now. With issues like this, it is always difficult to provide an accurate ETA, but we expect to complete the restore process within the next 2 hours and begin to allow API requests to proceed once again. We will continue to keep you updated if that ETA changes. Connectivity to existing instances is not impacted. Also, launch requests that refer to regional objects like subnets that already exist will succeed at this stage, as they do not depend on the affected subsystem. If you know the subnet ID, you can use that to launch instances within the region. We apologize for the impact and continue to work towards full resolution. </div><div><span class=""yellowfg"">10:10 PM PST</span>&nbsp;We continue to make steady progress towards the restoration of the affected data store and are currently within the 2 hours ETA published above.</div><div><span class=""yellowfg"">10:55 PM PST</span>&nbsp;We have completed the restoration of the affected data store but are still working towards re-enabling writes. We have seen an improvement in successful launches over the last 20 minutes and expect that to continue as we work towards full recovery. </div><div><span class=""yellowfg"">11:45 PM PST</span>&nbsp;We can confirm that all error rates and latencies have returned to normal levels. The issue has been resolved and the service is operating normally.</div><div><span class=""yellowfg"">Jan 23, 12:30 AM PST</span>&nbsp;Now that we are fully recovered, we wanted to provide a brief summary of the issue. Starting at 4:07 PM PST, customers began to experience increased error rates and latencies for the network-related APIs in the AP-SOUTHEAST-2 Region. Launches of new EC2 instances also experienced increased failure rates as a result of this issue. Connectivity to existing instances was not affected by this event. We immediately began investigating the root cause and identified that the data store used by the subsystem responsible for the Virtual Private Cloud (VPC) regional state was impaired. While the investigation into the issue was started immediately, it took us longer to understand the full extent of the issue and determine a path to recovery. We determined that the data store needed to be restored to a point before the issue began. We began the data store restoration process, which took a few hours and by 10:50 PM PST, we had fully restored the primary node in the affected data store. At this stage, we began to see recovery in instance launches within the AP-SOUTHEAST-2 Region, restoring many customer applications and services to a healthy state. We continued to bring the data store back to a fully operational state and by 11:20 PM PST, all API error rates and latencies had fully recovered. Other AWS services - including AppStream, Elastic Load Balancing, ElastiCache, Relational Database Service, Amazon WorkSpaces and Lambda – were also affected by this event. We apologize for any inconvenience this event may have caused as we know how critical our services are to our customers. We are never satisfied with operational performance of our services that is anything less than perfect, and will do everything we can to learn from this event and drive improvement across our services.</div>",ec2-ap-southeast-2,2020-01-23 00:41:15,Elastic Compute Cloud,Sydney,4:07 PM,11:20 PM,PST,433.0,2020,1,23,2020-01-23,increased api error rates," 4:41 PM PST -  We are investigating increased API error rates and latencies in the AP-SOUTHEAST-2 Region. Connectivity to existing instances is not impacted.
 5:18 PM PST -  We have identified the root cause of the issue causing increased API error rates and latencies in the AP-SOUTHEAST-2 Region and continue working towards resolution. This issue mainly affects EC2 RunInstances and VPC related API requests. Customer using the EC2 Management Console will also experience error rates for instance and network related functions. Connectivity to existing instances remains unaffected.
 6:25 PM PST -  We continue to experience increased API error rates for the EC2 APIs in the AP-SOUTHEAST-2 Region. We have confirmed the root cause, and are working on multiple paths toward recovering the subsytem that is impaired, which is responsible for networking related API calls. This issue mainly affects EC2 RunInstance, and VPC related API requests. Customers using the EC2 Management Console may experience errors Describing Resources, as well as making mutating API requests. Connectivity to existing instances in the AP-SOUTHEAST-2 remains unaffected.
 8:49 PM PST -  We wanted to provide you with more details on the issue causing increased API error rates and latencies in the AP-SOUTHEAST-2 Region. A data store used by a subsystem responsible for the configuration of Virtual Private Cloud (VPC) networks is currently offline and the engineering team are working to restore it. While the investigation into the issue was started immediately, it took us longer to understand the full extent of the issue and determine a path to recovery. We determined that the data store needed to be restored to a point before the issue began. In order to do this restore, we needed to disable writes. Error rates and latencies for the networking-related APIs will continue until the restore has been completed and writes re-enabled. We are working through the recovery process now. With issues like this, it is always difficult to provide an accurate ETA, but we expect to complete the restore process within the next 2 hours and begin to allow API requests to proceed once again. We will continue to keep you updated if that ETA changes. Connectivity to existing instances is not impacted. Also, launch requests that refer to regional objects like subnets that already exist will succeed at this stage, as they do not depend on the affected subsystem. If you know the subnet ID, you can use that to launch instances within the region. We apologize for the impact and continue to work towards full resolution. 
10:10 PM PST -  We continue to make steady progress towards the restoration of the affected data store and are currently within the 2 hours ETA published above.
10:55 PM PST -  We have completed the restoration of the affected data store but are still working towards re-enabling writes. We have seen an improvement in successful launches over the last 20 minutes and expect that to continue as we work towards full recovery. 
11:45 PM PST -  We can confirm that all error rates and latencies have returned to normal levels. The issue has been resolved and the service is operating normally.
Jan 23, 12:30 AM PST -  Now that we are fully recovered, we wanted to provide a brief summary of the issue. Starting at 4:07 PM PST, customers began to experience increased error rates and latencies for the network-related APIs in the AP-SOUTHEAST-2 Region. Launches of new EC2 instances also experienced increased failure rates as a result of this issue. Connectivity to existing instances was not affected by this event. We immediately began investigating the root cause and identified that the data store used by the subsystem responsible for the Virtual Private Cloud (VPC) regional state was impaired. While the investigation into the issue was started immediately, it took us longer to understand the full extent of the issue and determine a path to recovery. We determined that the data store needed to be restored to a point before the issue began. We began the data store restoration process, which took a few hours and by 10:50 PM PST, we had fully restored the primary node in the affected data store. At this stage, we began to see recovery in instance launches within the AP-SOUTHEAST-2 Region, restoring many customer applications and services to a healthy state. We continued to bring the data store back to a fully operational state and by 11:20 PM PST, all API error rates and latencies had fully recovered. Other AWS services - including AppStream, Elastic Load Balancing, ElastiCache, Relational Database Service, Amazon WorkSpaces and Lambda – were also affected by this event. We apologize for any inconvenience this event may have caused as we know how critical our services are to our customers. We are never satisfied with operational performance of our services that is anything less than perfect, and will do everything we can to learn from this event and drive improvement across our services."
238,Amazon Relational Database Service (Sydney),[RESOLVED] Increased API Error Rates,1579742960,1,,"<div><span class=""yellowfg""> 5:29 PM PST</span>&nbsp;We are investigating increased API error rates and latencies in the AP-SOUTHEAST-2 Region.</div><div><span class=""yellowfg""> 6:20 PM PST</span>&nbsp;We can confirm increased API error rates and latencies in the AP-SOUTHEAST-2 Region and continue to work towards resolution. Connectivity to existing instances remains unaffected.</div><div><span class=""yellowfg""> 7:35 PM PST</span>&nbsp;We are continuing to work towards resolution of increased API error rates and latencies in the AP-SOUTHEAST-2 Region. Connectivity to existing instances remains unaffected.</div><div><span class=""yellowfg""> 9:00 PM PST</span>&nbsp;We continue to experience increased API error rates and latencies due to the issue affecting EC2 within the AP-SOUTHEAST-2 Region. We continue to work towards full resolution.</div><div><span class=""yellowfg"">11:38 PM PST</span>&nbsp;Between 4:41 PM and 11:35 PM PDT we experienced increased API error rates and latencies due to the issue affecting EC2 within the AP-SOUTHEAST-2 Region. The issue has been resolved and the service is operating normally.</div>",rds-ap-southeast-2,2020-01-23 01:29:20,Relational Database Service,Sydney,4:41 PM,11:35 PM,PST,414.0,2020,1,23,2020-01-23,increased api error rates," 5:29 PM PST -  We are investigating increased API error rates and latencies in the AP-SOUTHEAST-2 Region.
 6:20 PM PST -  We can confirm increased API error rates and latencies in the AP-SOUTHEAST-2 Region and continue to work towards resolution. Connectivity to existing instances remains unaffected.
 7:35 PM PST -  We are continuing to work towards resolution of increased API error rates and latencies in the AP-SOUTHEAST-2 Region. Connectivity to existing instances remains unaffected.
 9:00 PM PST -  We continue to experience increased API error rates and latencies due to the issue affecting EC2 within the AP-SOUTHEAST-2 Region. We continue to work towards full resolution.
11:38 PM PST -  Between 4:41 PM and 11:35 PM PDT we experienced increased API error rates and latencies due to the issue affecting EC2 within the AP-SOUTHEAST-2 Region. The issue has been resolved and the service is operating normally."
239,Amazon Elastic Load Balancing (Sydney),[RESOLVED] Increased Provisioning Latencies,1579743205,1,,"<div><span class=""yellowfg""> 5:33 PM PST</span>&nbsp;We are investigating increased provisioning times and ELB API error rates for load balancers in the AP-SOUTHEAST-2 Region. Connectivity to existing load balancers is not affected.</div><div><span class=""yellowfg""> 6:13 PM PST</span>&nbsp;We can confirm increased provisioning/scaling latencies and ELB API error rates for load balancers in the AP-SOUTHEAST-2 Region and continue to work towards resolution. Traffic remains unaffected on running load balancers.</div><div><span class=""yellowfg""> 7:23 PM PST</span>&nbsp;We are continuing to work towards resolution of increased provisioning/scaling latencies and ELB API error rates for load balancers in the AP-SOUTHEAST-2 Region. Traffic remains unaffected on running load balancers.</div><div><span class=""yellowfg""> 8:59 PM PST</span>&nbsp;We continue to experience increased provisioning/scaling latencies and ELB API error rates for load balancers due to the issue affecting EC2 in the AP-SOUTHEAST-2 Region. We continue to work towards full resolution. Traffic remains unaffected on running load balancers.</div><div><span class=""yellowfg"">Jan 23, 12:22 AM PST</span>&nbsp;Between 4:10 PM and 11:40 PM PST, we experienced increased provisioning/scaling latencies and ELB API error rates for load balancers due to the issue affecting EC2 within the AP-SOUTHEAST-2 Region. The issue has been resolved and the service is operating normally.</div>",elb-ap-southeast-2,2020-01-23 01:33:25,Elastic Load Balancing,Sydney,4:10 PM,11:40 PM,PST,450.0,2020,1,23,2020-01-23,increased provisioning latencies," 5:33 PM PST -  We are investigating increased provisioning times and ELB API error rates for load balancers in the AP-SOUTHEAST-2 Region. Connectivity to existing load balancers is not affected.
 6:13 PM PST -  We can confirm increased provisioning/scaling latencies and ELB API error rates for load balancers in the AP-SOUTHEAST-2 Region and continue to work towards resolution. Traffic remains unaffected on running load balancers.
 7:23 PM PST -  We are continuing to work towards resolution of increased provisioning/scaling latencies and ELB API error rates for load balancers in the AP-SOUTHEAST-2 Region. Traffic remains unaffected on running load balancers.
 8:59 PM PST -  We continue to experience increased provisioning/scaling latencies and ELB API error rates for load balancers due to the issue affecting EC2 in the AP-SOUTHEAST-2 Region. We continue to work towards full resolution. Traffic remains unaffected on running load balancers.
Jan 23, 12:22 AM PST -  Between 4:10 PM and 11:40 PM PST, we experienced increased provisioning/scaling latencies and ELB API error rates for load balancers due to the issue affecting EC2 within the AP-SOUTHEAST-2 Region. The issue has been resolved and the service is operating normally."
240,Amazon AppStream 2.0 (Sydney),[RESOLVED] Increased Instance Provisioning Error Rates,1579745293,1,,"<div><span class=""yellowfg""> 6:08 PM PST</span>&nbsp;We are currently experiencing an issue provisioning new image builder and fleet streaming instances in the AP-SOUTHEAST-2 Region.</div><div><span class=""yellowfg""> 7:20 PM PST</span>&nbsp;We are continuing to investigate an increase in instance provisioning error rates in the AP-SOUTHEAST-2 Region.</div><div><span class=""yellowfg""> 8:32 PM PST</span>&nbsp;We have identified the cause of the increased provisioning error rates in the AP-SOUTHEAST-2 Region and continue working towards resolution.</div><div><span class=""yellowfg""> 8:59 PM PST</span>&nbsp;We continue to experience increased instance provisioning error rates due to the issue affecting EC2 within the AP-SOUTHEAST-2 Region. We continue to work towards full resolution. Existing streaming sessions and instances will continue to operate.</div><div><span class=""yellowfg"">Jan 23, 12:41 AM PST</span>&nbsp;We continue to experience increased instance provisioning error rates within the AP-SOUTHEAST-2 Region. We continue to work towards full resolution. Existing streaming sessions and instances will continue to operate.</div><div><span class=""yellowfg"">Jan 23,  1:51 AM PST</span>&nbsp;We are continuing to work towards resolution of increased instance provisioning error rates within the AP-SOUTHEAST-2 Region. Existing streaming sessions and instances will continue to operate.</div><div><span class=""yellowfg"">Jan 23,  2:38 AM PST</span>&nbsp;We recently experienced increased instance provisioning errors within the AP-SOUTHEAST-2 Region. The issue has been resolved and the service is operating normally.</div>",appstream2-ap-southeast-2,2020-01-23 02:08:13,AppStream 2.0,Sydney,6:08 PM,2:38 AM,PST,510.0,2020,1,23,2020-01-23,increased instance provisioning error rates," 6:08 PM PST -  We are currently experiencing an issue provisioning new image builder and fleet streaming instances in the AP-SOUTHEAST-2 Region.
 7:20 PM PST -  We are continuing to investigate an increase in instance provisioning error rates in the AP-SOUTHEAST-2 Region.
 8:32 PM PST -  We have identified the cause of the increased provisioning error rates in the AP-SOUTHEAST-2 Region and continue working towards resolution.
 8:59 PM PST -  We continue to experience increased instance provisioning error rates due to the issue affecting EC2 within the AP-SOUTHEAST-2 Region. We continue to work towards full resolution. Existing streaming sessions and instances will continue to operate.
Jan 23, 12:41 AM PST -  We continue to experience increased instance provisioning error rates within the AP-SOUTHEAST-2 Region. We continue to work towards full resolution. Existing streaming sessions and instances will continue to operate.
Jan 23,  1:51 AM PST -  We are continuing to work towards resolution of increased instance provisioning error rates within the AP-SOUTHEAST-2 Region. Existing streaming sessions and instances will continue to operate.
Jan 23,  2:38 AM PST -  We recently experienced increased instance provisioning errors within the AP-SOUTHEAST-2 Region. The issue has been resolved and the service is operating normally."
241,Amazon ElastiCache (Sydney),[RESOLVED] Increased API Error Rates,1579748460,1,,"<div><span class=""yellowfg""> 7:01 PM PST</span>&nbsp;We are experiencing increased latencies while provisioning new ElastiCache nodes and and elevated API error rates in the AP-SOUTHEAST-2 AWS Region. Existing ElastiCache clusters are not impacted and are continuing to serve traffic. We are working to resolve the issue.</div><div><span class=""yellowfg""> 9:32 PM PST</span>&nbsp;We continue to experience increased latencies for ElastiCache cluster creation, modification and deletion operations, and elevated API error rates due to the issue affecting EC2 within the AP-SOUTHEAST-2 Region. We continue to work towards full resolution. Existing clusters are operating normally.</div><div><span class=""yellowfg"">11:55 PM PST</span>&nbsp;Between 4:09 PM and 11:44 PM PST, we experienced increased latencies for ElastiCache cluster creation, modification and deletion operations, and elevated API error rates due to the issue affecting EC2 within the AP-SOUTHEAST-2 Region. The issue has been resolved and the service is operating normally.</div>",elasticache-ap-southeast-2,2020-01-23 03:01:00,ElastiCache,Sydney,4:09 PM,11:44 PM,PST,455.0,2020,1,23,2020-01-23,increased api error rates," 7:01 PM PST -  We are experiencing increased latencies while provisioning new ElastiCache nodes and and elevated API error rates in the AP-SOUTHEAST-2 AWS Region. Existing ElastiCache clusters are not impacted and are continuing to serve traffic. We are working to resolve the issue.
 9:32 PM PST -  We continue to experience increased latencies for ElastiCache cluster creation, modification and deletion operations, and elevated API error rates due to the issue affecting EC2 within the AP-SOUTHEAST-2 Region. We continue to work towards full resolution. Existing clusters are operating normally.
11:55 PM PST -  Between 4:09 PM and 11:44 PM PST, we experienced increased latencies for ElastiCache cluster creation, modification and deletion operations, and elevated API error rates due to the issue affecting EC2 within the AP-SOUTHEAST-2 Region. The issue has been resolved and the service is operating normally."
242,AWS Lambda (Sydney),[RESOLVED] Increased API Error Rates,1579749477,1,,"<div><span class=""yellowfg""> 7:17 PM PST</span>&nbsp;We can confirm increased API error rates in the AP-SOUTHEAST-2 Region for functions that are configured with VPC settings. Functions that are not configured with VPC settings are unaffected.</div><div><span class=""yellowfg""> 9:02 PM PST</span>&nbsp;We continue to experience increased API error rates in the AP-SOUTHEAST-2 Region for functions that are configured with VPC settings due to the issue affecting EC2 in the AP-SOUTHEAST-2 Region. We continue to work towards full resolution.</div><div><span class=""yellowfg"">11:31 PM PST</span>&nbsp;Between 4:50 PM and 11:00 PM PST, we experienced increased API error rates for functions due to an issue affecting EC2 in the AP-SOUTHEAST-2 Region. The issue has been resolved and the service is operating normally.</div>",lambda-ap-southeast-2,2020-01-23 03:17:57,Lambda,Sydney,4:50 PM,11:00 PM,PST,370.0,2020,1,23,2020-01-23,increased api error rates," 7:17 PM PST -  We can confirm increased API error rates in the AP-SOUTHEAST-2 Region for functions that are configured with VPC settings. Functions that are not configured with VPC settings are unaffected.
 9:02 PM PST -  We continue to experience increased API error rates in the AP-SOUTHEAST-2 Region for functions that are configured with VPC settings due to the issue affecting EC2 in the AP-SOUTHEAST-2 Region. We continue to work towards full resolution.
11:31 PM PST -  Between 4:50 PM and 11:00 PM PST, we experienced increased API error rates for functions due to an issue affecting EC2 in the AP-SOUTHEAST-2 Region. The issue has been resolved and the service is operating normally."
243,Amazon WorkSpaces (Sydney),[RESOLVED] Increased API Error Rates,1579750287,1,,"<div><span class=""yellowfg""> 7:31 PM PST</span>&nbsp;We are investigating increased Amazon WorkSpaces API error rates and provisioning times for Amazon WorkSpaces in the AP-SOUTHEAST-2 Region.</div><div><span class=""yellowfg""> 8:39 PM PST</span>&nbsp;We have identified the cause of increased Amazon WorkSpaces API error rates and provisioning times for Amazon WorkSpaces in the AP-SOUTHEAST-2 Region and continue working towards resolution.</div><div><span class=""yellowfg""> 9:02 PM PST</span>&nbsp;We continue to experience increased API error rates and provisioning times for Amazon WorkSpaces due to the issue affecting EC2 within the AP-SOUTHEAST-2 Region. Existing Amazon WorkSpaces sessions will continue to operate.</div><div><span class=""yellowfg"">11:32 PM PST</span>&nbsp;Between 4:04 PM and 10:55 PM PST, we experienced increased Amazon WorkSpaces API error rates and provisioning times due to the issue affecting EC2 within the AP-SOUTHEAST-2 Region. The issue has been resolved and the service is now operating normally.</div>",workspaces-ap-southeast-2,2020-01-23 03:31:27,WorkSpaces,Sydney,4:04 PM,10:55 PM,PST,411.0,2020,1,23,2020-01-23,increased api error rates," 7:31 PM PST -  We are investigating increased Amazon WorkSpaces API error rates and provisioning times for Amazon WorkSpaces in the AP-SOUTHEAST-2 Region.
 8:39 PM PST -  We have identified the cause of increased Amazon WorkSpaces API error rates and provisioning times for Amazon WorkSpaces in the AP-SOUTHEAST-2 Region and continue working towards resolution.
 9:02 PM PST -  We continue to experience increased API error rates and provisioning times for Amazon WorkSpaces due to the issue affecting EC2 within the AP-SOUTHEAST-2 Region. Existing Amazon WorkSpaces sessions will continue to operate.
11:32 PM PST -  Between 4:04 PM and 10:55 PM PST, we experienced increased Amazon WorkSpaces API error rates and provisioning times due to the issue affecting EC2 within the AP-SOUTHEAST-2 Region. The issue has been resolved and the service is now operating normally."
244,Amazon Route 53,[RESOLVED] Route 53 DNS Change Issues ,1579899055,2,,"<div><span class=""yellowfg"">12:50 PM PST</span>&nbsp;We are investigating increased propagation times of DNS edits to the Route 53 DNS servers. Queries to existing DNS records are not affected by this issue.</div><div><span class=""yellowfg""> 1:21 PM PST</span>&nbsp;We are still investigating increased propagation times of DNS edits to the Route 53 DNS servers. This will affect provisioning of new resources that rely on Route 53 for DNS, such as EFS and PrivateLink. Queries to existing DNS records are not affected by this issue</div><div><span class=""yellowfg""> 1:38 PM PST</span>&nbsp;We are still investigating increased propagation times of DNS edits to the Route 53 DNS servers. To help accelerate recovery, the Route 53 API is temporarily not accepting MakeChange or CreateHostedZone requests. This will also affect provisioning of new resources that rely on Route 53 for DNS, such as: EFS, PrivateLink, Amazon MQ, Amazon Managed Streaming for Apache Kafka, API Gateway.  Queries to existing DNS records are not affected by this issue. </div><div><span class=""yellowfg""> 2:58 PM PST</span>&nbsp;We are still investigating increased propagation times of DNS edits to the Route 53 DNS servers. To help accelerate recovery, the Route 53 API is temporarily not accepting MakeChange or CreateHostedZone requests. Queries to existing DNS records are not affected by this issue. This will also affect provisioning of new resources that rely on Route 53 for DNS, such as: EFS, PrivateLink, Amazon MQ, Amazon Managed Streaming for Apache Kafka, API Gateway, DocumentDB, FSx for Lustre, Certificate Manager, Transfer for SFTP, EKS, CloudFormation and Chime Voice Connector.</div><div><span class=""yellowfg""> 3:08 PM PST</span>&nbsp;We have identified root cause resulting in increased propagation times of DNS edits to the Route 53 DNS servers. The Route 53 API is temporarily not accepting MakeChange or CreateHostedZone requests in order to help accelerate recovery. Queries to existing DNS records are not affected by this issue. This will also affect provisioning of new resources that rely on Route 53 for DNS, such as: EFS, PrivateLink, Amazon MQ, Amazon Managed Streaming for Apache Kafka, API Gateway, DocumentDB, FSx for Lustre, Certificate Manager, Transfer for SFTP, EKS, CloudFormation, Chime Voice Connector and Global Accelerator.</div><div><span class=""yellowfg""> 3:45 PM PST</span>&nbsp;We have identified root cause resulting in increased propagation times of DNS edits to the Route 53 DNS servers, and are working towards recovery. The Route 53 API is now accepting changes again, though these changes are still experiencing delays propagating as there is a significant backlog of changes to process. Queries to existing DNS records are not affected by this issue. This will also affect provisioning of new resources that rely on Route 53 for DNS, such as: EFS, PrivateLink, Amazon MQ, Amazon Managed Streaming for Apache Kafka, API Gateway, DocumentDB, FSx for Lustre, Certificate Manager, Transfer for SFTP, EKS, CloudFormation, Chime Voice Connector, Global Accelerator, RDS, SageMaker Ground Truth, Amazon Managed Blockchain and Directory Service.</div><div><span class=""yellowfg""> 5:21 PM PST</span>&nbsp;Between 12:07 PM and 5:15 PM PST, customers experienced delays propagating changes submitted to the Route 53 API, as well as increased API error rates from 1:55 PM until 3:20 PM. This also affected provisioning of new resources that rely on Route 53 DNS, such as EFS, PrivateLink, Amazon MQ, Amazon Managed Streaming for Apache Kafka, API Gateway, DocumentDB, FSx for Lustre, Certificate Manager, Transfer for SFTP, EKS, CloudFormation, Chime Voice Connector, Global Accelerator, RDS, SageMaker Ground Truth, Amazon Managed Blockchain, Directory Service and Elastic Inference. The Route 53 API is now operating normally, and all changes that were accepted by the Route 53 API have been propagated. Queries for all existing records were answered normally during this time.</div>",route53,2020-01-24 20:50:55,Route 53,Global,12:07 PM,5:15 PM,PST,308.0,2020,1,24,2020-01-24,route 53 dns change issues,"12:50 PM PST -  We are investigating increased propagation times of DNS edits to the Route 53 DNS servers. Queries to existing DNS records are not affected by this issue.
 1:21 PM PST -  We are still investigating increased propagation times of DNS edits to the Route 53 DNS servers. This will affect provisioning of new resources that rely on Route 53 for DNS, such as EFS and PrivateLink. Queries to existing DNS records are not affected by this issue
 1:38 PM PST -  We are still investigating increased propagation times of DNS edits to the Route 53 DNS servers. To help accelerate recovery, the Route 53 API is temporarily not accepting MakeChange or CreateHostedZone requests. This will also affect provisioning of new resources that rely on Route 53 for DNS, such as: EFS, PrivateLink, Amazon MQ, Amazon Managed Streaming for Apache Kafka, API Gateway.  Queries to existing DNS records are not affected by this issue. 
 2:58 PM PST -  We are still investigating increased propagation times of DNS edits to the Route 53 DNS servers. To help accelerate recovery, the Route 53 API is temporarily not accepting MakeChange or CreateHostedZone requests. Queries to existing DNS records are not affected by this issue. This will also affect provisioning of new resources that rely on Route 53 for DNS, such as: EFS, PrivateLink, Amazon MQ, Amazon Managed Streaming for Apache Kafka, API Gateway, DocumentDB, FSx for Lustre, Certificate Manager, Transfer for SFTP, EKS, CloudFormation and Chime Voice Connector.
 3:08 PM PST -  We have identified root cause resulting in increased propagation times of DNS edits to the Route 53 DNS servers. The Route 53 API is temporarily not accepting MakeChange or CreateHostedZone requests in order to help accelerate recovery. Queries to existing DNS records are not affected by this issue. This will also affect provisioning of new resources that rely on Route 53 for DNS, such as: EFS, PrivateLink, Amazon MQ, Amazon Managed Streaming for Apache Kafka, API Gateway, DocumentDB, FSx for Lustre, Certificate Manager, Transfer for SFTP, EKS, CloudFormation, Chime Voice Connector and Global Accelerator.
 3:45 PM PST -  We have identified root cause resulting in increased propagation times of DNS edits to the Route 53 DNS servers, and are working towards recovery. The Route 53 API is now accepting changes again, though these changes are still experiencing delays propagating as there is a significant backlog of changes to process. Queries to existing DNS records are not affected by this issue. This will also affect provisioning of new resources that rely on Route 53 for DNS, such as: EFS, PrivateLink, Amazon MQ, Amazon Managed Streaming for Apache Kafka, API Gateway, DocumentDB, FSx for Lustre, Certificate Manager, Transfer for SFTP, EKS, CloudFormation, Chime Voice Connector, Global Accelerator, RDS, SageMaker Ground Truth, Amazon Managed Blockchain and Directory Service.
 5:21 PM PST -  Between 12:07 PM and 5:15 PM PST, customers experienced delays propagating changes submitted to the Route 53 API, as well as increased API error rates from 1:55 PM until 3:20 PM. This also affected provisioning of new resources that rely on Route 53 DNS, such as EFS, PrivateLink, Amazon MQ, Amazon Managed Streaming for Apache Kafka, API Gateway, DocumentDB, FSx for Lustre, Certificate Manager, Transfer for SFTP, EKS, CloudFormation, Chime Voice Connector, Global Accelerator, RDS, SageMaker Ground Truth, Amazon Managed Blockchain, Directory Service and Elastic Inference. The Route 53 API is now operating normally, and all changes that were accepted by the Route 53 API have been propagated. Queries for all existing records were answered normally during this time."
245,Amazon CloudFront,[RESOLVED] Change Propagation Delays,1580371190,1,,"<div><span class=""yellowfg"">11:59 PM PST</span>&nbsp;We are investigating longer than usual propagation times for changes to CloudFront configurations. End-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=""yellowfg"">Jan 30, 12:15 AM PST</span>&nbsp;We have confirmed that we are seeing increased propagation times for changes to a few CloudFront edge locations. Majority of CloudFront edge locations are consuming configuration changes normally. End-user requests for content from our edge locations are not affected by this issue and are being served normally. </div><div><span class=""yellowfg"">Jan 30,  1:23 AM PST</span>&nbsp;Between January 29 9:12 PM and January 30 12:48 AM PST we experienced delays in propagation times for changes to CloudFront configurations. During this time end-user requests for content from our edge locations were not affected. The issue has been resolved and the service is operating normally.</div>",cloudfront,2020-01-30 07:59:50,CloudFront,Global,9:12 PM,12:48 AM,PST,216.0,2020,1,30,2020-01-30,change propagation delays,"11:59 PM PST -  We are investigating longer than usual propagation times for changes to CloudFront configurations. End-user requests for content from our edge locations are not affected by this issue and are being served normally.
Jan 30, 12:15 AM PST -  We have confirmed that we are seeing increased propagation times for changes to a few CloudFront edge locations. Majority of CloudFront edge locations are consuming configuration changes normally. End-user requests for content from our edge locations are not affected by this issue and are being served normally. 
Jan 30,  1:23 AM PST -  Between January 29 9:12 PM and January 30 12:48 AM PST we experienced delays in propagation times for changes to CloudFront configurations. During this time end-user requests for content from our edge locations were not affected. The issue has been resolved and the service is operating normally."
246,Amazon Elastic Load Balancing (N. Virginia),[RESOLVED] Increased Error Rate,1580754533,1,,"<div><span class=""yellowfg"">10:28 AM PST</span>&nbsp;We are investigating increased error rates and latencies for ELB APIs in the US-EAST-1 Region. Traffic remains unaffected on running load balancers.</div><div><span class=""yellowfg"">10:45 AM PST</span>&nbsp;We have identified the root cause of the increased error rates and continue to work toward full resolution.</div><div><span class=""yellowfg"">11:24 AM PST</span>&nbsp;Between 9:55 and 11:17 AM PST, we experienced increased error rates and latencies in the US-EAST-1 region. Traffic on existing load balancers was unaffected. The issue has been resolved and the service is operating normally.</div>",elb-us-east-1,2020-02-03 18:28:53,Elastic Load Balancing,N. Virginia,10:28 AM,11:17 AM,PST,49.0,2020,2,3,2020-02-03,increased error rate,"10:28 AM PST -  We are investigating increased error rates and latencies for ELB APIs in the US-EAST-1 Region. Traffic remains unaffected on running load balancers.
10:45 AM PST -  We have identified the root cause of the increased error rates and continue to work toward full resolution.
11:24 AM PST -  Between 9:55 and 11:17 AM PST, we experienced increased error rates and latencies in the US-EAST-1 region. Traffic on existing load balancers was unaffected. The issue has been resolved and the service is operating normally."
247,AWS Certificate Manager (N. Virginia),[RESOLVED]  Certificate Issuance Delays,1580774436,1,,"<div><span class=""yellowfg""> 4:00 PM PST</span>&nbsp;We are investigating delays in certificate issuance globally. This does not impact existing certificates. Other dependent services utilizing Certificate Manager may be affected.</div><div><span class=""yellowfg""> 4:34 PM PST</span>&nbsp;Between 12:30 PM and 3:50 PM PST, we experienced delays in issuance of new certificates globally. Existing certificates were unaffected. The issue has been resolved and the service is operating normally. </div>",certificatemanager-us-east-1,2020-02-04 00:00:36,Certificate Manager,N. Virginia,12:30 PM,3:50 PM,PST,200.0,2020,2,4,2020-02-04,certificate issuance delays," 4:00 PM PST -  We are investigating delays in certificate issuance globally. This does not impact existing certificates. Other dependent services utilizing Certificate Manager may be affected.
 4:34 PM PST -  Between 12:30 PM and 3:50 PM PST, we experienced delays in issuance of new certificates globally. Existing certificates were unaffected. The issue has been resolved and the service is operating normally. "
248,Amazon CloudFront,[RESOLVED] Increased Console Errors,1581553041,1,,"<div><span class=""yellowfg""> 4:17 PM PST</span>&nbsp;Between 1:10 PM and 3:47 PM PST we experienced periods of increased error rates when accessing the CloudFront Management Console. Customers may have received a 404 Response. Existing distributions and the CloudFront APIs were not impacted by this issue. The issue has been resolved and the CloudFront Management Console is operating normally.</div>",cloudfront,2020-02-13 00:17:21,CloudFront,Global,1:10 PM,3:47 PM,PST,157.0,2020,2,13,2020-02-13,increased console errors, 4:17 PM PST -  Between 1:10 PM and 3:47 PM PST we experienced periods of increased error rates when accessing the CloudFront Management Console. Customers may have received a 404 Response. Existing distributions and the CloudFront APIs were not impacted by this issue. The issue has been resolved and the CloudFront Management Console is operating normally.
249,Auto Scaling (US-West),[RESOLVED] Increased error rates and latencies,1581624138,1,,"<div><span class=""yellowfg"">12:02 PM PST</span>&nbsp;We are investigating increased error rates and latencies for Autoscaling API calls in the US-GOV-WEST-1 Region.</div><div><span class=""yellowfg"">12:19 PM PST</span>&nbsp;Between 11:34 AM and 12:05 PM PST we experienced increased error rates and latencies for Autoscaling API calls in the US-GOV-WEST-1 Region. The issue is resolved and the service is operating normally.</div>",autoscaling-us-gov-west-1,2020-02-13 20:02:18,Auto Scaling,US-West,11:34 AM,12:05 PM,PST,31.0,2020,2,13,2020-02-13,increased error rates and latencies,"12:02 PM PST -  We are investigating increased error rates and latencies for Autoscaling API calls in the US-GOV-WEST-1 Region.
12:19 PM PST -  Between 11:34 AM and 12:05 PM PST we experienced increased error rates and latencies for Autoscaling API calls in the US-GOV-WEST-1 Region. The issue is resolved and the service is operating normally."
250,Amazon CloudWatch (N. Virginia),[RESOLVED] Elevated query error rates and delays for CloudWatch Logs Insights in US-EAST-1 Region,1583302711,1,,"<div><span class=""yellowfg"">10:18 PM PST</span>&nbsp;We can confirm increased error rates for CloudWatch Logs Insights queries in the US-EAST-1 Region. Customers running queries using the StartQuery API may not be able to successfully schedule the query execution. Log events continue to be available through the CloudWatch Logs GetLogEvents and FilterLogEvents APIs. We are actively working to resolve the issue.</div><div><span class=""yellowfg"">11:02 PM PST</span>&nbsp;We have identified the cause of the increased error rates for CloudWatch Logs Insights queries in the US-EAST-1 Region. Customers running queries using the StartQuery API may not be able to successfully schedule the query execution. Log events continue to be available through the CloudWatch Logs GetLogEvents and FilterLogEvents APIs. We continue to work towards resolution.</div><div><span class=""yellowfg"">Mar 4, 12:13 AM PST</span>&nbsp;We have resolved the increased error rates for CloudWatch Logs Insights queries in the US-EAST-1 Region. The Logs Insights system is currently processing data starting from 8:25 PM PST until now to make it available to query. Until that process completes, customers running Logs Insights queries using the CloudWatch console or the SDK may not be able to see that data from that time period. We continue to work towards full resolution.</div><div><span class=""yellowfg"">Mar 4,  2:40 AM PST</span>&nbsp;We have resolved the increased error rates for CloudWatch Logs Insights queries in the US-EAST-1 Region. The Logs Insights system is currently processing data starting from March 3 8:25 PM PST until now to make it available to query. Until that process completes, customers running Logs Insights queries using the CloudWatch console or the SDK may not be able to see that data from that time period. We have identified the issue and continue to work towards full resolution.</div><div><span class=""yellowfg"">Mar 4,  6:40 AM PST</span>&nbsp;Between March 3, 8:25 PM and March 4, 6:17 AM PST, some customers experienced increased delays for log events in query results in CloudWatch Logs Insights in the US-EAST-1 Region. We are in the process of backfilling the data. We have resolved the issue and the service is operating normally.</div>",cloudwatch-us-east-1,2020-03-04 06:18:31,CloudWatch,N. Virginia,8:25 PM,6:17 AM,PST,592.0,2020,3,4,2020-03-04,elevated query error rates and delays for cloudwatch logs insights in us-east-1 region,"10:18 PM PST -  We can confirm increased error rates for CloudWatch Logs Insights queries in the US-EAST-1 Region. Customers running queries using the StartQuery API may not be able to successfully schedule the query execution. Log events continue to be available through the CloudWatch Logs GetLogEvents and FilterLogEvents APIs. We are actively working to resolve the issue.
11:02 PM PST -  We have identified the cause of the increased error rates for CloudWatch Logs Insights queries in the US-EAST-1 Region. Customers running queries using the StartQuery API may not be able to successfully schedule the query execution. Log events continue to be available through the CloudWatch Logs GetLogEvents and FilterLogEvents APIs. We continue to work towards resolution.
Mar 4, 12:13 AM PST -  We have resolved the increased error rates for CloudWatch Logs Insights queries in the US-EAST-1 Region. The Logs Insights system is currently processing data starting from 8:25 PM PST until now to make it available to query. Until that process completes, customers running Logs Insights queries using the CloudWatch console or the SDK may not be able to see that data from that time period. We continue to work towards full resolution.
Mar 4,  2:40 AM PST -  We have resolved the increased error rates for CloudWatch Logs Insights queries in the US-EAST-1 Region. The Logs Insights system is currently processing data starting from March 3 8:25 PM PST until now to make it available to query. Until that process completes, customers running Logs Insights queries using the CloudWatch console or the SDK may not be able to see that data from that time period. We have identified the issue and continue to work towards full resolution.
Mar 4,  6:40 AM PST -  Between March 3, 8:25 PM and March 4, 6:17 AM PST, some customers experienced increased delays for log events in query results in CloudWatch Logs Insights in the US-EAST-1 Region. We are in the process of backfilling the data. We have resolved the issue and the service is operating normally."
251,Amazon Elastic Load Balancing (N. Virginia),[RESOLVED] Increased error rates and latencies,1583444770,1,,"<div><span class=""yellowfg""> 1:46 PM PST</span>&nbsp;We are investigating increased error rates and latencies for ELB APIs in the US-EAST-1 Region. This issue does not affect traffic on running load balancers.</div><div><span class=""yellowfg""> 2:05 PM PST</span>&nbsp;We can confirm increased error rates and latencies for ELB APIs in the US-EAST-1 Region and continue to work towards resolution. Traffic remains unaffected on running load balancers.</div><div><span class=""yellowfg""> 2:25 PM PST</span>&nbsp;We have identified the root cause of increased error rates and latencies for ELB APIs in the US-EAST-1 Region and continue to work towards resolution. Traffic remains unaffected on running load balancers.</div><div><span class=""yellowfg""> 2:48 PM PST</span>&nbsp;We have addressed one of the causes of increased error rates and latencies for ELB APIs in the US-EAST-1 Region, and continue to work towards resolution. Traffic remains unaffected on running load balancers.</div><div><span class=""yellowfg""> 3:33 PM PST</span>&nbsp;We have recovered from increased error rates and latencies for ELB APIs in the US-EAST-1 Region, and continue to work towards resolution of increased back-end instance registration times. Traffic remains unaffected on running load balancers.</div><div><span class=""yellowfg""> 3:52 PM PST</span>&nbsp;Between 1:25 PM and 3:40 PM PST, we experienced increased API error rates and latencies as well as increased back-end instance registration times in the US-EAST-1 Region. The issue is resolved and the service is operating normally.</div>",elb-us-east-1,2020-03-05 21:46:10,Elastic Load Balancing,N. Virginia,1:25 PM,3:40 PM,PST,135.0,2020,3,5,2020-03-05,increased error rates and latencies," 1:46 PM PST -  We are investigating increased error rates and latencies for ELB APIs in the US-EAST-1 Region. This issue does not affect traffic on running load balancers.
 2:05 PM PST -  We can confirm increased error rates and latencies for ELB APIs in the US-EAST-1 Region and continue to work towards resolution. Traffic remains unaffected on running load balancers.
 2:25 PM PST -  We have identified the root cause of increased error rates and latencies for ELB APIs in the US-EAST-1 Region and continue to work towards resolution. Traffic remains unaffected on running load balancers.
 2:48 PM PST -  We have addressed one of the causes of increased error rates and latencies for ELB APIs in the US-EAST-1 Region, and continue to work towards resolution. Traffic remains unaffected on running load balancers.
 3:33 PM PST -  We have recovered from increased error rates and latencies for ELB APIs in the US-EAST-1 Region, and continue to work towards resolution of increased back-end instance registration times. Traffic remains unaffected on running load balancers.
 3:52 PM PST -  Between 1:25 PM and 3:40 PM PST, we experienced increased API error rates and latencies as well as increased back-end instance registration times in the US-EAST-1 Region. The issue is resolved and the service is operating normally."
252,Amazon Elastic Compute Cloud (Sydney),[RESOLVED] EC2 Launch Failures,1583881487,1,,"<div><span class=""yellowfg""> 4:04 PM PDT</span>&nbsp;We are investigating increased error rates for new launches in a single Availability Zone in the AP-SOUTHEAST-2 Region.</div><div><span class=""yellowfg""> 4:41 PM PDT</span>&nbsp;We are still investigating increased error rates for new launches in a single Availability Zone in the AP-SOUTHEAST-2 Region and continue working towards resolution.</div><div><span class=""yellowfg""> 5:04 PM PDT</span>&nbsp;Between 2:15 PM and 4:40 PM PDT we experienced increased error rates for new launches in a single Availability Zone in the AP-SOUTHEAST-2 Region. The issue has been resolved and the service is operating normally.</div>",ec2-ap-southeast-2,2020-03-10 23:04:47,Elastic Compute Cloud,Sydney,2:15 PM,4:40 PM,PDT,145.0,2020,3,10,2020-03-10,ec2 launch failures," 4:04 PM PDT -  We are investigating increased error rates for new launches in a single Availability Zone in the AP-SOUTHEAST-2 Region.
 4:41 PM PDT -  We are still investigating increased error rates for new launches in a single Availability Zone in the AP-SOUTHEAST-2 Region and continue working towards resolution.
 5:04 PM PDT -  Between 2:15 PM and 4:40 PM PDT we experienced increased error rates for new launches in a single Availability Zone in the AP-SOUTHEAST-2 Region. The issue has been resolved and the service is operating normally."
253,Amazon Route 53,[RESOLVED] Increased Route 53 Console Errors,1583949127,1,,"<div><span class=""yellowfg"">10:52 AM PDT</span>&nbsp;We have confirmed an issue with the Route 53 console which is causing an increased error rate. All calls to the Route 53 API and DNS servers are being answered normally.</div><div><span class=""yellowfg"">11:11 AM PDT</span>&nbsp;Between 10:29 AM and 10:37 AM PDT customers experienced an elevated error rate when accessing the Route 53 Console. The issue has been resolved and all console requests are being answered normally. There was no impact to the Route 53 API, DNS, or Health Checking services, which were all operating normally during this time.</div>",route53,2020-03-11 17:52:07,Route 53,Global,10:29 AM,10:37 AM,PDT,8.0,2020,3,11,2020-03-11,increased route 53 console errors,"10:52 AM PDT -  We have confirmed an issue with the Route 53 console which is causing an increased error rate. All calls to the Route 53 API and DNS servers are being answered normally.
11:11 AM PDT -  Between 10:29 AM and 10:37 AM PDT customers experienced an elevated error rate when accessing the Route 53 Console. The issue has been resolved and all console requests are being answered normally. There was no impact to the Route 53 API, DNS, or Health Checking services, which were all operating normally during this time."
254,Amazon Elastic Compute Cloud (US-West),[RESOLVED] Increased Launch Error Rates,1584350375,2,,"<div><span class=""yellowfg""> 2:19 AM PDT</span>&nbsp;We are investigating increased error rates for new launches in a single Availability Zone in the US-GOV-WEST-1 Region.</div><div><span class=""yellowfg""> 2:37 AM PDT</span>&nbsp;We are currently experiencing increased error rates for new instance launches in the US-GOV-WEST-1 Region. Existing instances are not affected.</div><div><span class=""yellowfg""> 3:18 AM PDT</span>&nbsp;Between 1:47 AM and 3:02 AM PDT we experienced increased error rates for new instance launches in the US-GOV-WEST-1 Region. Existing instances were unaffected. The issue has been resolved and the service is operating normally.</div>",ec2-us-gov-west-1,2020-03-16 09:19:35,Elastic Compute Cloud,US-West,1:47 AM,3:02 AM,PDT,75.0,2020,3,16,2020-03-16,increased launch error rates," 2:19 AM PDT -  We are investigating increased error rates for new launches in a single Availability Zone in the US-GOV-WEST-1 Region.
 2:37 AM PDT -  We are currently experiencing increased error rates for new instance launches in the US-GOV-WEST-1 Region. Existing instances are not affected.
 3:18 AM PDT -  Between 1:47 AM and 3:02 AM PDT we experienced increased error rates for new instance launches in the US-GOV-WEST-1 Region. Existing instances were unaffected. The issue has been resolved and the service is operating normally."
255,AWS CodeBuild (Oregon),[RESOLVED] Increased Error Rates for Builds,1585097779,1,,"<div><span class=""yellowfg""> 5:56 PM PDT</span>&nbsp;We can confirm increased Build error rates in the US-WEST-2 Region. We have identified the root cause and are working toward resolution. </div><div><span class=""yellowfg""> 6:26 PM PDT</span>&nbsp;Between 4:05 PM and 5:51 PM PDT we experienced increased build errors due to a missing dependency in the build workflow. The issue has been resolved and the service is operating normally. </div>",codebuild-us-west-2,2020-03-25 00:56:19,CodeBuild,Oregon,4:05 PM,5:51 PM,PDT,106.0,2020,3,25,2020-03-25,increased error rates for builds," 5:56 PM PDT -  We can confirm increased Build error rates in the US-WEST-2 Region. We have identified the root cause and are working toward resolution. 
 6:26 PM PDT -  Between 4:05 PM and 5:51 PM PDT we experienced increased build errors due to a missing dependency in the build workflow. The issue has been resolved and the service is operating normally. "
256,AWS Marketplace,[RESOLVED] Increased Subscription Error Rates ,1585144373,1,,"<div><span class=""yellowfg""> 6:52 AM PDT</span>&nbsp;We are investigating increased AWS Marketplace subscription error rates. </div><div><span class=""yellowfg""> 7:21 AM PDT</span>&nbsp;We have identified the cause of the increased AWS Marketplace subscription error rates and continue working towards resolution.</div><div><span class=""yellowfg""> 8:41 AM PDT</span>&nbsp;Between 5:05 AM and 8:30 AM PDT we experienced increased AWS Marketplace subscription error rates. The issue has been resolved and the service is operating normally.</div>",marketplace,2020-03-25 13:52:53,Marketplace,Global,5:05 AM,8:30 AM,PDT,205.0,2020,3,25,2020-03-25,increased subscription error rates," 6:52 AM PDT -  We are investigating increased AWS Marketplace subscription error rates. 
 7:21 AM PDT -  We have identified the cause of the increased AWS Marketplace subscription error rates and continue working towards resolution.
 8:41 AM PDT -  Between 5:05 AM and 8:30 AM PDT we experienced increased AWS Marketplace subscription error rates. The issue has been resolved and the service is operating normally."
257,Amazon Simple Email Service (N. Virginia),[RESOLVED] Increased Email Receiving Latencies,1585342008,1,,"<div><span class=""yellowfg""> 1:46 PM PDT</span>&nbsp;We are currently investigating elevated latencies in our email-sending APIs in the US-EAST-1 Region. This includes the SendEmail/SendRawEmail APIs as well as calls made to the SMTP endpoint.</div><div><span class=""yellowfg""> 2:20 PM PDT</span>&nbsp;Between 12:25 PM and 1:52 PM PDT we experienced elevated delivery delays in the US-EAST-1 Region for mail sent to a specific external email provider. This issue impacted the SendEmail/SendRawEmail APIs as well as calls made to the SMTP endpoint. All delayed emails have now been delivered. The issue has been resolved and the service is operating normally. </div>",ses-us-east-1,2020-03-27 20:46:48,Simple Email Service,N. Virginia,12:25 PM,1:52 PM,PDT,87.0,2020,3,27,2020-03-27,increased email receiving latencies," 1:46 PM PDT -  We are currently investigating elevated latencies in our email-sending APIs in the US-EAST-1 Region. This includes the SendEmail/SendRawEmail APIs as well as calls made to the SMTP endpoint.
 2:20 PM PDT -  Between 12:25 PM and 1:52 PM PDT we experienced elevated delivery delays in the US-EAST-1 Region for mail sent to a specific external email provider. This issue impacted the SendEmail/SendRawEmail APIs as well as calls made to the SMTP endpoint. All delayed emails have now been delivered. The issue has been resolved and the service is operating normally. "
258,Amazon CloudFront,[RESOLVED] CloudFront High error rates,1585610898,1,,"<div><span class=""yellowfg""> 4:28 PM PDT</span>&nbsp;We are investigating elevated error rates and elevated latency in multiple edge locations. </div><div><span class=""yellowfg""> 5:08 PM PDT</span>&nbsp;We can confirm elevated error rates and high latency accessing content from multiple Edge Locations, which is also contributing to longer than usual propagation times for changes to CloudFront configurations. We have identified the root cause and continue to work toward resolution.</div><div><span class=""yellowfg""> 5:54 PM PDT</span>&nbsp;We are beginning to see recovery for the elevated error rates and high latency accessing content from multiple Edge Locations. Error rates have recovered for all locations except for Europe. Additionally, we continue to work toward recovery for the increased delays in propagating configuration changes to Cloudfront Distributions. </div><div><span class=""yellowfg""> 6:21 PM PDT</span>&nbsp;Starting 3:18 PM PDT, we experienced elevated error rates and high latency accessing content from multiple Edge Locations. The elevated error rates and elevated latency accessing content were fully recovered at 5:48 PM PDT. During this time, customers may also have experienced longer than usual change propagation delays for CloudFront configurations and invalidations. The backlog of CloudFront configuration changes and invalidations were fully processed by 6:14 PM PDT. All issues have been fully resolved and the system is operating normally.</div>",cloudfront,2020-03-30 23:28:18,CloudFront,Global,4:28 PM,6:14 PM,PDT,106.0,2020,3,30,2020-03-30,cloudfront high error rates," 4:28 PM PDT -  We are investigating elevated error rates and elevated latency in multiple edge locations. 
 5:08 PM PDT -  We can confirm elevated error rates and high latency accessing content from multiple Edge Locations, which is also contributing to longer than usual propagation times for changes to CloudFront configurations. We have identified the root cause and continue to work toward resolution.
 5:54 PM PDT -  We are beginning to see recovery for the elevated error rates and high latency accessing content from multiple Edge Locations. Error rates have recovered for all locations except for Europe. Additionally, we continue to work toward recovery for the increased delays in propagating configuration changes to Cloudfront Distributions. 
 6:21 PM PDT -  Starting 3:18 PM PDT, we experienced elevated error rates and high latency accessing content from multiple Edge Locations. The elevated error rates and elevated latency accessing content were fully recovered at 5:48 PM PDT. During this time, customers may also have experienced longer than usual change propagation delays for CloudFront configurations and invalidations. The backlog of CloudFront configuration changes and invalidations were fully processed by 6:14 PM PDT. All issues have been fully resolved and the system is operating normally."
259,Amazon Elastic Compute Cloud (N. Virginia),[RESOLVED] Increased Launch Errors &amp; API Errors,1586473044,1,,"<div><span class=""yellowfg""> 3:57 PM PDT</span>&nbsp;We are investigating increased error rates for new launches in a single Availability Zone in the US-EAST-1 Region. </div><div><span class=""yellowfg""> 4:21 PM PDT</span>&nbsp;We can confirm increased error rates for new launches and API errors for RunInstances, AttachVolume and AttachNetworkInterface in a single Availability Zone in the US-EAST-1 Region. We continue to work towards resolution.</div><div><span class=""yellowfg""> 4:54 PM PDT</span>&nbsp;We have identified the root cause resulting in increased error rates for new instance launches and API errors for RunInstances, AttachVolume and AttachNetworkInterface in a single Availability Zone in the US-EAST-1 Region. We are seeing signs of recovery and continue to work toward full resolution.</div><div><span class=""yellowfg""> 5:48 PM PDT</span>&nbsp;Between 3:26 PM and 5:44 PM PDT we experienced increased error rates for new instance launches and periods of API errors for RunInstances, AttachVolume and AttachNetworkInterface in a single Availability Zone in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",ec2-us-east-1,2020-04-09 22:57:24,Elastic Compute Cloud,N. Virginia,3:26 PM,5:44 PM,PDT,138.0,2020,4,9,2020-04-09,increased launch errors &amp; api errors," 3:57 PM PDT -  We are investigating increased error rates for new launches in a single Availability Zone in the US-EAST-1 Region. 
 4:21 PM PDT -  We can confirm increased error rates for new launches and API errors for RunInstances, AttachVolume and AttachNetworkInterface in a single Availability Zone in the US-EAST-1 Region. We continue to work towards resolution.
 4:54 PM PDT -  We have identified the root cause resulting in increased error rates for new instance launches and API errors for RunInstances, AttachVolume and AttachNetworkInterface in a single Availability Zone in the US-EAST-1 Region. We are seeing signs of recovery and continue to work toward full resolution.
 5:48 PM PDT -  Between 3:26 PM and 5:44 PM PDT we experienced increased error rates for new instance launches and periods of API errors for RunInstances, AttachVolume and AttachNetworkInterface in a single Availability Zone in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
260,Amazon Simple Queue Service (N. Virginia),[RESOLVED] Elevated Error Rates For FIFO Queues,1587119196,1,,"<div><span class=""yellowfg""> 3:26 AM PDT</span>&nbsp;We are investigating increased error rates for send and receive operations for FIFO Queues in the US-EAST-1 Region. All other queues are operating normally.
</div><div><span class=""yellowfg""> 4:10 AM PDT</span>&nbsp;We have identified the cause of the increased error rates for send and receive operations for FIFO Queues in the US-EAST-1 Region and continue to work towards resolution. All other queues are operating normally and newly created FIFO queues will work without error.</div><div><span class=""yellowfg""> 5:05 AM PDT</span>&nbsp;We have identified the cause of the increased error rates for send and receive operations for FIFO Queues in the US-EAST-1 Region, and we are currently in the process of deploying the fix.</div><div><span class=""yellowfg""> 5:17 AM PDT</span>&nbsp;Between 2:01 AM PDT and 5:07 AM PDT we experienced increased error rates for send and receive operations for FIFO Queues in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",sqs-us-east-1,2020-04-17 10:26:36,Simple Queue Service,N. Virginia,2:01 AM,5:07 AM,PDT,186.0,2020,4,17,2020-04-17,elevated error rates for fifo queues," 3:26 AM PDT -  We are investigating increased error rates for send and receive operations for FIFO Queues in the US-EAST-1 Region. All other queues are operating normally.
 4:10 AM PDT -  We have identified the cause of the increased error rates for send and receive operations for FIFO Queues in the US-EAST-1 Region and continue to work towards resolution. All other queues are operating normally and newly created FIFO queues will work without error.
 5:05 AM PDT -  We have identified the cause of the increased error rates for send and receive operations for FIFO Queues in the US-EAST-1 Region, and we are currently in the process of deploying the fix.
 5:17 AM PDT -  Between 2:01 AM PDT and 5:07 AM PDT we experienced increased error rates for send and receive operations for FIFO Queues in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
261,Amazon CloudWatch (Tokyo),[RESOLVED] Alarm delays,1587379127,1,,"<div><span class=""yellowfg""> 3:38 AM PDT</span>&nbsp;現在、AP-NORTHEAST-1 リージョンのいくつかのアラームの処理における増加したレイテンシーについて調査を行なっております。| We are investigating increased latencies for processing some alarms in the AP-NORTHEAST-1 Region.</div><div><span class=""yellowfg""> 4:23 AM PDT</span>&nbsp;AP-NORTHEAST-1 リージョンのいくつかのアラーム処理の遅延について引き続き調査しております。問題の解決に向けて対応しております。| We continue to investigate delays in processing some alarms in the AP-NORTHEAST-1 Region. We continue to work toward resolution. </div><div><span class=""yellowfg""> 6:18 AM PDT</span>&nbsp;4/20 19:03から21:42(JST)にかけて AP-NORTHEAST-1 リージョンにてお客様がいくつかのアラームの遅延が発生していた可能性がございます。現在ではこちらの問題は解決され、サービスは正常な状態に復旧しております。| Between 3:03 AM and 5:42 AM PDT, customers may have experienced some delayed alarms in the AP-NORTHEAST-1 Region. We have resolved the issue and the service is operating normally. </div>",cloudwatch-ap-northeast-1,2020-04-20 10:38:47,CloudWatch,Tokyo,3:03 AM,5:42 AM,PDT,159.0,2020,4,20,2020-04-20,alarm delays," 3:38 AM PDT -  現在、AP-NORTHEAST-1 リージョンのいくつかのアラームの処理における増加したレイテンシーについて調査を行なっております。| We are investigating increased latencies for processing some alarms in the AP-NORTHEAST-1 Region.
 4:23 AM PDT -  AP-NORTHEAST-1 リージョンのいくつかのアラーム処理の遅延について引き続き調査しております。問題の解決に向けて対応しております。| We continue to investigate delays in processing some alarms in the AP-NORTHEAST-1 Region. We continue to work toward resolution. 
 6:18 AM PDT -  4/20 19:03から21:42(JST)にかけて AP-NORTHEAST-1 リージョンにてお客様がいくつかのアラームの遅延が発生していた可能性がございます。現在ではこちらの問題は解決され、サービスは正常な状態に復旧しております。| Between 3:03 AM and 5:42 AM PDT, customers may have experienced some delayed alarms in the AP-NORTHEAST-1 Region. We have resolved the issue and the service is operating normally. "
262,Amazon Simple Queue Service (Tokyo),[RESOLVED] Elevated Error Rates,1587379347,2,,"<div><span class=""yellowfg""> 3:42 AM PDT</span>&nbsp;AP-NORTHEAST-1 リージョンの送信および受信操作におけるエラーレートの上昇について調査を行なっております。 | We are investigating elevated error rates for send and receive operations in the AP-NORTHEAST-1 Region</div><div><span class=""yellowfg""> 4:21 AM PDT</span>&nbsp;AP-NORTHEAST-1 リージョンの送信および受信操作におけるエラーレート上昇を認識しております。| We can confirm elevated error rates for send and receive operations in the AP-NORTHEAST-1 Region.</div><div><span class=""yellowfg""> 5:45 AM PDT</span>&nbsp;AP-NORTHEAST-1 リージョンの送信および受信操作におけるエラーレート上昇の原因を特定いたしました。現在問題の軽減に向けて対応を進めております。| We have identified the cause of the increased error rates for send and receive operations in the AP-NORTHEAST-1 Region. We are working towards mitigation.</div><div><span class=""yellowfg""> 6:28 AM PDT</span>&nbsp;4/20 18:56から22:04(JST)にかけて AP-NORTHEAST-1 リージョンの送信および受信操作におけるエラーレートの上昇が発生しておりました。現在ではこちらの問題は解決され、サービスは正常な状態に復旧しております。| Between 2:56 AM and 6:04 AM PDT, we experienced elevated error rates for send and receive operations in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally.</div>",sqs-ap-northeast-1,2020-04-20 10:42:27,Simple Queue Service,Tokyo,2:56 AM,6:04 AM,PDT,188.0,2020,4,20,2020-04-20,elevated error rates," 3:42 AM PDT -  AP-NORTHEAST-1 リージョンの送信および受信操作におけるエラーレートの上昇について調査を行なっております。 | We are investigating elevated error rates for send and receive operations in the AP-NORTHEAST-1 Region
 4:21 AM PDT -  AP-NORTHEAST-1 リージョンの送信および受信操作におけるエラーレート上昇を認識しております。| We can confirm elevated error rates for send and receive operations in the AP-NORTHEAST-1 Region.
 5:45 AM PDT -  AP-NORTHEAST-1 リージョンの送信および受信操作におけるエラーレート上昇の原因を特定いたしました。現在問題の軽減に向けて対応を進めております。| We have identified the cause of the increased error rates for send and receive operations in the AP-NORTHEAST-1 Region. We are working towards mitigation.
 6:28 AM PDT -  4/20 18:56から22:04(JST)にかけて AP-NORTHEAST-1 リージョンの送信および受信操作におけるエラーレートの上昇が発生しておりました。現在ではこちらの問題は解決され、サービスは正常な状態に復旧しております。| Between 2:56 AM and 6:04 AM PDT, we experienced elevated error rates for send and receive operations in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally."
263,AWS Lambda (Tokyo),[RESOLVED] Elevated Error Rates,1587379534,1,,"<div><span class=""yellowfg""> 3:45 AM PDT</span>&nbsp;現在、 AP-NORTHEAST-1 リージョンの非同期実行およびコントロールプレーンへの書き込みAPIにおけるエラーレートの上昇について調査を行なっております。 | We are investigating elevated error rates for asynchronous invocations and control plane APIs in the AP-NORTHEAST-1 Region.</div><div><span class=""yellowfg""> 6:42 AM PDT</span>&nbsp;AP-NORTHEAST-1 リージョンの非同期実行およびコントロールプレーンへの書き込みAPIにおけるエラーレートの上昇につきまして、原因を特定いたしました。現在では大幅なエラーレートの減少を確認できており、完全な復旧のために継続して対応を行なっております。 | We identified the cause of the increased error rates for asynchronous invocations and write control plane APIs in the AP-NORTHEAST-1 Region. We are now seeing significantly reduced error rates, and continue to work towards full recovery.</div><div><span class=""yellowfg""> 7:09 AM PDT</span>&nbsp;4/20 19:03から22:50(JST)にかけて AP-NORTHEAST-1 リージョンの非同期実行およびコントロールプレーンへの書き込みAPIにおいてエラーレートの上昇が発生しておりました。現在ではこちらの問題は解決され、サービスは正常な状態に復旧しております。| Between 3:03 AM and 6:50 AM PDT, we we experienced increased error rates for asynchronous invocations and write control plane APIs in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally.</div>",lambda-ap-northeast-1,2020-04-20 10:45:34,Lambda,Tokyo,3:03 AM,6:50 AM,PDT,227.0,2020,4,20,2020-04-20,elevated error rates," 3:45 AM PDT -  現在、 AP-NORTHEAST-1 リージョンの非同期実行およびコントロールプレーンへの書き込みAPIにおけるエラーレートの上昇について調査を行なっております。 | We are investigating elevated error rates for asynchronous invocations and control plane APIs in the AP-NORTHEAST-1 Region.
 6:42 AM PDT -  AP-NORTHEAST-1 リージョンの非同期実行およびコントロールプレーンへの書き込みAPIにおけるエラーレートの上昇につきまして、原因を特定いたしました。現在では大幅なエラーレートの減少を確認できており、完全な復旧のために継続して対応を行なっております。 | We identified the cause of the increased error rates for asynchronous invocations and write control plane APIs in the AP-NORTHEAST-1 Region. We are now seeing significantly reduced error rates, and continue to work towards full recovery.
 7:09 AM PDT -  4/20 19:03から22:50(JST)にかけて AP-NORTHEAST-1 リージョンの非同期実行およびコントロールプレーンへの書き込みAPIにおいてエラーレートの上昇が発生しておりました。現在ではこちらの問題は解決され、サービスは正常な状態に復旧しております。| Between 3:03 AM and 6:50 AM PDT, we we experienced increased error rates for asynchronous invocations and write control plane APIs in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally."
264,AWS CloudFormation (Tokyo),[RESOLVED] Elevated Error Rates,1587380986,1,,"<div><span class=""yellowfg""> 4:09 AM PDT</span>&nbsp;現在 AP-NORTHEAST-1 リージョンのすべてのスタック操作のエラーレート上昇および遅延について調査を行なっております。| We are investigating elevated error rates and latencies for all stack operations in the AP-NORTHEAST-1 Region.</div><div><span class=""yellowfg""> 6:24 AM PDT</span>&nbsp;/20 19:00から21:40(JST)にかけて AP-NORTHEAST-1 リージョンにおいてエラーレートとレイテンシーの上昇が発生しておりました。現在ではこちらの問題は解決され、サービスは正常な状態に復旧しております。|Between 3:00 AM and 5:40 AM PDT we experienced increased error rates and latencies in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally. | Between 3:00 AM and 5:40 AM PDT we experienced increased error rates and latencies in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally.</div>",cloudformation-ap-northeast-1,2020-04-20 11:09:46,CloudFormation,Tokyo,3:00 AM,5:40 AM,PDT,160.0,2020,4,20,2020-04-20,elevated error rates," 4:09 AM PDT -  現在 AP-NORTHEAST-1 リージョンのすべてのスタック操作のエラーレート上昇および遅延について調査を行なっております。| We are investigating elevated error rates and latencies for all stack operations in the AP-NORTHEAST-1 Region.
 6:24 AM PDT -  /20 19:00から21:40(JST)にかけて AP-NORTHEAST-1 リージョンにおいてエラーレートとレイテンシーの上昇が発生しておりました。現在ではこちらの問題は解決され、サービスは正常な状態に復旧しております。|Between 3:00 AM and 5:40 AM PDT we experienced increased error rates and latencies in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally. | Between 3:00 AM and 5:40 AM PDT we experienced increased error rates and latencies in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally."
265,Amazon Elastic Compute Cloud (N. Virginia),[RESOLVED] Increased API Error Rates and Latencies,1587462325,1,,"<div><span class=""yellowfg""> 2:45 AM PDT</span>&nbsp;We are investigating increased API error rates and latencies in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 3:16 AM PDT</span>&nbsp;Between 2:10 AM and 2:59 AM PDT we experienced increased API error rates and latencies in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",ec2-us-east-1,2020-04-21 09:45:25,Elastic Compute Cloud,N. Virginia,2:10 AM,2:59 AM,PDT,49.0,2020,4,21,2020-04-21,increased api error rates and latencies," 2:45 AM PDT -  We are investigating increased API error rates and latencies in the US-EAST-1 Region.
 3:16 AM PDT -  Between 2:10 AM and 2:59 AM PDT we experienced increased API error rates and latencies in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
266,Amazon CloudFront,[RESOLVED] Delays in Invalidation Change Times,1587509850,1,,"<div><span class=""yellowfg""> 3:57 PM PDT</span>&nbsp;We are investigating longer than usual invalidation change times for changes to CloudFront configurations. This issue is not impacting propagation times for changes to CloudFront configurations. End-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=""yellowfg""> 4:28 PM PDT</span>&nbsp;Between 2:26 PM and 4:20 PM PDT, we experienced delays in invalidation times for changes to CloudFront configurations. During this time end-user requests for content and propagation times for changes to CloudFront configurations were not affected. The issue has been resolved and the service is operating normally.</div>",cloudfront,2020-04-21 22:57:30,CloudFront,Global,2:26 PM,4:20 PM,PDT,114.0,2020,4,21,2020-04-21,delays in invalidation change times," 3:57 PM PDT -  We are investigating longer than usual invalidation change times for changes to CloudFront configurations. This issue is not impacting propagation times for changes to CloudFront configurations. End-user requests for content from our edge locations are not affected by this issue and are being served normally.
 4:28 PM PDT -  Between 2:26 PM and 4:20 PM PDT, we experienced delays in invalidation times for changes to CloudFront configurations. During this time end-user requests for content and propagation times for changes to CloudFront configurations were not affected. The issue has been resolved and the service is operating normally."
267,AWS CloudFormation (Oregon),Increased Error Rates,1588279630,1,,"<div><span class=""yellowfg""> 1:47 PM PDT</span>&nbsp;We are investigating increased error rates and latencies for AWS CloudFormation stack operations in the US-WEST-2 Region.</div><div><span class=""yellowfg""> 2:38 PM PDT</span>&nbsp;We have identified the root cause for the increased error rates and latencies for AWS CloudFormation stack operations in the US-WEST-2 Region and continue to work toward resolution.</div><div><span class=""yellowfg""> 3:10 PM PDT</span>&nbsp;Between 12:02 PM and 2:35 PM PDT we experienced increased error rates and latencies for AWS CloudFormation Stacks operations in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",cloudformation-us-west-2,2020-04-30 20:47:10,CloudFormation,Oregon,12:02 PM,2:35 PM,PDT,153.0,2020,4,30,2020-04-30,increased error rates," 1:47 PM PDT -  We are investigating increased error rates and latencies for AWS CloudFormation stack operations in the US-WEST-2 Region.
 2:38 PM PDT -  We have identified the root cause for the increased error rates and latencies for AWS CloudFormation stack operations in the US-WEST-2 Region and continue to work toward resolution.
 3:10 PM PDT -  Between 12:02 PM and 2:35 PM PDT we experienced increased error rates and latencies for AWS CloudFormation Stacks operations in the US-WEST-2 Region. The issue has been resolved and the service is operating normally."
268,Amazon Elastic Container Service (N. California),[RESOLVED] Elevated API Error Rates,1589077890,1,,"<div><span class=""yellowfg""> 7:31 PM PDT</span>&nbsp;We are investigating elevated API error rates and latencies in the US-WEST-1 Region.</div><div><span class=""yellowfg""> 8:13 PM PDT</span>&nbsp;We can confirm elevated API error rates and latencies in the US-WEST-1 Region and continue to work towards resolution.</div><div><span class=""yellowfg""> 9:06 PM PDT</span>&nbsp;We have identified the cause of the elevated API error rates and latencies in the US-WEST-1 Region and continue working towards resolution. Running tasks are not impacted.</div><div><span class=""yellowfg""> 9:52 PM PDT</span>&nbsp;Between 6:53 PM and 9:37 PM PDT we experienced elevated API error rates and latencies in the US-WEST-1 Region. The issue has been resolved and the service is operating normally. Running tasks were not impacted.</div>",ecs-us-west-1,2020-05-10 02:31:30,Elastic Container Service,N. California,6:53 PM,9:37 PM,PDT,164.0,2020,5,10,2020-05-10,elevated api error rates," 7:31 PM PDT -  We are investigating elevated API error rates and latencies in the US-WEST-1 Region.
 8:13 PM PDT -  We can confirm elevated API error rates and latencies in the US-WEST-1 Region and continue to work towards resolution.
 9:06 PM PDT -  We have identified the cause of the elevated API error rates and latencies in the US-WEST-1 Region and continue working towards resolution. Running tasks are not impacted.
 9:52 PM PDT -  Between 6:53 PM and 9:37 PM PDT we experienced elevated API error rates and latencies in the US-WEST-1 Region. The issue has been resolved and the service is operating normally. Running tasks were not impacted."
269,AWS Batch (N. California),[RESOLVED]  Increased Delays in Job State Transition,1589078329,1,,"<div><span class=""yellowfg""> 7:38 PM PDT</span>&nbsp;We are investigating delay in job state transitions of AWS Batch Jobs in the US-WEST-1 Region. </div><div><span class=""yellowfg""> 8:32 PM PDT</span>&nbsp;We can confirm increased delay in job state transitions of AWS Batch Jobs in the US-WEST-1 Region and continue to work towards resolution.</div><div><span class=""yellowfg""> 9:12 PM PDT</span>&nbsp;We have identified the cause of increased delays in job state transitions of AWS Batch Jobs in the US-WEST-1 Region and continue to work towards resolution.</div><div><span class=""yellowfg""> 9:55 PM PDT</span>&nbsp;Between 6:55 PM and 9:38PM PDT we experienced delayed job state transitions of AWS Batch Jobs in the US-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",batch-us-west-1,2020-05-10 02:38:49,Batch,N. California,6:55 PM,9:38PM PDT,PDT,163.0,2020,5,10,2020-05-10,increased delays in job state transition," 7:38 PM PDT -  We are investigating delay in job state transitions of AWS Batch Jobs in the US-WEST-1 Region. 
 8:32 PM PDT -  We can confirm increased delay in job state transitions of AWS Batch Jobs in the US-WEST-1 Region and continue to work towards resolution.
 9:12 PM PDT -  We have identified the cause of increased delays in job state transitions of AWS Batch Jobs in the US-WEST-1 Region and continue to work towards resolution.
 9:55 PM PDT -  Between 6:55 PM and 9:38PM PDT we experienced delayed job state transitions of AWS Batch Jobs in the US-WEST-1 Region. The issue has been resolved and the service is operating normally."
270,Amazon Elasticsearch Service (N. Virginia),Increased indexing and query error rates for some VPC Domains,1590072270,1,,"<div><span class=""yellowfg""> 7:44 AM PDT</span>&nbsp;We are experiencing elevated error rates for indexing and query operations affecting a small number of VPC domains in the US-EAST-1 Region. We have identified the issue and working on recovery. </div><div><span class=""yellowfg""> 8:38 AM PDT</span>&nbsp;Between 2:55 AM and 8:30 AM PDT, we experienced elevated error rates for indexing and query operations affecting a small number of VPC domains in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",elasticsearch-us-east-1,2020-05-21 14:44:30,Elasticsearch Service,N. Virginia,2:55 AM,8:30 AM,PDT,335.0,2020,5,21,2020-05-21,increased indexing and query error rates for some vpc domains," 7:44 AM PDT -  We are experiencing elevated error rates for indexing and query operations affecting a small number of VPC domains in the US-EAST-1 Region. We have identified the issue and working on recovery. 
 8:38 AM PDT -  Between 2:55 AM and 8:30 AM PDT, we experienced elevated error rates for indexing and query operations affecting a small number of VPC domains in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
271,Amazon Elastic Container Registry (Oregon),[RESOLVED] Increased Error Rates,1590187729,1,,"<div><span class=""yellowfg""> 3:48 PM PDT</span>&nbsp;Between 2:48 PM and 3:10 PM PDT Amazon ECR experienced certificate errors while using Docker clients in the US-WEST-2 Region. We have resolved the issue and the service is operating normally.</div>",ecr-us-west-2,2020-05-22 22:48:49,Elastic Container Registry,Oregon,2:48 PM,3:10 PM,PDT,22.0,2020,5,22,2020-05-22,increased error rates, 3:48 PM PDT -  Between 2:48 PM and 3:10 PM PDT Amazon ECR experienced certificate errors while using Docker clients in the US-WEST-2 Region. We have resolved the issue and the service is operating normally.
272,Amazon Connect (Oregon),[RESOLVED] Increased Error Rates ,1591222848,1,,"<div><span class=""yellowfg""> 3:20 PM PDT</span>&nbsp;We are investigating intermittent issues accessing the Amazon Connect web application in the US-WEST-2 Region.</div><div><span class=""yellowfg""> 3:51 PM PDT</span>&nbsp;We have identified the root cause of the issue causing intermittent issues accessing the Amazon Connect web application in the US-WEST-2 Region and continue working towards resolution.</div><div><span class=""yellowfg""> 4:40 PM PDT</span>&nbsp;We are beginning to see recovery for intermittent issues accessing the Amazon Connect web application in the US-WEST-2 Region. We continue to work toward full recovery.</div><div><span class=""yellowfg""> 5:53 PM PDT</span>&nbsp;Between 2:04 PM and 5:10 PM PDT some Amazon Connect customers experienced issues accessing the Amazon Connect web application in the US-WEST-2 Region. For a portion of that time, some users may have seen error messages when accessing the Amazon Connect web application. The issue has been resolved and the service is operating normally.</div>",connect-us-west-2,2020-06-03 22:20:48,Connect,Oregon,2:04 PM,5:10 PM,PDT,186.0,2020,6,3,2020-06-03,increased error rates," 3:20 PM PDT -  We are investigating intermittent issues accessing the Amazon Connect web application in the US-WEST-2 Region.
 3:51 PM PDT -  We have identified the root cause of the issue causing intermittent issues accessing the Amazon Connect web application in the US-WEST-2 Region and continue working towards resolution.
 4:40 PM PDT -  We are beginning to see recovery for intermittent issues accessing the Amazon Connect web application in the US-WEST-2 Region. We continue to work toward full recovery.
 5:53 PM PDT -  Between 2:04 PM and 5:10 PM PDT some Amazon Connect customers experienced issues accessing the Amazon Connect web application in the US-WEST-2 Region. For a portion of that time, some users may have seen error messages when accessing the Amazon Connect web application. The issue has been resolved and the service is operating normally."
273,Amazon DynamoDB (Sao Paulo),[RESOLVED] Increased 400 error rates ,1591406183,1,,"<div><span class=""yellowfg""> 6:16 PM PDT</span>&nbsp;We are investigating increased error rates for API requests talking to DynamoDB streams in the SA-EAST-1 Region.</div><div><span class=""yellowfg""> 6:34 PM PDT</span>&nbsp;We have identified the root cause of increased 400 error rate for API requests talking to DynamoDB streams in the SA-EAST-1 Region. Restarting applications talking to streams will resolve the issue while we continue to work towards resolution.</div><div><span class=""yellowfg""> 7:43 PM PDT</span>&nbsp;We are beginning to see recovery for the increased 400 error rate for API requests talking to DynamoDB streams in the SA-EAST-1 Region. Restarting applications talking to streams will resolve the issue while we continue to work towards full recovery.</div><div><span class=""yellowfg""> 9:59 PM PDT</span>&nbsp;Between 3:50 PM and 9:53 PM PDT customers experienced increased 400 error rate for API requests talking to DynamoDB streams in the SA-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",dynamodb-sa-east-1,2020-06-06 01:16:23,DynamoDB,Sao Paulo,3:50 PM,9:53 PM,PDT,363.0,2020,6,6,2020-06-06,increased 400 error rates," 6:16 PM PDT -  We are investigating increased error rates for API requests talking to DynamoDB streams in the SA-EAST-1 Region.
 6:34 PM PDT -  We have identified the root cause of increased 400 error rate for API requests talking to DynamoDB streams in the SA-EAST-1 Region. Restarting applications talking to streams will resolve the issue while we continue to work towards resolution.
 7:43 PM PDT -  We are beginning to see recovery for the increased 400 error rate for API requests talking to DynamoDB streams in the SA-EAST-1 Region. Restarting applications talking to streams will resolve the issue while we continue to work towards full recovery.
 9:59 PM PDT -  Between 3:50 PM and 9:53 PM PDT customers experienced increased 400 error rate for API requests talking to DynamoDB streams in the SA-EAST-1 Region. The issue has been resolved and the service is operating normally."
274,Amazon Simple Notification Service (N. Virginia),[RESOLVED] Increased Error Rates for SMS Delivery,1591822333,1,,"<div><span class=""yellowfg""> 1:52 PM PDT</span>&nbsp;We are investigating elevated error rates and latencies when delivering SMS messages in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 2:33 PM PDT</span>&nbsp;We have identified the root cause of increased error rates and latencies when delivering SMS messages in the US-EAST-1 Region and are working towards resolution.</div><div><span class=""yellowfg""> 2:47 PM PDT</span>&nbsp;Between 9:10 AM and 2:04 PM PDT, we experienced elevated error rates and latencies when delivering SMS messages in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",sns-us-east-1,2020-06-10 20:52:13,Simple Notification Service,N. Virginia,9:10 AM,2:04 PM,PDT,294.0,2020,6,10,2020-06-10,increased error rates for sms delivery," 1:52 PM PDT -  We are investigating elevated error rates and latencies when delivering SMS messages in the US-EAST-1 Region.
 2:33 PM PDT -  We have identified the root cause of increased error rates and latencies when delivering SMS messages in the US-EAST-1 Region and are working towards resolution.
 2:47 PM PDT -  Between 9:10 AM and 2:04 PM PDT, we experienced elevated error rates and latencies when delivering SMS messages in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
275,AWS Identity and Access Management,[RESOLVED] Increased API Error Rates ,1591943413,2,,"<div><span class=""yellowfg"">11:30 PM PDT</span>&nbsp;We are investigating increased error rates and latencies on AWS IAM administrative APIs with potential impact in multiple regions. IAM role creation is impacted. Other AWS services whose features require these actions may also be impacted. User authentications and authorizations are not impacted.</div><div><span class=""yellowfg"">Jun 12, 12:03 AM PDT</span>&nbsp;We continue to investigate increased error rates and latencies on AWS IAM administrative APIs with potential impact in multiple regions. IAM role creation is impacted. Other AWS services like AWS CloudFormation whose features require these actions may also be impacted. User authentications and authorizations are not impacted.</div><div><span class=""yellowfg"">Jun 12,  2:12 AM PDT</span>&nbsp;We have identified the root cause of the increased error rates and latencies on the AWS IAM CreateRole and CreateServiceLinkedRole APIs and are working towards resolution. Other AWS services such as AWS CloudFormation whose features require these actions may also be impacted. User authentications and authorizations are not impacted.</div><div><span class=""yellowfg"">Jun 12,  3:30 AM PDT</span>&nbsp;We wanted to provide you with more details on the issue causing increased error rates and latencies on the AWS IAM CreateRole and CreateServiceLinkedRole APIs. While we have identified the root cause and are working towards resolution, with an issue like this, it is always difficult to provide an accurate ETA, but we expect to restore access to the CreateRole and CreateServiceLinkedRole APIs within the next several hours. We are working through the recovery process now and will continue to keep you updated if this ETA changes. IAM user authentications and authorizations are not impacted. Other AWS services like AWS CloudFormation whose features require these actions may also be impacted.</div><div><span class=""yellowfg"">Jun 12,  4:27 AM PDT</span>&nbsp;We are beginning to see improvements as we continue to work towards recovery for the AWS IAM CreateRole and CreateServiceLinkedRole API Error Rates. Full resolution is estimated to take a few hours. Other AWS services such as AWS CloudFormation whose features require these actions will continue to be impacted. User authentications and authorizations are not impacted.</div><div><span class=""yellowfg"">Jun 12,  5:20 AM PDT</span>&nbsp;We continue to see significant recovery for impacted customers for the increased error rates and latencies on the AWS IAM CreateRole and CreateServiceLinkedRoles APIs. We expect the residual error rates and latency to subside further over the next 2-3 hours. Other AWS services such as AWS CloudFormation whose features require these actions will see a similar overall reduction in errors during that time. User authentications and authorizations remain unimpacted.</div><div><span class=""yellowfg"">Jun 12,  6:43 AM PDT</span>&nbsp;Between June 11 9:56 PM PDT and June 12 6:40 AM PDT, AWS IAM experienced increased error rates and latencies on the AWS IAM CreateRole and CreateServiceLinkedRoles APIs. The issue has been resolved and the service is operating normally.</div><div><span class=""yellowfg"">Jun 12,  7:24 PM PDT</span>&nbsp;Although this issue has been resolved and the service is operating normally, if you have any questions or any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at <a href=""https://console.aws.amazon.com/support/"">https://console.aws.amazon.com/support/</a>.</div>",iam,2020-06-12 06:30:13,Identity and Access Management,Global,9:56 PM,6:40 AM,PDT,524.0,2020,6,12,2020-06-12,increased api error rates,"11:30 PM PDT -  We are investigating increased error rates and latencies on AWS IAM administrative APIs with potential impact in multiple regions. IAM role creation is impacted. Other AWS services whose features require these actions may also be impacted. User authentications and authorizations are not impacted.
Jun 12, 12:03 AM PDT -  We continue to investigate increased error rates and latencies on AWS IAM administrative APIs with potential impact in multiple regions. IAM role creation is impacted. Other AWS services like AWS CloudFormation whose features require these actions may also be impacted. User authentications and authorizations are not impacted.
Jun 12,  2:12 AM PDT -  We have identified the root cause of the increased error rates and latencies on the AWS IAM CreateRole and CreateServiceLinkedRole APIs and are working towards resolution. Other AWS services such as AWS CloudFormation whose features require these actions may also be impacted. User authentications and authorizations are not impacted.
Jun 12,  3:30 AM PDT -  We wanted to provide you with more details on the issue causing increased error rates and latencies on the AWS IAM CreateRole and CreateServiceLinkedRole APIs. While we have identified the root cause and are working towards resolution, with an issue like this, it is always difficult to provide an accurate ETA, but we expect to restore access to the CreateRole and CreateServiceLinkedRole APIs within the next several hours. We are working through the recovery process now and will continue to keep you updated if this ETA changes. IAM user authentications and authorizations are not impacted. Other AWS services like AWS CloudFormation whose features require these actions may also be impacted.
Jun 12,  4:27 AM PDT -  We are beginning to see improvements as we continue to work towards recovery for the AWS IAM CreateRole and CreateServiceLinkedRole API Error Rates. Full resolution is estimated to take a few hours. Other AWS services such as AWS CloudFormation whose features require these actions will continue to be impacted. User authentications and authorizations are not impacted.
Jun 12,  5:20 AM PDT -  We continue to see significant recovery for impacted customers for the increased error rates and latencies on the AWS IAM CreateRole and CreateServiceLinkedRoles APIs. We expect the residual error rates and latency to subside further over the next 2-3 hours. Other AWS services such as AWS CloudFormation whose features require these actions will see a similar overall reduction in errors during that time. User authentications and authorizations remain unimpacted.
Jun 12,  6:43 AM PDT -  Between June 11 9:56 PM PDT and June 12 6:40 AM PDT, AWS IAM experienced increased error rates and latencies on the AWS IAM CreateRole and CreateServiceLinkedRoles APIs. The issue has been resolved and the service is operating normally.
Jun 12,  7:24 PM PDT -  Although this issue has been resolved and the service is operating normally, if you have any questions or any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support/."
276,AWS Organizations,[RESOLVED]  Increased API Error Rates,1591948670,1,,"<div><span class=""yellowfg"">12:57 AM PDT</span>&nbsp;We are investigating increased error rates for account creation and invitations for Organizations with all features enabled. Other AWS services using Organizations service trust may also be impacted when activating new organizations or accounts.</div><div><span class=""yellowfg""> 2:13 AM PDT</span>&nbsp;We have identified the root cause of the increased error rates for account creation and invitations for Organizations with all features enabled and are working towards resolution. Other AWS services using Organizations service trust may also be impacted when activating new organizations or accounts.</div><div><span class=""yellowfg""> 5:38 AM PDT</span>&nbsp;We confirm significant recovery for account creation and invitations for Organizations with all features enabled. Other AWS services using Organizations service trust are also recovering when activating new organizations or accounts. In line with IAM, we expect the residual error rates and latency to subside further over the next 2-3 hours. </div><div><span class=""yellowfg""> 6:27 AM PDT</span>&nbsp;Between June 11 9:55 PM PDT and June 12 5:54 AM PDT, we experienced increased error rates for account creation and invitations for Organizations with all features enabled. Other AWS services using Organizations service trust may have also been impacted when activating new organizations or accounts. The issue has been resolved and the service is operating normally. If you have any questions or operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at <a href=""https://aws.amazon.com/support"">https://aws.amazon.com/support</a>.</div>",organizations,2020-06-12 07:57:50,Organizations,Global,9:55 PM,5:54 AM,PDT,479.0,2020,6,12,2020-06-12,increased api error rates,"12:57 AM PDT -  We are investigating increased error rates for account creation and invitations for Organizations with all features enabled. Other AWS services using Organizations service trust may also be impacted when activating new organizations or accounts.
 2:13 AM PDT -  We have identified the root cause of the increased error rates for account creation and invitations for Organizations with all features enabled and are working towards resolution. Other AWS services using Organizations service trust may also be impacted when activating new organizations or accounts.
 5:38 AM PDT -  We confirm significant recovery for account creation and invitations for Organizations with all features enabled. Other AWS services using Organizations service trust are also recovering when activating new organizations or accounts. In line with IAM, we expect the residual error rates and latency to subside further over the next 2-3 hours. 
 6:27 AM PDT -  Between June 11 9:55 PM PDT and June 12 5:54 AM PDT, we experienced increased error rates for account creation and invitations for Organizations with all features enabled. Other AWS services using Organizations service trust may have also been impacted when activating new organizations or accounts. The issue has been resolved and the service is operating normally. If you have any questions or operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://aws.amazon.com/support."
277,Amazon CloudWatch (Ireland),Elevated API faults and latencies in EU-WEST-1.,1592911198,1,,"<div><span class=""yellowfg""> 4:19 AM PDT</span>&nbsp;We are investigating increased faults and latencies for CloudWatch APIs and metrics in the EU-WEST-1 Region. CloudWatch alarms may transition into ""INSUFFICIENT_DATA"" state if set on delayed metrics.</div><div><span class=""yellowfg""> 4:55 AM PDT</span>&nbsp;Between 3:13 AM and 4:37 AM PDT, some customers experienced elevated faults when calling CloudWatch APIs in the EU-WEST-1 Region. Some metrics were delayed, and CloudWatch alarms on delayed metrics transitioned into INSUFFICIENT_DATA state. The issue has been resolved and the service is operating normally.</div>",cloudwatch-eu-west-1,2020-06-23 11:19:58,CloudWatch,Ireland,3:13 AM,4:37 AM,PDT,84.0,2020,6,23,2020-06-23,elevated api faults and latencies in eu-west-1.," 4:19 AM PDT -  We are investigating increased faults and latencies for CloudWatch APIs and metrics in the EU-WEST-1 Region. CloudWatch alarms may transition into ""INSUFFICIENT_DATA"" state if set on delayed metrics.
 4:55 AM PDT -  Between 3:13 AM and 4:37 AM PDT, some customers experienced elevated faults when calling CloudWatch APIs in the EU-WEST-1 Region. Some metrics were delayed, and CloudWatch alarms on delayed metrics transitioned into INSUFFICIENT_DATA state. The issue has been resolved and the service is operating normally."
278,Amazon Elastic Compute Cloud (Oregon),[RESOLVED] Increased Network Provisioning Latencies,1593207544,1,,"<div><span class=""yellowfg""> 2:39 PM PDT</span>&nbsp;We are experiencing delayed network provisioning times for newly launched instances and newly mapped Elastic IP addresses in a single Availability Zone in the US-WEST-2 Region. Existing instances are not affected by this issue. </div><div><span class=""yellowfg""> 3:04 PM PDT</span>&nbsp;We have resolved the issue affecting network provisioning times for newly launched instances and newly mapped Elastic IP addresses in a single Availability Zone in the US-WEST-2 Region. Existing instances were not affected by this issue. The issue is resolved and the service is operating normally.</div>",ec2-us-west-2,2020-06-26 21:39:04,Elastic Compute Cloud,Oregon,2:39 PM,3:04 PM,PDT,25.0,2020,6,26,2020-06-26,increased network provisioning latencies," 2:39 PM PDT -  We are experiencing delayed network provisioning times for newly launched instances and newly mapped Elastic IP addresses in a single Availability Zone in the US-WEST-2 Region. Existing instances are not affected by this issue. 
 3:04 PM PDT -  We have resolved the issue affecting network provisioning times for newly launched instances and newly mapped Elastic IP addresses in a single Availability Zone in the US-WEST-2 Region. Existing instances were not affected by this issue. The issue is resolved and the service is operating normally."
279,Amazon Elastic Compute Cloud (N. Virginia),[RESOLVED] Increased API Error Rates,1594241886,1,,"<div><span class=""yellowfg""> 1:58 PM PDT</span>&nbsp;We are investigating an increased API error rate for the DescribeInstances and DescribeImages APIs in the US-EAST-1 Region. </div><div><span class=""yellowfg""> 2:11 PM PDT</span>&nbsp;Between 1:22 PM and 2:06 PM PDT we experienced increased error rates and latencies for the DescribeInstances and DescribeImages APIs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",ec2-us-east-1,2020-07-08 20:58:06,Elastic Compute Cloud,N. Virginia,1:22 PM,2:06 PM,PDT,44.0,2020,7,8,2020-07-08,increased api error rates," 1:58 PM PDT -  We are investigating an increased API error rate for the DescribeInstances and DescribeImages APIs in the US-EAST-1 Region. 
 2:11 PM PDT -  Between 1:22 PM and 2:06 PM PDT we experienced increased error rates and latencies for the DescribeInstances and DescribeImages APIs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
280,Amazon CloudWatch (London),[RESOLVED] Elevated latencies and Faults in EU-WEST-2,1594383041,2,,"<div><span class=""yellowfg""> 5:10 AM PDT</span>&nbsp;We are investigating increased faults and latencies for CloudWatch APIs and metrics in the EU-WEST-2 Region.</div><div><span class=""yellowfg""> 5:24 AM PDT</span>&nbsp;We can confirm elevated API faults and some delayed metrics in EU-WEST-2 Region. We are actively working to resolve the issue.</div><div><span class=""yellowfg""> 6:04 AM PDT</span>&nbsp;We have identified the root cause for elevated API faults and latencies for the GetMetricData and GetMetricWidgetImage APIs in the EU-WEST-2 Region. We are actively working to resolve the issue and are starting to see recovery.</div><div><span class=""yellowfg""> 6:32 AM PDT</span>&nbsp;We have identified the root cause for elevated API faults and latencies for the GetMetricData and GetMetricWidgetImage APIs in the EU-WEST-2 Region. We can confirm significant recovery and are working actively to fully resolve the issue.</div><div><span class=""yellowfg""> 8:02 AM PDT</span>&nbsp;Between 4:07 AM and 7:28 AM PDT, customers experienced elevated API fault rates and latencies in the EU-WEST-2 Region. Some metrics were also delayed initially. We have resolved the issue and the service is operating normally.</div>",cloudwatch-eu-west-2,2020-07-10 12:10:41,CloudWatch,London,4:07 AM,7:28 AM,PDT,201.0,2020,7,10,2020-07-10,elevated latencies and faults in eu-west-2," 5:10 AM PDT -  We are investigating increased faults and latencies for CloudWatch APIs and metrics in the EU-WEST-2 Region.
 5:24 AM PDT -  We can confirm elevated API faults and some delayed metrics in EU-WEST-2 Region. We are actively working to resolve the issue.
 6:04 AM PDT -  We have identified the root cause for elevated API faults and latencies for the GetMetricData and GetMetricWidgetImage APIs in the EU-WEST-2 Region. We are actively working to resolve the issue and are starting to see recovery.
 6:32 AM PDT -  We have identified the root cause for elevated API faults and latencies for the GetMetricData and GetMetricWidgetImage APIs in the EU-WEST-2 Region. We can confirm significant recovery and are working actively to fully resolve the issue.
 8:02 AM PDT -  Between 4:07 AM and 7:28 AM PDT, customers experienced elevated API fault rates and latencies in the EU-WEST-2 Region. Some metrics were also delayed initially. We have resolved the issue and the service is operating normally."
281,Amazon Elastic Compute Cloud (Ireland),[RESOLVED] Network connectivity issues,1594732662,1,,"<div><span class=""yellowfg""> 6:17 AM PDT</span>&nbsp;We are investigating network connectivity issues for some instances in a single Availability Zone in the EU-WEST-1 Region.

</div><div><span class=""yellowfg""> 6:37 AM PDT</span>&nbsp;Network connectivity has been restored for the vast majority of the affected instances in a single Availability Zone in the EU-WEST-1 Region. Some EBS volumes within the affected Availability Zone are also experiencing degraded performance. We continue to work towards full recovery.</div><div><span class=""yellowfg""> 7:35 AM PDT</span>&nbsp;Starting at 5:35 AM PDT we experienced power and network connectivity issues for some instances, and degraded performance for some EBS volumes in a single Availability Zone in the EU-WEST-1 Region. By 6:00 AM PDT, power and networking connectivity had been restored for affected instances and by 6:31 AM PDT, degraded performance for affected EBS volumes had been resolved. By 7:08 AM PDT, the vast majority of affected instances had fully recovered. The small number of remaining instances are hosted on hardware which was adversely affected by the loss of power. While we will continue to work to recover all affected instances and volumes, for immediate recovery, we recommend replacing any remaining affected instances or volumes if possible. The issue has been resolved and the service is operating normally.</div>",ec2-eu-west-1,2020-07-14 13:17:42,Elastic Compute Cloud,Ireland,5:35 AM,7:08 AM,PDT,93.0,2020,7,14,2020-07-14,network connectivity issues," 6:17 AM PDT -  We are investigating network connectivity issues for some instances in a single Availability Zone in the EU-WEST-1 Region.
 6:37 AM PDT -  Network connectivity has been restored for the vast majority of the affected instances in a single Availability Zone in the EU-WEST-1 Region. Some EBS volumes within the affected Availability Zone are also experiencing degraded performance. We continue to work towards full recovery.
 7:35 AM PDT -  Starting at 5:35 AM PDT we experienced power and network connectivity issues for some instances, and degraded performance for some EBS volumes in a single Availability Zone in the EU-WEST-1 Region. By 6:00 AM PDT, power and networking connectivity had been restored for affected instances and by 6:31 AM PDT, degraded performance for affected EBS volumes had been resolved. By 7:08 AM PDT, the vast majority of affected instances had fully recovered. The small number of remaining instances are hosted on hardware which was adversely affected by the loss of power. While we will continue to work to recover all affected instances and volumes, for immediate recovery, we recommend replacing any remaining affected instances or volumes if possible. The issue has been resolved and the service is operating normally."
282,Amazon Elastic Compute Cloud (US-East),[RESOLVED] Increased error rates ,1594851373,1,,"<div><span class=""yellowfg""> 3:16 PM PDT</span>&nbsp;We are investigating increased API error rates and latencies in the US-GOV-EAST-1 Region.</div><div><span class=""yellowfg""> 3:47 PM PDT</span>&nbsp;Between 2:39 PM and 3:38 PM PDT we experienced increased API error rates and latencies in the US-GOV-EAST-1 Region. The issue has been resolved and the service is operating normally.
</div>",ec2-us-gov-east-1,2020-07-15 22:16:13,Elastic Compute Cloud,US-East,2:39 PM,3:38 PM,PDT,59.0,2020,7,15,2020-07-15,increased error rates," 3:16 PM PDT -  We are investigating increased API error rates and latencies in the US-GOV-EAST-1 Region.
 3:47 PM PDT -  Between 2:39 PM and 3:38 PM PDT we experienced increased API error rates and latencies in the US-GOV-EAST-1 Region. The issue has been resolved and the service is operating normally."
283,Amazon Simple Workflow Service (N. Virginia),[RESOLVED] Elevated API and Workflow Execution Latencies ,1595269046,1,,"<div><span class=""yellowfg"">11:17 AM PDT</span>&nbsp;Between 5:50 AM and 10:50 AM PDT, we experienced elevated API and workflow execution latencies in the US-EAST-1 Region. Some AWS services were also impacted during this period. The issue has been resolved and the services are operating normally.</div>",swf-us-east-1,2020-07-20 18:17:26,Simple Workflow Service,N. Virginia,5:50 AM,10:50 AM,PDT,300.0,2020,7,20,2020-07-20,elevated api and workflow execution latencies,"11:17 AM PDT -  Between 5:50 AM and 10:50 AM PDT, we experienced elevated API and workflow execution latencies in the US-EAST-1 Region. Some AWS services were also impacted during this period. The issue has been resolved and the services are operating normally."
284,AWS CloudFormation (N. Virginia),[RESOLVED] Elevated Stack Latencies,1595269919,1,,"<div><span class=""yellowfg"">11:32 AM PDT</span>&nbsp;Between 5:50 AM and 10:50 AM PDT, AWS CloudFormation experienced increased latencies when creating, updating, and deleting AWS CloudFormation stacks in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",cloudformation-us-east-1,2020-07-20 18:31:59,CloudFormation,N. Virginia,5:50 AM,10:50 AM,PDT,300.0,2020,7,20,2020-07-20,elevated stack latencies,"11:32 AM PDT -  Between 5:50 AM and 10:50 AM PDT, AWS CloudFormation experienced increased latencies when creating, updating, and deleting AWS CloudFormation stacks in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
285,AWS Identity and Access Management,[RESOLVED] Increased API Error Rates,1595327154,1,,"<div><span class=""yellowfg""> 3:25 AM PDT</span>&nbsp;Between 12:02 AM and 2:35 AM PDT AWS customers experienced increased error rates while calling the IAM assume role, get session token and other APIs with the long term credentials. As of 2:35 AM PDT, we are fully recovered and the issue is resolved now. Other AWS services such as AWS CloudFormation whose features require these actions experienced similar impact.</div>",iam,2020-07-21 10:25:54,Identity and Access Management,Global,12:02 AM,2:35 AM,PDT,153.0,2020,7,21,2020-07-21,increased api error rates," 3:25 AM PDT -  Between 12:02 AM and 2:35 AM PDT AWS customers experienced increased error rates while calling the IAM assume role, get session token and other APIs with the long term credentials. As of 2:35 AM PDT, we are fully recovered and the issue is resolved now. Other AWS services such as AWS CloudFormation whose features require these actions experienced similar impact."
286,Amazon Elastic Compute Cloud (N. Virginia),[RESOLVED] Increased API error rates,1595527657,1,,"<div><span class=""yellowfg"">11:07 AM PDT</span>&nbsp;We are investigating increased API error rates in the US-EAST-1 Region.</div><div><span class=""yellowfg"">12:23 PM PDT</span>&nbsp;We're seeing some recovery for the increased EC2 API error rates and latencies in the US-EAST-1 Region, but some API requests may see a ""request limit exceeded"" responses as we work towards full recovery. Some Instance status checks for newly launched EC2 instances in a single Availability Zone may return insufficient-data for their instance and system status checks. Connectivity to existing EC2 instances is not affected by this event.</div><div><span class=""yellowfg"">12:57 PM PDT</span>&nbsp;We're seeing recovery for the increased EC2 API error rates and latencies in the US-EAST-1 Region, but some API requests may still see ""request limit exceeded"" responses as we continue to work on this issue. Instance status checks for newly launched EC2 instances are no longer returning insufficient-data for their instance and system status checks. Connectivity to existing EC2 instances is not affected by this event.</div><div><span class=""yellowfg""> 2:03 PM PDT</span>&nbsp;We're seeing recovery for the increased EC2 API error rates and latencies in the US-EAST-1 Region and a smaller number of API requests are now returning ""request limit exceeded"" responses as we continue to work on this issue. Instance status checks for newly launched EC2 instances are no longer returning insufficient-data for their instance and system status checks. Connectivity to existing EC2 instances is not affected by this event. </div><div><span class=""yellowfg""> 2:52 PM PDT</span>&nbsp;We're seeing recovery for the increased EC2 API error rates and latencies in the US-EAST-1 Region. The API DescribeVolumes may return ""request limit exceeded"" responses as we continue to work on this issue. Instance status checks for newly launched EC2 instances are no longer returning insufficient-data for their instance and system status checks. Connectivity to existing EC2 instances is not affected by this event.</div><div><span class=""yellowfg""> 3:56 PM PDT</span>&nbsp;We are seeing recovery for increased EC2 API error rates and latencies with the exception of small number of DescribeVolumes calls which may return ""request limit exceeded"" responses as we continue to work on this issue. Instance status checks for EC2 instances have recovered. Connectivity to existing EC2 instances is not affected by this event.</div><div><span class=""yellowfg""> 5:02 PM PDT</span>&nbsp;We have resolved the issue causing periods of increased EC2 API error rates and latencies as well as insufficient-data results for EC2 instance and system status checks in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",ec2-us-east-1,2020-07-23 18:07:37,Elastic Compute Cloud,N. Virginia,11:07 AM,5:02 PM,PDT,355.0,2020,7,23,2020-07-23,increased api error rates,"11:07 AM PDT -  We are investigating increased API error rates in the US-EAST-1 Region.
12:23 PM PDT -  We're seeing some recovery for the increased EC2 API error rates and latencies in the US-EAST-1 Region, but some API requests may see a ""request limit exceeded"" responses as we work towards full recovery. Some Instance status checks for newly launched EC2 instances in a single Availability Zone may return insufficient-data for their instance and system status checks. Connectivity to existing EC2 instances is not affected by this event.
12:57 PM PDT -  We're seeing recovery for the increased EC2 API error rates and latencies in the US-EAST-1 Region, but some API requests may still see ""request limit exceeded"" responses as we continue to work on this issue. Instance status checks for newly launched EC2 instances are no longer returning insufficient-data for their instance and system status checks. Connectivity to existing EC2 instances is not affected by this event.
 2:03 PM PDT -  We're seeing recovery for the increased EC2 API error rates and latencies in the US-EAST-1 Region and a smaller number of API requests are now returning ""request limit exceeded"" responses as we continue to work on this issue. Instance status checks for newly launched EC2 instances are no longer returning insufficient-data for their instance and system status checks. Connectivity to existing EC2 instances is not affected by this event. 
 2:52 PM PDT -  We're seeing recovery for the increased EC2 API error rates and latencies in the US-EAST-1 Region. The API DescribeVolumes may return ""request limit exceeded"" responses as we continue to work on this issue. Instance status checks for newly launched EC2 instances are no longer returning insufficient-data for their instance and system status checks. Connectivity to existing EC2 instances is not affected by this event.
 3:56 PM PDT -  We are seeing recovery for increased EC2 API error rates and latencies with the exception of small number of DescribeVolumes calls which may return ""request limit exceeded"" responses as we continue to work on this issue. Instance status checks for EC2 instances have recovered. Connectivity to existing EC2 instances is not affected by this event.
 5:02 PM PDT -  We have resolved the issue causing periods of increased EC2 API error rates and latencies as well as insufficient-data results for EC2 instance and system status checks in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
287,Amazon Route 53 Resolver (N. Virginia),[RESOLVED] DNS Propagation Delays for EC2 Instance Names ,1595861956,1,,"<div><span class=""yellowfg""> 7:59 AM PDT</span>&nbsp;We are investigating VPC DNS propagation delays in the US-EAST-1 Region. This issue only affects resolution of newly launched EC2 instances that rely on public to private IP DNS mapping, such as newly launched Relational Database Service (RDS) instances. It will also affect configuration changes to VPC peering associations. We have identified root cause and are actively working towards identifying a mitigation.</div><div><span class=""yellowfg""> 8:24 AM PDT</span>&nbsp;We have identified a mitigation path and are working towards resolution. This issue only affects resolution of newly launched EC2 instances that rely on public to private IP DNS mapping, such as newly launched Relational Database Service (RDS) instances from within a VPC. It will also affect configuration changes to VPC peering associations.</div><div><span class=""yellowfg""> 9:26 AM PDT</span>&nbsp;We have confirmed our mitigation is correct, and are actively deploying it. Recovery is in progress now.</div><div><span class=""yellowfg"">10:43 AM PDT</span>&nbsp;We are continuing to deploy the mitigation to this issue. Impact is limited to workloads that require resolving public DNS names to private IP addresses for instances launched in a newly created VPC, as well as reverse DNS lookups from within a newly created VPC. Resolution of all instances in existing VPCs continue to work normally.</div><div><span class=""yellowfg"">12:04 PM PDT</span>&nbsp;We are continuing to deploy the mitigation to this issue. Customers may see partial recovery as we work towards full resolution.</div><div><span class=""yellowfg"">12:57 PM PDT</span>&nbsp;Mitigation efforts continue, and customers should see increased recovery as we work towards full resolution.</div><div><span class=""yellowfg""> 1:41 PM PDT</span>&nbsp;Between 6:32 AM and 1:35 PM PDT, customers experienced issues resolving newly created public EC2 instance names to private IPs within newly created VPCs in the US-EAST-1 Region. Creation of new Route 53 Resolver Endpoints was also delayed during this time. Newly created instances within existing VPCs resolved normally, and all other DNS functionality worked normally in all VPCs during this time. The issue has been resolved and all DNS queries are being answered normally.</div>",route53resolver-us-east-1,2020-07-27 14:59:16,Route 53 Resolver,N. Virginia,6:32 AM,1:35 PM,PDT,423.0,2020,7,27,2020-07-27,dns propagation delays for ec2 instance names," 7:59 AM PDT -  We are investigating VPC DNS propagation delays in the US-EAST-1 Region. This issue only affects resolution of newly launched EC2 instances that rely on public to private IP DNS mapping, such as newly launched Relational Database Service (RDS) instances. It will also affect configuration changes to VPC peering associations. We have identified root cause and are actively working towards identifying a mitigation.
 8:24 AM PDT -  We have identified a mitigation path and are working towards resolution. This issue only affects resolution of newly launched EC2 instances that rely on public to private IP DNS mapping, such as newly launched Relational Database Service (RDS) instances from within a VPC. It will also affect configuration changes to VPC peering associations.
 9:26 AM PDT -  We have confirmed our mitigation is correct, and are actively deploying it. Recovery is in progress now.
10:43 AM PDT -  We are continuing to deploy the mitigation to this issue. Impact is limited to workloads that require resolving public DNS names to private IP addresses for instances launched in a newly created VPC, as well as reverse DNS lookups from within a newly created VPC. Resolution of all instances in existing VPCs continue to work normally.
12:04 PM PDT -  We are continuing to deploy the mitigation to this issue. Customers may see partial recovery as we work towards full resolution.
12:57 PM PDT -  Mitigation efforts continue, and customers should see increased recovery as we work towards full resolution.
 1:41 PM PDT -  Between 6:32 AM and 1:35 PM PDT, customers experienced issues resolving newly created public EC2 instance names to private IPs within newly created VPCs in the US-EAST-1 Region. Creation of new Route 53 Resolver Endpoints was also delayed during this time. Newly created instances within existing VPCs resolved normally, and all other DNS functionality worked normally in all VPCs during this time. The issue has been resolved and all DNS queries are being answered normally."
288,Amazon Route 53 Resolver (N. Virginia),[RESOLVED] Single AZ Intermittent DNS Lookup,1595927486,1,,"<div><span class=""yellowfg""> 2:11 AM PDT</span>&nbsp;We are investigating an increase in DNS lookup failures from EC2 instances in a single Availability Zone in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 2:57 AM PDT</span>&nbsp;We are continuing to investigate increased DNS resolution errors from EC2 instances in a single Availability Zone of the US-EAST-1 Region. This could also impact functionality of other services that use EC2, such as App Mesh, Cloud9, ElasticSearch, EMR, Managed Blockchain, SageMaker, Transfer for SFTP, WorkMail, and Glue.</div><div><span class=""yellowfg""> 3:44 AM PDT</span>&nbsp;We are implementing a mitigation to the increased DNS resolution errors from EC2 instances in a single Availability Zone in the US-EAST-1 Region, and are starting to see recovery. This issue could also impact functionality of other services that use EC2, such as App Mesh, Cloud9, ElasticSearch, EMR, Managed Blockchain, SageMaker, Transfer for SFTP, WorkMail, RDS, and Glue.</div><div><span class=""yellowfg""> 4:19 AM PDT</span>&nbsp;DNS resolution failures in a single Availability Zone in the US-EAST-1 Region have largely been mitigated, and we are continuing to work towards full mitigation. </div><div><span class=""yellowfg""> 4:37 AM PDT</span>&nbsp;Between 1:22 AM and 4:13 AM PDT, customers experienced an increase in DNS resolution errors from EC2 instances in a single Availability Zone of the US-EAST-1 Region. This could have also impacted functionality of other services that use EC2, such as RDS, SageMaker, EMR, WorkMail, MSK, AWS IoT Analytics Service, Amazon ElasticSearch, Cloud9, AppMesh, Amazon Managed Blockchain, Glue, and AWS Transfer. The issue has been resolved and all DNS queries are being answered normally.</div>",route53resolver-us-east-1,2020-07-28 09:11:26,Route 53 Resolver,N. Virginia,1:22 AM,4:13 AM,PDT,171.0,2020,7,28,2020-07-28,single az intermittent dns lookup," 2:11 AM PDT -  We are investigating an increase in DNS lookup failures from EC2 instances in a single Availability Zone in the US-EAST-1 Region.
 2:57 AM PDT -  We are continuing to investigate increased DNS resolution errors from EC2 instances in a single Availability Zone of the US-EAST-1 Region. This could also impact functionality of other services that use EC2, such as App Mesh, Cloud9, ElasticSearch, EMR, Managed Blockchain, SageMaker, Transfer for SFTP, WorkMail, and Glue.
 3:44 AM PDT -  We are implementing a mitigation to the increased DNS resolution errors from EC2 instances in a single Availability Zone in the US-EAST-1 Region, and are starting to see recovery. This issue could also impact functionality of other services that use EC2, such as App Mesh, Cloud9, ElasticSearch, EMR, Managed Blockchain, SageMaker, Transfer for SFTP, WorkMail, RDS, and Glue.
 4:19 AM PDT -  DNS resolution failures in a single Availability Zone in the US-EAST-1 Region have largely been mitigated, and we are continuing to work towards full mitigation. 
 4:37 AM PDT -  Between 1:22 AM and 4:13 AM PDT, customers experienced an increase in DNS resolution errors from EC2 instances in a single Availability Zone of the US-EAST-1 Region. This could have also impacted functionality of other services that use EC2, such as RDS, SageMaker, EMR, WorkMail, MSK, AWS IoT Analytics Service, Amazon ElasticSearch, Cloud9, AppMesh, Amazon Managed Blockchain, Glue, and AWS Transfer. The issue has been resolved and all DNS queries are being answered normally."
289,Amazon Elastic Compute Cloud (N. Virginia),[RESOLVED]  Increased API Error Rates ,1596028900,1,,"<div><span class=""yellowfg""> 6:21 AM PDT</span>&nbsp;We have identified the cause of the increased API error rates in a single Availability Zone in the US-EAST-1 Region and continue working towards resolution. Customers experiencing errors launching new EC2 instances may attempt to launch their EC2 instances in another Availability Zone. Existing running instances are unaffected.</div><div><span class=""yellowfg""> 8:23 AM PDT</span>&nbsp;We want to provide more information on this issue and progress toward resolution. At 5:18 AM PDT, we began experiencing increased API errors that originated from one of our EC2 sub-systems that is responsible for managing EC2 instances. This resulted in increased error rates for some EC2 APIs and affected new instance launches in a Single Availability Zone. The root cause of the error rates affecting the sub-system has been identified and engineers are currently working on resolving the issue. Existing instances remain unaffected by this issue.</div><div><span class=""yellowfg"">10:19 AM PDT</span>&nbsp;We have deployed a fix to the impacted EC2 sub-system causing increased API error rates and new instance launch failures in a Single Availability zone in the US-EAST-1 Region and are beginning to see recovery. We continue to work towards full resolution. Existing instances remain unaffected by this issue.
</div><div><span class=""yellowfg"">10:52 AM PDT</span>&nbsp;Between 5:18 AM and 10:25 AM PDT we experienced increased error rates for some EC2 APIs and new instance launches in a Single Availability Zone in the US-EAST-1 region. Existing instances were unaffected. We are working to address API errors affecting a small number of EBS volumes as a result of this issue. The issue has been resolved and the service is operating normally.</div>",ec2-us-east-1,2020-07-29 13:21:40,Elastic Compute Cloud,N. Virginia,5:18 AM,10:25 AM,PDT,307.0,2020,7,29,2020-07-29,increased api error rates," 6:21 AM PDT -  We have identified the cause of the increased API error rates in a single Availability Zone in the US-EAST-1 Region and continue working towards resolution. Customers experiencing errors launching new EC2 instances may attempt to launch their EC2 instances in another Availability Zone. Existing running instances are unaffected.
 8:23 AM PDT -  We want to provide more information on this issue and progress toward resolution. At 5:18 AM PDT, we began experiencing increased API errors that originated from one of our EC2 sub-systems that is responsible for managing EC2 instances. This resulted in increased error rates for some EC2 APIs and affected new instance launches in a Single Availability Zone. The root cause of the error rates affecting the sub-system has been identified and engineers are currently working on resolving the issue. Existing instances remain unaffected by this issue.
10:19 AM PDT -  We have deployed a fix to the impacted EC2 sub-system causing increased API error rates and new instance launch failures in a Single Availability zone in the US-EAST-1 Region and are beginning to see recovery. We continue to work towards full resolution. Existing instances remain unaffected by this issue.
10:52 AM PDT -  Between 5:18 AM and 10:25 AM PDT we experienced increased error rates for some EC2 APIs and new instance launches in a Single Availability Zone in the US-EAST-1 region. Existing instances were unaffected. We are working to address API errors affecting a small number of EBS volumes as a result of this issue. The issue has been resolved and the service is operating normally."
290,Amazon Elastic Compute Cloud (N. California), [RESOLVED] Instance Connectivity and Instance Status Check Metrics,1597885789,1,,"<div><span class=""yellowfg""> 6:09 PM PDT</span>&nbsp;Between 4:38 PM and 5:35 PM PDT a small number of instances were unavailable due to power loss in a single Availability Zone (usw1-az1)  in the US-WEST-1 Region. Those customers received notification on their Personal Health Dashboard. During this time, some customers also experienced insufficient-data in the results from EC2 instance status checks for instances that were not impacted by the power event. During this time, CloudWatch alarms may have transitioned into ""INSUFFICIENT_DATA"" state if set on the delayed metrics. The issue has been resolved and the service is operating normally. </div>",ec2-us-west-1,2020-08-20 01:09:49,Elastic Compute Cloud,N. California,4:38 PM,5:35 PM,PDT,57.0,2020,8,20,2020-08-20,instance connectivity and instance status check metrics," 6:09 PM PDT -  Between 4:38 PM and 5:35 PM PDT a small number of instances were unavailable due to power loss in a single Availability Zone (usw1-az1)  in the US-WEST-1 Region. Those customers received notification on their Personal Health Dashboard. During this time, some customers also experienced insufficient-data in the results from EC2 instance status checks for instances that were not impacted by the power event. During this time, CloudWatch alarms may have transitioned into ""INSUFFICIENT_DATA"" state if set on the delayed metrics. The issue has been resolved and the service is operating normally. "
291,Amazon Elastic Compute Cloud (London),[RESOLVED] Instance Connectivity,1598350861,1,,"<div><span class=""yellowfg""> 3:21 AM PDT</span>&nbsp;We are investigating instance connectivity issues in a single Availability Zone (euw2-az2) in the EU-WEST-2 Region.</div><div><span class=""yellowfg""> 3:52 AM PDT</span>&nbsp;We are experiencing instance connectivity issues in a single Availability Zone (euw2-az2) in the EU-WEST-2 Region. As a result we are experiencing elevated API latencies for multiple APIs, as well as elevated API failure rates on the CreateSnapshot API only. We are also experiencing elevated instance launch failure rates in the affected Availability Zone.</div><div><span class=""yellowfg""> 4:46 AM PDT</span>&nbsp;We have restored connectivity to some of the instances in the affected Availability Zone (euw2-az2), and API latencies and launch failure rates are largely recovered. Access to some EBS volumes in the affected Availability Zone was also affected. We are working to restore connectivity to the remaining affected instances and volumes.</div><div><span class=""yellowfg""> 5:50 AM PDT</span>&nbsp;We have restored connectivity to a further subset of the instances and volumes in the affected Availability Zone (euw2-az2). We are continuing to work to restore connectivity to the remaining affected instances and volumes.</div><div><span class=""yellowfg""> 7:01 AM PDT</span>&nbsp;We wanted to provide some more information for the event affecting some instances in a single Availability Zone (euw2-az2) in the EU-WEST-2 Region. Starting at 2:05 AM PDT we experienced power and network connectivity issues for some instances, and degraded performance for some EBS volumes in the affected Availability Zone. As a result of this we also experienced elevated API latencies for some of our APIs, elevated API failure rates on the CreateSnapshot API, and elevated instance launch failure rates in the affected Availability Zone. By 4:50 AM PDT, power and networking connectivity had been restored to the majority of affected instances, and degraded performance for the majority of affected EBS volumes had been resolved. While we will continue to work to recover the remaining instances and volumes, for immediate recovery, we recommend replacing any remaining affected instances or volumes if possible.</div><div><span class=""yellowfg"">12:13 PM PDT</span>&nbsp;Starting at 2:05 AM PDT we experienced power and network connectivity issues for some instances, and degraded performance for some EBS volumes in the affected Availability Zone (euw2-az2) . By 4:50 AM PDT, power and networking connectivity had been restored to the majority of affected instances, and degraded performance for the majority of affected EBS volumes had been resolved. Since the beginning of the impact, we have been working to recover the remaining instances and volumes. A small number of remaining instances and volumes are hosted on hardware which was adversely affected by the loss of power. We continue to work to recover all affected instances and volumes and have opened notifications for the remaining impacted customers via the Personal Health Dashboard. For immediate recovery, we recommend replacing any remaining affected instances or volumes if possible.</div>",ec2-eu-west-2,2020-08-25 10:21:01,Elastic Compute Cloud,London,2:05 AM,4:50 AM,PDT,165.0,2020,8,25,2020-08-25,instance connectivity," 3:21 AM PDT -  We are investigating instance connectivity issues in a single Availability Zone (euw2-az2) in the EU-WEST-2 Region.
 3:52 AM PDT -  We are experiencing instance connectivity issues in a single Availability Zone (euw2-az2) in the EU-WEST-2 Region. As a result we are experiencing elevated API latencies for multiple APIs, as well as elevated API failure rates on the CreateSnapshot API only. We are also experiencing elevated instance launch failure rates in the affected Availability Zone.
 4:46 AM PDT -  We have restored connectivity to some of the instances in the affected Availability Zone (euw2-az2), and API latencies and launch failure rates are largely recovered. Access to some EBS volumes in the affected Availability Zone was also affected. We are working to restore connectivity to the remaining affected instances and volumes.
 5:50 AM PDT -  We have restored connectivity to a further subset of the instances and volumes in the affected Availability Zone (euw2-az2). We are continuing to work to restore connectivity to the remaining affected instances and volumes.
 7:01 AM PDT -  We wanted to provide some more information for the event affecting some instances in a single Availability Zone (euw2-az2) in the EU-WEST-2 Region. Starting at 2:05 AM PDT we experienced power and network connectivity issues for some instances, and degraded performance for some EBS volumes in the affected Availability Zone. As a result of this we also experienced elevated API latencies for some of our APIs, elevated API failure rates on the CreateSnapshot API, and elevated instance launch failure rates in the affected Availability Zone. By 4:50 AM PDT, power and networking connectivity had been restored to the majority of affected instances, and degraded performance for the majority of affected EBS volumes had been resolved. While we will continue to work to recover the remaining instances and volumes, for immediate recovery, we recommend replacing any remaining affected instances or volumes if possible.
12:13 PM PDT -  Starting at 2:05 AM PDT we experienced power and network connectivity issues for some instances, and degraded performance for some EBS volumes in the affected Availability Zone (euw2-az2) . By 4:50 AM PDT, power and networking connectivity had been restored to the majority of affected instances, and degraded performance for the majority of affected EBS volumes had been resolved. Since the beginning of the impact, we have been working to recover the remaining instances and volumes. A small number of remaining instances and volumes are hosted on hardware which was adversely affected by the loss of power. We continue to work to recover all affected instances and volumes and have opened notifications for the remaining impacted customers via the Personal Health Dashboard. For immediate recovery, we recommend replacing any remaining affected instances or volumes if possible."
292,Amazon Relational Database Service (London),[RESOLVED] Instance Impairments,1598351776,1,,"<div><span class=""yellowfg""> 3:36 AM PDT</span>&nbsp;We are investigating connectivity issues affecting some instances in a single Availability Zone (euw2-az2) in the EU-WEST-2 Region.</div><div><span class=""yellowfg""> 4:19 AM PDT</span>&nbsp;We can confirm connectivity issues affecting some instances in a single Availability Zone (euw2-az2) in the EU-WEST-2 Region.</div><div><span class=""yellowfg""> 5:24 AM PDT</span>&nbsp;We have restored connectivity to some of the instances in the affected Availability Zone (euw2-az2) in the EU-WEST-2 Region, and continue to work to restore connectivity to the remaining affected instances.</div><div><span class=""yellowfg""> 6:26 AM PDT</span>&nbsp;We have restored connectivity to a further subset of the instances in the affected Availability Zone (euw2-az2) in the EU-WEST-2 Region, and continue to work to restore connectivity to the remaining affected instances.</div><div><span class=""yellowfg""> 7:55 AM PDT</span>&nbsp;We wanted to provide additional information for the event affecting some RDS database instances in a single Availability Zone (euw2-az2) in the EU-WEST-2 Region. Starting at 2:05 AM PDT we experienced power and network connectivity issues for some database instances within the affected Availability Zone. The affected Multi-AZ databases failed over to another Availability Zone as expected, and only Single-AZ databases remained affected. By 5:30 AM PDT, power and networking connectivity had been restored to the majority of affected Single-AZ database instances. We are continuing to work to recover the remaining instances.</div><div><span class=""yellowfg"">12:29 PM PDT</span>&nbsp;We have recovered the vast majority of RDS instances affected by the power event within a single Availability Zone (euw2-az2) in the EU-WEST-2 Region. Instances in other Availability Zones remain unaffected by this event.</div><div><span class=""yellowfg"">12:45 PM PDT</span>&nbsp;Starting at 2:05 AM PDT we experienced power and network connectivity issues for some database instances within the affected Availability Zone. The affected Multi-AZ databases failed over to another Availability Zone as expected, and only Single-AZ databases remained affected. By 5:30 AM PDT, power and networking connectivity had been restored to the majority of affected Single-AZ database instances. Since the beginning of the impact, we have been working to recover the remaining database instances. A small number of remaining database instances are hosted on hardware which was adversely affected by the loss of power. For these database instances, we have provided additional recovery guidance via the Personal Health Dashboard. The issue has been resolved and the service is operating normally. </div>",rds-eu-west-2,2020-08-25 10:36:16,Relational Database Service,London,2:05 AM,5:30 AM,PDT,205.0,2020,8,25,2020-08-25,instance impairments," 3:36 AM PDT -  We are investigating connectivity issues affecting some instances in a single Availability Zone (euw2-az2) in the EU-WEST-2 Region.
 4:19 AM PDT -  We can confirm connectivity issues affecting some instances in a single Availability Zone (euw2-az2) in the EU-WEST-2 Region.
 5:24 AM PDT -  We have restored connectivity to some of the instances in the affected Availability Zone (euw2-az2) in the EU-WEST-2 Region, and continue to work to restore connectivity to the remaining affected instances.
 6:26 AM PDT -  We have restored connectivity to a further subset of the instances in the affected Availability Zone (euw2-az2) in the EU-WEST-2 Region, and continue to work to restore connectivity to the remaining affected instances.
 7:55 AM PDT -  We wanted to provide additional information for the event affecting some RDS database instances in a single Availability Zone (euw2-az2) in the EU-WEST-2 Region. Starting at 2:05 AM PDT we experienced power and network connectivity issues for some database instances within the affected Availability Zone. The affected Multi-AZ databases failed over to another Availability Zone as expected, and only Single-AZ databases remained affected. By 5:30 AM PDT, power and networking connectivity had been restored to the majority of affected Single-AZ database instances. We are continuing to work to recover the remaining instances.
12:29 PM PDT -  We have recovered the vast majority of RDS instances affected by the power event within a single Availability Zone (euw2-az2) in the EU-WEST-2 Region. Instances in other Availability Zones remain unaffected by this event.
12:45 PM PDT -  Starting at 2:05 AM PDT we experienced power and network connectivity issues for some database instances within the affected Availability Zone. The affected Multi-AZ databases failed over to another Availability Zone as expected, and only Single-AZ databases remained affected. By 5:30 AM PDT, power and networking connectivity had been restored to the majority of affected Single-AZ database instances. Since the beginning of the impact, we have been working to recover the remaining database instances. A small number of remaining database instances are hosted on hardware which was adversely affected by the loss of power. For these database instances, we have provided additional recovery guidance via the Personal Health Dashboard. The issue has been resolved and the service is operating normally. "
293,Amazon Elastic Load Balancing (Oregon),[RESOLVED] Increased Provisioning and Registration Times,1598395861,1,,"<div><span class=""yellowfg""> 3:51 PM PDT</span>&nbsp;Between 11:46 AM and 3:21 PM PDT, we experienced increased provisioning and registration times for load balancers in the US-WEST-2 Region. Connectivity to existing load balancers was not affected. The issue has been resolved and the service is operating normally.</div>",elb-us-west-2,2020-08-25 22:51:01,Elastic Load Balancing,Oregon,11:46 AM,3:21 PM,PDT,215.0,2020,8,25,2020-08-25,increased provisioning and registration times," 3:51 PM PDT -  Between 11:46 AM and 3:21 PM PDT, we experienced increased provisioning and registration times for load balancers in the US-WEST-2 Region. Connectivity to existing load balancers was not affected. The issue has been resolved and the service is operating normally."
294,AWS Batch (N. Virginia),[RESOLVED] Increased Job Processing Delays,1598918190,1,,"<div><span class=""yellowfg""> 4:56 PM PDT</span>&nbsp;We are investigating increased delay in job state transitions of AWS Batch Jobs in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 5:27 PM PDT</span>&nbsp;We can confirm intermittent increased delays in job state transitions of AWS Batch Jobs in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 5:58 PM PDT</span>&nbsp;We continue to investigate the root cause of intermittent increased delays in job state transitions of AWS Batch Jobs in the US-EAST-1 Region and work toward resolution.</div><div><span class=""yellowfg""> 6:41 PM PDT</span>&nbsp;We continue to investigate intermittent increased delays in job state transitions of AWS Batch Jobs in the US-EAST-1 Region and work toward resolution.</div><div><span class=""yellowfg""> 9:08 PM PDT</span>&nbsp;We recently experienced intermittent delayed job state transitions of AWS Batch Jobs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",batch-us-east-1,2020-08-31 23:56:30,Batch,N. Virginia,4:56 PM,9:08 PM,PDT,252.0,2020,8,31,2020-08-31,increased job processing delays," 4:56 PM PDT -  We are investigating increased delay in job state transitions of AWS Batch Jobs in the US-EAST-1 Region.
 5:27 PM PDT -  We can confirm intermittent increased delays in job state transitions of AWS Batch Jobs in the US-EAST-1 Region.
 5:58 PM PDT -  We continue to investigate the root cause of intermittent increased delays in job state transitions of AWS Batch Jobs in the US-EAST-1 Region and work toward resolution.
 6:41 PM PDT -  We continue to investigate intermittent increased delays in job state transitions of AWS Batch Jobs in the US-EAST-1 Region and work toward resolution.
 9:08 PM PDT -  We recently experienced intermittent delayed job state transitions of AWS Batch Jobs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
295,Amazon SageMaker (N. Virginia),[RESOLVED] Increased Error Rates and Latencies for Multiple API operations,1598920406,1,,"<div><span class=""yellowfg""> 5:33 PM PDT</span>&nbsp;We are investigating increased error rates and latencies for CreateTrainingJob, CreateHyperParameterTuningJob, and CreateEndpoint API operations in the US-EAST-1 Region. Previously created jobs and endpoints are unaffected.</div><div><span class=""yellowfg""> 6:04 PM PDT</span>&nbsp;We are continuing to investigate increased error rates and latencies for CreateTrainingJob, CreateHyperParameterTuningJob, and CreateEndpoint API operations in the US-EAST-1 Region. Previously created jobs and endpoints are unaffected.</div><div><span class=""yellowfg""> 6:47 PM PDT</span>&nbsp;We continue to investigate increased error rates and latencies for CreateTrainingJob, CreateHyperParameterTuningJob, and CreateEndpoint API operations in the US-EAST-1 Region and work towards resolution. Previously created jobs and endpoints are unaffected.</div><div><span class=""yellowfg""> 9:04 PM PDT</span>&nbsp;Between 1:04 PM PDT and 8:40 PM PDT we experienced increased error rates and latencies for CreateTrainingJob, CreateHyperParameterTuningJob, and CreateEndpoint API operations in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",sagemaker-us-east-1,2020-09-01 00:33:26,SageMaker,N. Virginia,1:04 PM,8:40 PM,PDT,456.0,2020,9,1,2020-09-01,increased error rates and latencies for multiple api operations," 5:33 PM PDT -  We are investigating increased error rates and latencies for CreateTrainingJob, CreateHyperParameterTuningJob, and CreateEndpoint API operations in the US-EAST-1 Region. Previously created jobs and endpoints are unaffected.
 6:04 PM PDT -  We are continuing to investigate increased error rates and latencies for CreateTrainingJob, CreateHyperParameterTuningJob, and CreateEndpoint API operations in the US-EAST-1 Region. Previously created jobs and endpoints are unaffected.
 6:47 PM PDT -  We continue to investigate increased error rates and latencies for CreateTrainingJob, CreateHyperParameterTuningJob, and CreateEndpoint API operations in the US-EAST-1 Region and work towards resolution. Previously created jobs and endpoints are unaffected.
 9:04 PM PDT -  Between 1:04 PM PDT and 8:40 PM PDT we experienced increased error rates and latencies for CreateTrainingJob, CreateHyperParameterTuningJob, and CreateEndpoint API operations in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. "
296,AWS DeepRacer (N. Virginia),[RESOLVED] Increased Error Rates for Training Jobs,1598923959,1,,"<div><span class=""yellowfg""> 6:32 PM PDT</span>&nbsp;We are investigating increased error rates for training jobs in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 7:06 PM PDT</span>&nbsp;We have confirmed intermittent issues with training jobs in the US-EAST-1 Region which result in them getting stuck in an “Initializing” state. We continue to work toward resolution.</div><div><span class=""yellowfg""> 9:10 PM PDT</span>&nbsp;Between 2:05 PM and 9:00 PM PDT we experienced increased error rates for training jobs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",deepracer-us-east-1,2020-09-01 01:32:39,DeepRacer,N. Virginia,2:05 PM,9:00 PM,PDT,415.0,2020,9,1,2020-09-01,increased error rates for training jobs," 6:32 PM PDT -  We are investigating increased error rates for training jobs in the US-EAST-1 Region.
 7:06 PM PDT -  We have confirmed intermittent issues with training jobs in the US-EAST-1 Region which result in them getting stuck in an “Initializing” state. We continue to work toward resolution.
 9:10 PM PDT -  Between 2:05 PM and 9:00 PM PDT we experienced increased error rates for training jobs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
297,Amazon CloudFront,[RESOLVED] Increased Propagation Time for Invalidations,1599068507,1,,"<div><span class=""yellowfg"">10:41 AM PDT</span>&nbsp;We are experiencing longer than usual propagation time for invalidations to a few edge locations in the CloudFront network. Changes for all other configurations are propagating normally. End-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=""yellowfg"">11:42 AM PDT</span>&nbsp;Between 9:30 AM and 11:38 AM PDT we experienced longer than usual propagation time for invalidations to a few edge locations in the CloudFront network. During this event changes for all other configurations propagated normally. Additionally, end-user requests for content from our edge locations were not affected by this issue and were being served normally. The issue is resolved and the service is operating normally.</div>",cloudfront,2020-09-02 17:41:47,CloudFront,Global,9:30 AM,11:38 AM,PDT,128.0,2020,9,2,2020-09-02,increased propagation time for invalidations,"10:41 AM PDT -  We are experiencing longer than usual propagation time for invalidations to a few edge locations in the CloudFront network. Changes for all other configurations are propagating normally. End-user requests for content from our edge locations are not affected by this issue and are being served normally.
11:42 AM PDT -  Between 9:30 AM and 11:38 AM PDT we experienced longer than usual propagation time for invalidations to a few edge locations in the CloudFront network. During this event changes for all other configurations propagated normally. Additionally, end-user requests for content from our edge locations were not affected by this issue and were being served normally. The issue is resolved and the service is operating normally."
298,AWS Identity and Access Management,[RESOLVED] Increased Error Rates and Latencies,1599168415,2,,"<div><span class=""yellowfg""> 2:26 PM PDT</span>&nbsp;We are investigating increased authentication error rates and latencies affecting IAM globally. Authenticated requests to other AWS services are also impacted. </div><div><span class=""yellowfg""> 2:35 PM PDT</span>&nbsp;We are seeing significant recovery for IAM authentication error rates and latencies and continue to work toward full resolution.</div><div><span class=""yellowfg""> 3:03 PM PDT</span>&nbsp;Between 2:02 PM and 2:23 PM PDT we experienced increased error rates and latencies for IAM operations in all AWS Regions. Attempts to create, describe, modify or delete IAM accounts and roles may have experienced a failure during this time. Authentication using IAM accounts and roles was not affected. The issue has been resolved and the service is operating normally.</div>",iam,2020-09-03 21:26:55,Identity and Access Management,Global,2:02 PM,2:23 PM,PDT,21.0,2020,9,3,2020-09-03,increased error rates and latencies," 2:26 PM PDT -  We are investigating increased authentication error rates and latencies affecting IAM globally. Authenticated requests to other AWS services are also impacted. 
 2:35 PM PDT -  We are seeing significant recovery for IAM authentication error rates and latencies and continue to work toward full resolution.
 3:03 PM PDT -  Between 2:02 PM and 2:23 PM PDT we experienced increased error rates and latencies for IAM operations in all AWS Regions. Attempts to create, describe, modify or delete IAM accounts and roles may have experienced a failure during this time. Authentication using IAM accounts and roles was not affected. The issue has been resolved and the service is operating normally."
299,AWS Storage Gateway (N. California),[RESOLVED] Storage Gateway VMs Offline,1600026383,1,,"<div><span class=""yellowfg"">12:46 PM PDT</span>&nbsp;Beginning at 7:39 AM PDT, we are experiencing an issue where Storage Gateway VMs appear to be offline and are not able to perform out of cache reads or upload to our service. The gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. We have identified the root cause and are working towards resolution.</div><div><span class=""yellowfg""> 2:58 PM PDT</span>&nbsp;We continue to work towards resolution on the issue impacting Storage Gateway. The Gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. Based upon the information available at this time, full resolution is estimated to take 2 hours. We are working through the recovery process now and will continue to keep you updated if this timeline changes.</div><div><span class=""yellowfg""> 5:32 PM PDT</span>&nbsp;We are beginning to see recovery in some AWS Regions for the issue impacting Storage Gateway. We continue to work towards resolution for all impacted Regions. We will update the message for each AWS Region as recovery occurs.</div><div><span class=""yellowfg""> 6:55 PM PDT</span>&nbsp;Beginning at 7:39 AM PDT we experienced an issue impacting Storage Gateway in the US-WEST-1 Region. During the event Gateways were unable to perform out of cache reads and appeared offline in the SGW console. Gateways continued to accept writes but were unable to upload them to the AWS service. The issue has been resolved and the service is operating normally.</div>",storagegateway-us-west-1,2020-09-13 19:46:23,Storage Gateway,N. California,7:39 AM,6:55 PM,PDT,676.0,2020,9,13,2020-09-13,storage gateway vms offline,"12:46 PM PDT -  Beginning at 7:39 AM PDT, we are experiencing an issue where Storage Gateway VMs appear to be offline and are not able to perform out of cache reads or upload to our service. The gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. We have identified the root cause and are working towards resolution.
 2:58 PM PDT -  We continue to work towards resolution on the issue impacting Storage Gateway. The Gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. Based upon the information available at this time, full resolution is estimated to take 2 hours. We are working through the recovery process now and will continue to keep you updated if this timeline changes.
 5:32 PM PDT -  We are beginning to see recovery in some AWS Regions for the issue impacting Storage Gateway. We continue to work towards resolution for all impacted Regions. We will update the message for each AWS Region as recovery occurs.
 6:55 PM PDT -  Beginning at 7:39 AM PDT we experienced an issue impacting Storage Gateway in the US-WEST-1 Region. During the event Gateways were unable to perform out of cache reads and appeared offline in the SGW console. Gateways continued to accept writes but were unable to upload them to the AWS service. The issue has been resolved and the service is operating normally."
300,AWS Storage Gateway (Frankfurt),[RESOLVED] Storage Gateway VMs Offline,1600026390,1,,"<div><span class=""yellowfg"">12:46 PM PDT</span>&nbsp;Beginning at 7:39 AM PDT, we are experiencing an issue where Storage Gateway VMs appear to be offline and are not able to perform out of cache reads or upload to our service. The gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. We have identified the root cause and are working towards resolution.</div><div><span class=""yellowfg""> 2:58 PM PDT</span>&nbsp;We continue to work towards resolution on the issue impacting Storage Gateway. The Gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. Based upon the information available at this time, full resolution is estimated to take 2 hours. We are working through the recovery process now and will continue to keep you updated if this timeline changes.</div><div><span class=""yellowfg""> 5:33 PM PDT</span>&nbsp;We are beginning to see recovery in some AWS Regions for the issue impacting Storage Gateway. We continue to work towards resolution for all impacted Regions. We will update the message for each AWS Region as recovery occurs.</div><div><span class=""yellowfg""> 7:05 PM PDT</span>&nbsp;Beginning at 7:39 AM PDT we experienced an issue impacting Storage Gateway in the EU-CENTRAL-1 Region. During the event Gateways were unable to perform out of cache reads and appeared offline in the SGW console. Gateways continued to accept writes but were unable to upload them to the AWS service. The issue has been resolved and the service is operating normally.</div>",storagegateway-eu-central-1,2020-09-13 19:46:30,Storage Gateway,Frankfurt,7:39 AM,7:05 PM,PDT,686.0,2020,9,13,2020-09-13,storage gateway vms offline,"12:46 PM PDT -  Beginning at 7:39 AM PDT, we are experiencing an issue where Storage Gateway VMs appear to be offline and are not able to perform out of cache reads or upload to our service. The gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. We have identified the root cause and are working towards resolution.
 2:58 PM PDT -  We continue to work towards resolution on the issue impacting Storage Gateway. The Gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. Based upon the information available at this time, full resolution is estimated to take 2 hours. We are working through the recovery process now and will continue to keep you updated if this timeline changes.
 5:33 PM PDT -  We are beginning to see recovery in some AWS Regions for the issue impacting Storage Gateway. We continue to work towards resolution for all impacted Regions. We will update the message for each AWS Region as recovery occurs.
 7:05 PM PDT -  Beginning at 7:39 AM PDT we experienced an issue impacting Storage Gateway in the EU-CENTRAL-1 Region. During the event Gateways were unable to perform out of cache reads and appeared offline in the SGW console. Gateways continued to accept writes but were unable to upload them to the AWS service. The issue has been resolved and the service is operating normally."
301,AWS Storage Gateway (Sao Paulo),[RESOLVED] Storage Gateway VMs Offline,1600026396,1,,"<div><span class=""yellowfg"">12:46 PM PDT</span>&nbsp;Beginning at 7:39 AM PDT, we are experiencing an issue where Storage Gateway VMs appear to be offline and are not able to perform out of cache reads or upload to our service. The gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. We have identified the root cause and are working towards resolution.</div><div><span class=""yellowfg""> 2:58 PM PDT</span>&nbsp;We continue to work towards resolution on the issue impacting Storage Gateway. The Gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. Based upon the information available at this time, full resolution is estimated to take 2 hours. We are working through the recovery process now and will continue to keep you updated if this timeline changes.</div><div><span class=""yellowfg""> 5:34 PM PDT</span>&nbsp;We are beginning to see recovery in some AWS Regions for the issue impacting Storage Gateway. We continue to work towards resolution for all impacted Regions. We will update the message for each AWS Region as recovery occurs.</div><div><span class=""yellowfg""> 7:06 PM PDT</span>&nbsp;Beginning at 7:39 AM PDT we experienced an issue impacting Storage Gateway in the SA-EAST-1 Region. During the event Gateways were unable to perform out of cache reads and appeared offline in the SGW console. Gateways continued to accept writes but were unable to upload them to the AWS service. The issue has been resolved and the service is operating normally.</div>",storagegateway-sa-east-1,2020-09-13 19:46:36,Storage Gateway,Sao Paulo,7:39 AM,7:06 PM,PDT,687.0,2020,9,13,2020-09-13,storage gateway vms offline,"12:46 PM PDT -  Beginning at 7:39 AM PDT, we are experiencing an issue where Storage Gateway VMs appear to be offline and are not able to perform out of cache reads or upload to our service. The gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. We have identified the root cause and are working towards resolution.
 2:58 PM PDT -  We continue to work towards resolution on the issue impacting Storage Gateway. The Gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. Based upon the information available at this time, full resolution is estimated to take 2 hours. We are working through the recovery process now and will continue to keep you updated if this timeline changes.
 5:34 PM PDT -  We are beginning to see recovery in some AWS Regions for the issue impacting Storage Gateway. We continue to work towards resolution for all impacted Regions. We will update the message for each AWS Region as recovery occurs.
 7:06 PM PDT -  Beginning at 7:39 AM PDT we experienced an issue impacting Storage Gateway in the SA-EAST-1 Region. During the event Gateways were unable to perform out of cache reads and appeared offline in the SGW console. Gateways continued to accept writes but were unable to upload them to the AWS service. The issue has been resolved and the service is operating normally."
302,AWS Storage Gateway (Singapore),[RESOLVED] Storage Gateway VMs Offline,1600026398,1,,"<div><span class=""yellowfg"">12:46 PM PDT</span>&nbsp;Beginning at 7:39 AM PDT, we are experiencing an issue where Storage Gateway VMs appear to be offline and are not able to perform out of cache reads or upload to our service. The gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. We have identified the root cause and are working towards resolution.</div><div><span class=""yellowfg""> 2:59 PM PDT</span>&nbsp;We continue to work towards resolution on the issue impacting Storage Gateway. The Gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. Based upon the information available at this time, full resolution is estimated to take 2 hours. We are working through the recovery process now and will continue to keep you updated if this timeline changes.</div><div><span class=""yellowfg""> 5:39 PM PDT</span>&nbsp;We are beginning to see recovery in some AWS Regions for the issue impacting Storage Gateway. We continue to work towards resolution for all impacted Regions. We will update the message for each AWS Region as recovery occurs.</div><div><span class=""yellowfg""> 5:40 PM PDT</span>&nbsp;</div><div><span class=""yellowfg""> 6:53 PM PDT</span>&nbsp;Beginning at 7:39 AM PDT we experienced an issue impacting Storage Gateway in the AP-SOUTHEAST-1 Region. During the event Gateways were unable to perform out of cache reads and appeared offline in the SGW console. Gateways continued to accept writes but were unable to upload them to the AWS service. The issue has been resolved and the service is operating normally. </div>",storagegateway-ap-southeast-1,2020-09-13 19:46:38,Storage Gateway,Singapore,7:39 AM,6:53 PM,PDT,674.0,2020,9,13,2020-09-13,storage gateway vms offline,"12:46 PM PDT -  Beginning at 7:39 AM PDT, we are experiencing an issue where Storage Gateway VMs appear to be offline and are not able to perform out of cache reads or upload to our service. The gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. We have identified the root cause and are working towards resolution.
 2:59 PM PDT -  We continue to work towards resolution on the issue impacting Storage Gateway. The Gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. Based upon the information available at this time, full resolution is estimated to take 2 hours. We are working through the recovery process now and will continue to keep you updated if this timeline changes.
 5:39 PM PDT -  We are beginning to see recovery in some AWS Regions for the issue impacting Storage Gateway. We continue to work towards resolution for all impacted Regions. We will update the message for each AWS Region as recovery occurs.
 5:40 PM PDT -  
 6:53 PM PDT -  Beginning at 7:39 AM PDT we experienced an issue impacting Storage Gateway in the AP-SOUTHEAST-1 Region. During the event Gateways were unable to perform out of cache reads and appeared offline in the SGW console. Gateways continued to accept writes but were unable to upload them to the AWS service. The issue has been resolved and the service is operating normally. "
303,AWS Storage Gateway (Ireland),[RESOLVED] Storage Gateway VMs Offline,1600026405,1,,"<div><span class=""yellowfg"">12:46 PM PDT</span>&nbsp;Beginning at 7:39 AM PDT, we are experiencing an issue where Storage Gateway VMs appear to be offline and are not able to perform out of cache reads or upload to our service. The gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. We have identified the root cause and are working towards resolution.</div><div><span class=""yellowfg""> 2:59 PM PDT</span>&nbsp;We continue to work towards resolution on the issue impacting Storage Gateway. The Gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. Based upon the information available at this time, full resolution is estimated to take 2 hours. We are working through the recovery process now and will continue to keep you updated if this timeline changes.</div><div><span class=""yellowfg""> 5:35 PM PDT</span>&nbsp;We are beginning to see recovery in some AWS Regions for the issue impacting Storage Gateway. We continue to work towards resolution for all impacted Regions. We will update the message for each AWS Region as recovery occurs.</div><div><span class=""yellowfg""> 6:54 PM PDT</span>&nbsp;Beginning at 7:39 AM PDT we experienced an issue impacting Storage Gateway in the EU-WEST-1 Region. During the event Gateways were unable to perform out of cache reads and appeared offline in the SGW console. Gateways continued to accept writes but were unable to upload them to the AWS service. The issue has been resolved and the service is operating normally. </div>",storagegateway-eu-west-1,2020-09-13 19:46:45,Storage Gateway,Ireland,7:39 AM,6:54 PM,PDT,675.0,2020,9,13,2020-09-13,storage gateway vms offline,"12:46 PM PDT -  Beginning at 7:39 AM PDT, we are experiencing an issue where Storage Gateway VMs appear to be offline and are not able to perform out of cache reads or upload to our service. The gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. We have identified the root cause and are working towards resolution.
 2:59 PM PDT -  We continue to work towards resolution on the issue impacting Storage Gateway. The Gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. Based upon the information available at this time, full resolution is estimated to take 2 hours. We are working through the recovery process now and will continue to keep you updated if this timeline changes.
 5:35 PM PDT -  We are beginning to see recovery in some AWS Regions for the issue impacting Storage Gateway. We continue to work towards resolution for all impacted Regions. We will update the message for each AWS Region as recovery occurs.
 6:54 PM PDT -  Beginning at 7:39 AM PDT we experienced an issue impacting Storage Gateway in the EU-WEST-1 Region. During the event Gateways were unable to perform out of cache reads and appeared offline in the SGW console. Gateways continued to accept writes but were unable to upload them to the AWS service. The issue has been resolved and the service is operating normally. "
304,AWS Storage Gateway (N. Virginia),[RESOLVED] Storage Gateway VMs Offline,1600026407,1,,"<div><span class=""yellowfg"">12:46 PM PDT</span>&nbsp;Beginning at 7:39 AM PDT, we are experiencing an issue where Storage Gateway VMs appear to be offline and are not able to perform out of cache reads or upload to our service. The gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. We have identified the root cause and are working towards resolution.</div><div><span class=""yellowfg""> 2:59 PM PDT</span>&nbsp;We continue to work towards resolution on the issue impacting Storage Gateway. The Gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. Based upon the information available at this time, full resolution is estimated to take 2 hours. We are working through the recovery process now and will continue to keep you updated if this timeline changes.</div><div><span class=""yellowfg""> 5:39 PM PDT</span>&nbsp;We are beginning to see recovery in some AWS Regions for the issue impacting Storage Gateway. We continue to work towards resolution for all impacted Regions. We will update the message for each AWS Region as recovery occurs.</div><div><span class=""yellowfg""> 6:32 PM PDT</span>&nbsp;Beginning at 7:39 AM PDT we experienced an issue impacting Storage Gateway in the US-EAST-1 Region. During the event Gateways were unable to perform out of cache reads and appeared offline in the SGW console. Gateways continued to accept writes but were unable to upload them to the AWS service. The issue has been resolved and the service is operating normally. </div>",storagegateway-us-east-1,2020-09-13 19:46:47,Storage Gateway,N. Virginia,7:39 AM,6:32 PM,PDT,653.0,2020,9,13,2020-09-13,storage gateway vms offline,"12:46 PM PDT -  Beginning at 7:39 AM PDT, we are experiencing an issue where Storage Gateway VMs appear to be offline and are not able to perform out of cache reads or upload to our service. The gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. We have identified the root cause and are working towards resolution.
 2:59 PM PDT -  We continue to work towards resolution on the issue impacting Storage Gateway. The Gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. Based upon the information available at this time, full resolution is estimated to take 2 hours. We are working through the recovery process now and will continue to keep you updated if this timeline changes.
 5:39 PM PDT -  We are beginning to see recovery in some AWS Regions for the issue impacting Storage Gateway. We continue to work towards resolution for all impacted Regions. We will update the message for each AWS Region as recovery occurs.
 6:32 PM PDT -  Beginning at 7:39 AM PDT we experienced an issue impacting Storage Gateway in the US-EAST-1 Region. During the event Gateways were unable to perform out of cache reads and appeared offline in the SGW console. Gateways continued to accept writes but were unable to upload them to the AWS service. The issue has been resolved and the service is operating normally. "
305,AWS Storage Gateway (Oregon),[RESOLVED] Storage Gateway VMs Offline,1600026408,1,,"<div><span class=""yellowfg"">12:46 PM PDT</span>&nbsp;Beginning at 7:39 AM PDT, we are experiencing an issue where Storage Gateway VMs appear to be offline and are not able to perform out of cache reads or upload to our service. The gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. We have identified the root cause and are working towards resolution.</div><div><span class=""yellowfg""> 2:59 PM PDT</span>&nbsp;We continue to work towards resolution on the issue impacting Storage Gateway. The Gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. Based upon the information available at this time, full resolution is estimated to take 2 hours. We are working through the recovery process now and will continue to keep you updated if this timeline changes.</div><div><span class=""yellowfg""> 5:29 PM PDT</span>&nbsp;We are beginning to see recovery in some AWS Regions for the issue impacting Storage Gateway. We continue to work towards resolution for all impacted Regions. We will update the message for each AWS Region as recovery occurs.</div><div><span class=""yellowfg""> 6:30 PM PDT</span>&nbsp;Beginning at 7:39 AM PDT we experienced an issue impacting Storage Gateway in the US-WEST-2 Region. During the event Gateways were unable to perform out of cache reads and appeared offline in the SGW console. Gateways continued to accept writes but were unable to upload them to the AWS service. The issue has been resolved and the service is operating normally. </div>",storagegateway-us-west-2,2020-09-13 19:46:48,Storage Gateway,Oregon,7:39 AM,6:30 PM,PDT,651.0,2020,9,13,2020-09-13,storage gateway vms offline,"12:46 PM PDT -  Beginning at 7:39 AM PDT, we are experiencing an issue where Storage Gateway VMs appear to be offline and are not able to perform out of cache reads or upload to our service. The gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. We have identified the root cause and are working towards resolution.
 2:59 PM PDT -  We continue to work towards resolution on the issue impacting Storage Gateway. The Gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. Based upon the information available at this time, full resolution is estimated to take 2 hours. We are working through the recovery process now and will continue to keep you updated if this timeline changes.
 5:29 PM PDT -  We are beginning to see recovery in some AWS Regions for the issue impacting Storage Gateway. We continue to work towards resolution for all impacted Regions. We will update the message for each AWS Region as recovery occurs.
 6:30 PM PDT -  Beginning at 7:39 AM PDT we experienced an issue impacting Storage Gateway in the US-WEST-2 Region. During the event Gateways were unable to perform out of cache reads and appeared offline in the SGW console. Gateways continued to accept writes but were unable to upload them to the AWS service. The issue has been resolved and the service is operating normally. "
306,AWS Storage Gateway (Sydney),[RESOLVED] Storage Gateway VMs Offline,1600026439,1,,"<div><span class=""yellowfg"">12:47 PM PDT</span>&nbsp;Beginning at 7:39 AM PDT, we are experiencing an issue where Storage Gateway VMs appear to be offline and are not able to perform out of cache reads or upload to our service. The gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. We have identified the root cause and are working towards resolution.</div><div><span class=""yellowfg""> 2:59 PM PDT</span>&nbsp;We continue to work towards resolution on the issue impacting Storage Gateway. The Gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. Based upon the information available at this time, full resolution is estimated to take 2 hours. We are working through the recovery process now and will continue to keep you updated if this timeline changes.</div><div><span class=""yellowfg""> 5:30 PM PDT</span>&nbsp;We are beginning to see recovery in some AWS Regions for the issue impacting Storage Gateway. We continue to work towards resolution for all impacted Regions. We will update the message for each AWS Region as recovery occurs.</div><div><span class=""yellowfg""> 6:31 PM PDT</span>&nbsp;Beginning at 7:39 AM PDT we experienced an issue impacting Storage Gateway in the AP-SOUTHEAST-2 Region. During the event Gateways were unable to perform out of cache reads and appeared offline in the SGW console. Gateways continued to accept writes but were unable to upload them to the AWS service. The issue has been resolved and the service is operating normally. </div>",storagegateway-ap-southeast-2,2020-09-13 19:47:19,Storage Gateway,Sydney,7:39 AM,6:31 PM,PDT,652.0,2020,9,13,2020-09-13,storage gateway vms offline,"12:47 PM PDT -  Beginning at 7:39 AM PDT, we are experiencing an issue where Storage Gateway VMs appear to be offline and are not able to perform out of cache reads or upload to our service. The gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. We have identified the root cause and are working towards resolution.
 2:59 PM PDT -  We continue to work towards resolution on the issue impacting Storage Gateway. The Gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. Based upon the information available at this time, full resolution is estimated to take 2 hours. We are working through the recovery process now and will continue to keep you updated if this timeline changes.
 5:30 PM PDT -  We are beginning to see recovery in some AWS Regions for the issue impacting Storage Gateway. We continue to work towards resolution for all impacted Regions. We will update the message for each AWS Region as recovery occurs.
 6:31 PM PDT -  Beginning at 7:39 AM PDT we experienced an issue impacting Storage Gateway in the AP-SOUTHEAST-2 Region. During the event Gateways were unable to perform out of cache reads and appeared offline in the SGW console. Gateways continued to accept writes but were unable to upload them to the AWS service. The issue has been resolved and the service is operating normally. "
307,AWS Storage Gateway (Tokyo),[RESOLVED] Storage Gateway VM オフライン| Storage Gateway VMs Offline,1600026628,1,,"<div><span class=""yellowfg"">12:50 PM PDT</span>&nbsp;9/13 23:39 (JST) から、Storage Gateway VM がオフラインとして表示され、キャッシュ読み取りや、サービスへのアップロードを実行できないという問題が発生しています。ゲートウェイは引き続き書き込みを受け付けますが、問題が解消するまで、サービスへのアップロードを続行できません。根本原因は特定され、現在、解決に向けて取り組んでいます。
--
Beginning at 7:39 AM PDT, we are experiencing an issue where Storage Gateway VMs appear to be offline and are not able to perform out of cache reads or upload to our service. The gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. We have identified the root cause and are working towards resolution.</div><div><span class=""yellowfg""> 2:59 PM PDT</span>&nbsp;Storage Gateway に影響する問題について、引き続き解決に向けて取り組んでいます。Gateway は引き続き書き込みを受け付けますが、この問題が解決するまでサービスへのアップロードを行うことができません。現時点で入手可能な情報に基づいて、完全な問題の解決には 2 時間かかると推定されます。現在、復旧プロセスを行っており、このタイムラインが変更された場合でも、引き続き最新の状況ををアップデートし続けます。
--
We continue to work towards resolution on the issue impacting Storage Gateway. The Gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. Based upon the information available at this time, full resolution is estimated to take 2 hours. We are working through the recovery process now and will continue to keep you updated if this timeline changes.</div><div><span class=""yellowfg""> 5:36 PM PDT</span>&nbsp;Storage Gateway に影響する問題について、一部の AWS リージョンで復旧を開始しています。AWS では影響を受けているすべての AWS リージョンの復旧に向けて引き続き取り組んでいます。復旧した場合、各 AWS リージョンのメッセージを更新します。-- We are beginning to see recovery in some AWS Regions for the issue impacting Storage Gateway. We continue to work towards resolution for all impacted Regions. We will update the message for each AWS Region as recovery occurs.</div><div><span class=""yellowfg""> 6:36 PM PDT</span>&nbsp;日本時間 9/13 23:39 から、AP-NORTHEAST-1 リージョンの Storage Gateway に影響する問題が発生しました。イベント中、ゲートウェイはキャッシュからの読み取りを実行できず、SGW コンソールにオフラインとして表示されました。ゲートウェイは引き続き書き込みを受け付けましたが、AWS サービスにアップロードができませんでした。現在、問題は解決され、サービスは正常に動作しています。-- Beginning at 7:39 AM PDT we experienced an issue impacting Storage Gateway in the AP-NORTHEAST-1 Region. During the event Gateways were unable to perform out of cache reads and appeared offline in the SGW console. Gateways continued to accept writes but were unable to upload them to the AWS service. The issue has been resolved and the service is operating normally.</div>",storagegateway-ap-northeast-1,2020-09-13 19:50:28,Storage Gateway,Tokyo,7:39 AM,6:36 PM,PDT,657.0,2020,9,13,2020-09-13,storage gateway vm storage gateway vms offline,"12:50 PM PDT -  9/13 23:39 (JST) から、Storage Gateway VM がオフラインとして表示され、キャッシュ読み取りや、サービスへのアップロードを実行できないという問題が発生しています。ゲートウェイは引き続き書き込みを受け付けますが、問題が解消するまで、サービスへのアップロードを続行できません。根本原因は特定され、現在、解決に向けて取り組んでいます。--Beginning at 7:39 AM PDT, we are experiencing an issue where Storage Gateway VMs appear to be offline and are not able to perform out of cache reads or upload to our service. The gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. We have identified the root cause and are working towards resolution.
 2:59 PM PDT -  Storage Gateway に影響する問題について、引き続き解決に向けて取り組んでいます。Gateway は引き続き書き込みを受け付けますが、この問題が解決するまでサービスへのアップロードを行うことができません。現時点で入手可能な情報に基づいて、完全な問題の解決には 2 時間かかると推定されます。現在、復旧プロセスを行っており、このタイムラインが変更された場合でも、引き続き最新の状況ををアップデートし続けます。--We continue to work towards resolution on the issue impacting Storage Gateway. The Gateways will continue to accept writes but will not be able to proceed to upload them to our service until this issue is resolved. Based upon the information available at this time, full resolution is estimated to take 2 hours. We are working through the recovery process now and will continue to keep you updated if this timeline changes.
 5:36 PM PDT -  Storage Gateway に影響する問題について、一部の AWS リージョンで復旧を開始しています。AWS では影響を受けているすべての AWS リージョンの復旧に向けて引き続き取り組んでいます。復旧した場合、各 AWS リージョンのメッセージを更新します。-- We are beginning to see recovery in some AWS Regions for the issue impacting Storage Gateway. We continue to work towards resolution for all impacted Regions. We will update the message for each AWS Region as recovery occurs.
 6:36 PM PDT -  日本時間 9/13 23:39 から、AP-NORTHEAST-1 リージョンの Storage Gateway に影響する問題が発生しました。イベント中、ゲートウェイはキャッシュからの読み取りを実行できず、SGW コンソールにオフラインとして表示されました。ゲートウェイは引き続き書き込みを受け付けましたが、AWS サービスにアップロードができませんでした。現在、問題は解決され、サービスは正常に動作しています。-- Beginning at 7:39 AM PDT we experienced an issue impacting Storage Gateway in the AP-NORTHEAST-1 Region. During the event Gateways were unable to perform out of cache reads and appeared offline in the SGW console. Gateways continued to accept writes but were unable to upload them to the AWS service. The issue has been resolved and the service is operating normally."
308,Amazon Elastic Compute Cloud (N. Virginia),[RESOLVED] Increased API Error Rates,1600266641,1,,"<div><span class=""yellowfg""> 7:30 AM PDT</span>&nbsp;We are investigating increased API error rates in the US-EAST-1 Region.

</div><div><span class=""yellowfg""> 7:45 AM PDT</span>&nbsp;Between 6:55 AM and 7:29 AM PDT we experienced increased API error rates and latencies in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",ec2-us-east-1,2020-09-16 14:30:41,Elastic Compute Cloud,N. Virginia,6:55 AM,7:29 AM,PDT,34.0,2020,9,16,2020-09-16,increased api error rates," 7:30 AM PDT -  We are investigating increased API error rates in the US-EAST-1 Region.
 7:45 AM PDT -  Between 6:55 AM and 7:29 AM PDT we experienced increased API error rates and latencies in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
309,AWS Identity and Access Management,[RESOLVED] Increased Error Rates &amp; Latencies,1600294646,2,,"<div><span class=""yellowfg""> 3:17 PM PDT</span>&nbsp;We are investigating increased authentication error rates and latencies affecting IAM. IAM related requests to other AWS services may also be impacted.</div><div><span class=""yellowfg""> 3:22 PM PDT</span>&nbsp;We can confirm error rates and latencies affecting IAM. IAM related requests to other AWS Services may also be impacted. </div><div><span class=""yellowfg""> 3:41 PM PDT</span>&nbsp;We can confirm that we are experiencing increased error rates and latencies for IAM operations in all AWS Regions. Attempts to create, describe, modify or delete IAM accounts and roles may be experiencing elevated failures. Authentication using IAM accounts and roles are not affected. We have identified the root cause and are working toward resolution.</div><div><span class=""yellowfg""> 4:12 PM PDT</span>&nbsp;We continue to work towards resolution of the increased error rates and latencies for IAM operations in all AWS Regions. The issue continues to affect create, describe, modify or delete of IAM accounts and roles. Other AWS Services that perform these IAM mutating operations may be impacted during this issue. Authentication using IAM accounts and roles are not affected.</div><div><span class=""yellowfg""> 4:20 PM PDT</span>&nbsp;We are seeing recovery for the increased error rates and latencies affecting IAM operations in all AWS Regions. We continue to work toward full resolution.</div><div><span class=""yellowfg""> 4:30 PM PDT</span>&nbsp;We continue to observe significant recovery for the increased error rates and latencies affecting IAM operations in all AWS Regions. As we work toward full recovery, mutating IAM operations may take longer than normal to propagate. We continue to work toward full resolution.</div><div><span class=""yellowfg""> 5:05 PM PDT</span>&nbsp;We continue to work toward full resolution. We are currently investigating increased latency which may result in intermittent timeouts for Read/Describe IAM Operations for some customers.</div><div><span class=""yellowfg""> 5:42 PM PDT</span>&nbsp;Between 2:48 PM and 5:28 PM PDT we experienced periods of increased error rates and latencies for IAM operations impacting all AWS Regions. This issue affected mutating operations of IAM Users and Roles as well as Assume Role. Other AWS Services that performed these IAM operations were also impacted. Authentication using existing IAM users and roles was not affected. The majority of the impact was mitigated at 4:12 PM, and the issue was fully resolved at 5:28 PM. The issue has been resolved and the service is operating normally.</div>",iam,2020-09-16 22:17:26,Identity and Access Management,Global,2:48 PM,5:28 PM,PDT,160.0,2020,9,16,2020-09-16,increased error rates &amp; latencies," 3:17 PM PDT -  We are investigating increased authentication error rates and latencies affecting IAM. IAM related requests to other AWS services may also be impacted.
 3:22 PM PDT -  We can confirm error rates and latencies affecting IAM. IAM related requests to other AWS Services may also be impacted. 
 3:41 PM PDT -  We can confirm that we are experiencing increased error rates and latencies for IAM operations in all AWS Regions. Attempts to create, describe, modify or delete IAM accounts and roles may be experiencing elevated failures. Authentication using IAM accounts and roles are not affected. We have identified the root cause and are working toward resolution.
 4:12 PM PDT -  We continue to work towards resolution of the increased error rates and latencies for IAM operations in all AWS Regions. The issue continues to affect create, describe, modify or delete of IAM accounts and roles. Other AWS Services that perform these IAM mutating operations may be impacted during this issue. Authentication using IAM accounts and roles are not affected.
 4:20 PM PDT -  We are seeing recovery for the increased error rates and latencies affecting IAM operations in all AWS Regions. We continue to work toward full resolution.
 4:30 PM PDT -  We continue to observe significant recovery for the increased error rates and latencies affecting IAM operations in all AWS Regions. As we work toward full recovery, mutating IAM operations may take longer than normal to propagate. We continue to work toward full resolution.
 5:05 PM PDT -  We continue to work toward full resolution. We are currently investigating increased latency which may result in intermittent timeouts for Read/Describe IAM Operations for some customers.
 5:42 PM PDT -  Between 2:48 PM and 5:28 PM PDT we experienced periods of increased error rates and latencies for IAM operations impacting all AWS Regions. This issue affected mutating operations of IAM Users and Roles as well as Assume Role. Other AWS Services that performed these IAM operations were also impacted. Authentication using existing IAM users and roles was not affected. The majority of the impact was mitigated at 4:12 PM, and the issue was fully resolved at 5:28 PM. The issue has been resolved and the service is operating normally."
310,Amazon Kinesis Data Streams (Oregon),[RESOLVED] Increased API Error Rates,1600360315,1,,"<div><span class=""yellowfg""> 9:31 AM PDT</span>&nbsp;We are investigating increased API error rates for the SubscribeToShard API in the US-WEST-2 Region.</div><div><span class=""yellowfg""> 9:55 AM PDT</span>&nbsp;We have identified the root cause of the issue causing increased API error rates for the SubscribeToShard API in the US-WEST-2 Region and we are currently mitigating the impact.</div><div><span class=""yellowfg"">10:07 AM PDT</span>&nbsp;Between 06:20 AM and 09:56 AM PDT we experienced an increased error rate for the SubscribeToShard API, affecting some customers in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",kinesis-us-west-2,2020-09-17 16:31:55,Kinesis Data Streams,Oregon,06:20 AM,09:56 AM,PDT,216.0,2020,9,17,2020-09-17,increased api error rates," 9:31 AM PDT -  We are investigating increased API error rates for the SubscribeToShard API in the US-WEST-2 Region.
 9:55 AM PDT -  We have identified the root cause of the issue causing increased API error rates for the SubscribeToShard API in the US-WEST-2 Region and we are currently mitigating the impact.
10:07 AM PDT -  Between 06:20 AM and 09:56 AM PDT we experienced an increased error rate for the SubscribeToShard API, affecting some customers in the US-WEST-2 Region. The issue has been resolved and the service is operating normally."
311,Amazon Elastic Compute Cloud (Sao Paulo),[RESOLVED] Network connectivity,1600380261,1,,"<div><span class=""yellowfg""> 3:04 PM PDT</span>&nbsp;Between 1:37 PM and 2:35 PM PDT, we experienced intermittent connectivity issues from and to instances in a single Availability Zone (sae1-az1) in the SA-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",ec2-sa-east-1,2020-09-17 22:04:21,Elastic Compute Cloud,Sao Paulo,1:37 PM,2:35 PM,PDT,58.0,2020,9,17,2020-09-17,network connectivity," 3:04 PM PDT -  Between 1:37 PM and 2:35 PM PDT, we experienced intermittent connectivity issues from and to instances in a single Availability Zone (sae1-az1) in the SA-EAST-1 Region. The issue has been resolved and the service is operating normally. "
312,Auto Scaling (Oregon),[RESOLVED] Increased API Error Rates and Latencies,1600411909,1,,"<div><span class=""yellowfg"">11:51 PM PDT</span>&nbsp;We are investigating increased API error rates and latencies in the US-WEST-2 Region</div><div><span class=""yellowfg"">Sep 18, 12:26 AM PDT</span>&nbsp;We continue to investigate increased API error rates and latencies in the US-WEST-2 Region.</div><div><span class=""yellowfg"">Sep 18,  1:17 AM PDT</span>&nbsp;We can confirm increased API error rates and latencies in the US-WEST-2 Region and continue to work towards resolution.</div><div><span class=""yellowfg"">Sep 18,  3:24 AM PDT</span>&nbsp;We have identified the root cause of the issue causing increased API error rates and latencies in the US-WEST-2 Region and continue working towards resolution.</div><div><span class=""yellowfg"">Sep 18,  3:51 AM PDT</span>&nbsp;Between September 17 8:52 PM and September 18 3:24 AM PDT, we experienced increased API error rates and latencies in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",autoscaling-us-west-2,2020-09-18 06:51:49,Auto Scaling,Oregon,8:52 PM,3:24 AM,PDT,392.0,2020,9,18,2020-09-18,increased api error rates and latencies,"11:51 PM PDT -  We are investigating increased API error rates and latencies in the US-WEST-2 Region
Sep 18, 12:26 AM PDT -  We continue to investigate increased API error rates and latencies in the US-WEST-2 Region.
Sep 18,  1:17 AM PDT -  We can confirm increased API error rates and latencies in the US-WEST-2 Region and continue to work towards resolution.
Sep 18,  3:24 AM PDT -  We have identified the root cause of the issue causing increased API error rates and latencies in the US-WEST-2 Region and continue working towards resolution.
Sep 18,  3:51 AM PDT -  Between September 17 8:52 PM and September 18 3:24 AM PDT, we experienced increased API error rates and latencies in the US-WEST-2 Region. The issue has been resolved and the service is operating normally."
313,Amazon CloudFront,[RESOLVED] 日本のエッジロケーションの一部でエラーが上昇しておりました。| Elevated Errors from one of our edge locations in Japan ,1601153265,1,,"<div><span class=""yellowfg""> 1:47 PM PDT</span>&nbsp;日本時間 9/26 PM 5:55 から PM 6:45 の間、CloudFrontをご利用中の一部のお客様で、日本のエッジロケーションで断続的にエラーが発生しておりました。現在、問題は解決され、サービスは正常に動作しています。| Between 5:55 PM and 6:45 PM JST, some CloudFront customers may have experienced intermittent errors from one of our edge locations in Japan. The issue has been resolved and service is operating normally. </div>",cloudfront,2020-09-26 20:47:45,CloudFront,Global,1:47 PM,6:45 PM,PDT,298.0,2020,9,26,2020-09-26,elevated errors from one of our edge locations in japan," 1:47 PM PDT -  日本時間 9/26 PM 5:55 から PM 6:45 の間、CloudFrontをご利用中の一部のお客様で、日本のエッジロケーションで断続的にエラーが発生しておりました。現在、問題は解決され、サービスは正常に動作しています。| Between 5:55 PM and 6:45 PM JST, some CloudFront customers may have experienced intermittent errors from one of our edge locations in Japan. The issue has been resolved and service is operating normally. "
314,Amazon Simple Notification Service (N. Virginia),[RESOLVED] Increased SNS Message Delivery Latency,1601479698,1,,"<div><span class=""yellowfg""> 8:28 AM PDT</span>&nbsp;We are investigating increased delivery times for SNS messages in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 9:34 AM PDT</span>&nbsp;We have identified the cause of increased delivery times for SNS messages and are working towards recovery.</div><div><span class=""yellowfg"">10:17 AM PDT</span>&nbsp;We have observed significant recovery for SNS message delivery times in the US-EAST-1 Region. All notifications continue to be delivered as we work towards full recovery.</div><div><span class=""yellowfg"">11:14 AM PDT</span>&nbsp;We continue to observe significant recovery for SNS message delivery times in the US-EAST-1 Region. All notifications continue to be delivered as we to work towards full recovery.</div><div><span class=""yellowfg"">11:42 AM PDT</span>&nbsp;Recently, we experienced increased SNS message delivery times in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. We are continuing to process pending message deliveries for a small number of customers.</div>",sns-us-east-1,2020-09-30 15:28:18,Simple Notification Service,N. Virginia,8:28 AM,11:42 AM,PDT,194.0,2020,9,30,2020-09-30,increased sns message delivery latency," 8:28 AM PDT -  We are investigating increased delivery times for SNS messages in the US-EAST-1 Region.
 9:34 AM PDT -  We have identified the cause of increased delivery times for SNS messages and are working towards recovery.
10:17 AM PDT -  We have observed significant recovery for SNS message delivery times in the US-EAST-1 Region. All notifications continue to be delivered as we work towards full recovery.
11:14 AM PDT -  We continue to observe significant recovery for SNS message delivery times in the US-EAST-1 Region. All notifications continue to be delivered as we to work towards full recovery.
11:42 AM PDT -  Recently, we experienced increased SNS message delivery times in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. We are continuing to process pending message deliveries for a small number of customers."
315,Amazon CloudFront,[RESOLVED] CloudFront DNS Errors,1601574323,1,,"<div><span class=""yellowfg"">10:45 AM PDT</span>&nbsp;現在日本で CloudFront ディストリビューションの名前解決において断続的な DNS エラーが発生している事象について調査しております。
| We are actively investigating intermittent DNS errors resolving some CloudFront distributions in Japan.
</div><div><span class=""yellowfg"">11:37 AM PDT</span>&nbsp;日本時間 10月2日 00:55 から 02:25 にかけて、日本の一部お客様において CloudFront ディストリビューションの DNS 名前解決が失敗する事象が発生しておりました。この問題は解決され、サービスは正常に稼働しています。
| Between 8:55 AM and 10:25 AM PDT, some customers in Japan had a small percentage of DNS lookups fail when resolving CloudFront distributions. The issue has been resolved and the service is operating normally.</div>",cloudfront,2020-10-01 17:45:23,CloudFront,Global,8:55 AM,10:25 AM,PDT,90.0,2020,10,1,2020-10-01,cloudfront dns errors,"10:45 AM PDT -  現在日本で CloudFront ディストリビューションの名前解決において断続的な DNS エラーが発生している事象について調査しております。| We are actively investigating intermittent DNS errors resolving some CloudFront distributions in Japan.
11:37 AM PDT -  日本時間 10月2日 00:55 から 02:25 にかけて、日本の一部お客様において CloudFront ディストリビューションの DNS 名前解決が失敗する事象が発生しておりました。この問題は解決され、サービスは正常に稼働しています。| Between 8:55 AM and 10:25 AM PDT, some customers in Japan had a small percentage of DNS lookups fail when resolving CloudFront distributions. The issue has been resolved and the service is operating normally."
316,AWS Identity and Access Management,[RESOLVED] Increased IAM Error Rates and Latencies,1601585449,1,,"<div><span class=""yellowfg""> 1:50 PM PDT</span>&nbsp;We are investigating increased error rates and latencies affecting IAM. IAM related requests to other AWS services may also be impacted. </div><div><span class=""yellowfg""> 2:37 PM PDT</span>&nbsp;Between 1:10 PM and 1:50 PM PDT, we experienced increased error rates for mutating (create, update, delete) IAM APIs. IAM related requests to other AWS services may have been impacted. The issue has been resolved and the service is operating normally.</div>",iam,2020-10-01 20:50:49,Identity and Access Management,Global,1:10 PM,1:50 PM,PDT,40.0,2020,10,1,2020-10-01,increased iam error rates and latencies," 1:50 PM PDT -  We are investigating increased error rates and latencies affecting IAM. IAM related requests to other AWS services may also be impacted. 
 2:37 PM PDT -  Between 1:10 PM and 1:50 PM PDT, we experienced increased error rates for mutating (create, update, delete) IAM APIs. IAM related requests to other AWS services may have been impacted. The issue has been resolved and the service is operating normally."
317,Amazon Elastic Compute Cloud (N. Virginia),[RESOLVED] Network connectivity issues,1602233331,1,,"<div><span class=""yellowfg""> 1:48 AM PDT</span>&nbsp;We are investigating networking connectivity issues for a small subset of newly launched EC2 instances within a single Availability Zone (use1-az2) in the US-EAST-1 Region. We have identified root cause and are working towards resolution. Network connectivity for existing instances is not affected by this issue. For newly launched instances that are affected, relaunching a new instance may resolve the issue.</div><div><span class=""yellowfg""> 2:47 AM PDT</span>&nbsp;We continue to work toward recovery for the networking connectivity issues affecting a small subset of newly launched EC2 instances within a single Availability Zone (use1-az2) in the US-EAST-1 Region. Network connectivity for existing instances remains unaffected by this issue. For newly launched instances that are affected, relaunching a new instance may resolve the issue.</div><div><span class=""yellowfg""> 4:53 AM PDT</span>&nbsp;We are still working toward recovery for the networking connectivity issues affecting a small subset of newly launched EC2 instances within a single Availability Zone (use1-az2) in the US-EAST-1 Region. Network connectivity for existing instances remains unaffected by this issue. For newly launched instances that are affected, launching a replacement instance may resolve the issue.</div><div><span class=""yellowfg""> 6:35 AM PDT</span>&nbsp;We are still working towards recovery for the ongoing networking connectivity issues. These affect a small subset of EC2 instances launched after October 08, 2020, at 9:37 PM PDT within a single Availability Zone (use1-az2) in the US-EAST-1 Region. For instances that are affected, customers can launch replacement instances in another Availability Zone.</div><div><span class=""yellowfg""> 9:57 AM PDT</span>&nbsp;We wanted to provide you with some more details on the issue affecting network connectivity for a subset of EC2 instances in a single Availability Zone (use1-az2) in the US-EAST-1 Region. The issue is affecting the subsystem responsible for updating VPC network configuration and mappings when new instances are launched or Elastic Network Interfaces (ENI) are attached to instances, within the affected Availability Zone. This subsystem makes use of a cell-based architecture, which subdivides the Availability Zone into smaller cells, with each cell being responsible for the VPC network configuration and mappings for a subset of instances within the Availability Zone.

At 9:37 PM PDT on October 8th, a single cell within this subsystem began experiencing elevated failures in updating VPC network configuration and mappings for instances managed by the affected cell. These elevated failures cause network configuration and mappings to be delayed or to fail for new instance launches and attachments of ENIs within the affected cell. The issue can also cause connectivity issues between an affected instance in the affected Availability Zone and newly launched instances within other Availability Zones in the US-EAST-1 Region, since updated VPC network configuration and mappings are not able to be updated within the affected Availability Zone.

We have identified the root cause and have been working to resolve the issue and restore the updating of VPC network configuration and mappings within the affected cell. For instances that are affected by this issue, relaunching the instance within the affected Availability Zone may mitigate the issue. If possible, relaunching the instance in other Availability Zones will mitigate the issue.

We will continue to provide updates as we work towards full resolution.</div><div><span class=""yellowfg"">11:11 AM PDT</span>&nbsp;We have taken steps to address the issue affecting network connectivity for some instances in a single Availability Zone (use1-az2) in the US-EAST-1 Region. As of 10:20 AM PDT, we started to see recovery for affected instances and continue to work toward full resolution of the issue.
</div><div><span class=""yellowfg"">11:54 AM PDT</span>&nbsp;Starting at 9:37 PM PDT on October 8th, we experienced increased network connectivity issues for a subset of instances within a single Availability Zone (use1-az2) in the US-EAST-1 Region. This was caused by a single cell within the subsystem responsible for the updating VPC network configuration and mappings experiencing elevated failures. These elevated failures caused network configuration and mappings to be delayed or to fail for new instance launches and attachments of ENIs within the affected cell. The issue has also caused connectivity issues between an affected instance in the affected Availability Zone(use1-az2) and newly launched instances within other Availability Zones in the US-EAST-1 Region, since updated VPC network configuration and mappings were not able to be updated within the affected Availability Zone(use1-az2). The root cause of the issue was addressed and at 10:20 AM PDT on October 9th, we began to see recovery for the affected instances. By 11:10 AM PDT, all affected instances had fully recovered. The issue has been resolved and the service is operating normally</div>",ec2-us-east-1,2020-10-09 08:48:51,Elastic Compute Cloud,N. Virginia,10:20 AM,9:37 PM,PDT,677.0,2020,10,9,2020-10-09,network connectivity issues," 1:48 AM PDT -  We are investigating networking connectivity issues for a small subset of newly launched EC2 instances within a single Availability Zone (use1-az2) in the US-EAST-1 Region. We have identified root cause and are working towards resolution. Network connectivity for existing instances is not affected by this issue. For newly launched instances that are affected, relaunching a new instance may resolve the issue.
 2:47 AM PDT -  We continue to work toward recovery for the networking connectivity issues affecting a small subset of newly launched EC2 instances within a single Availability Zone (use1-az2) in the US-EAST-1 Region. Network connectivity for existing instances remains unaffected by this issue. For newly launched instances that are affected, relaunching a new instance may resolve the issue.
 4:53 AM PDT -  We are still working toward recovery for the networking connectivity issues affecting a small subset of newly launched EC2 instances within a single Availability Zone (use1-az2) in the US-EAST-1 Region. Network connectivity for existing instances remains unaffected by this issue. For newly launched instances that are affected, launching a replacement instance may resolve the issue.
 6:35 AM PDT -  We are still working towards recovery for the ongoing networking connectivity issues. These affect a small subset of EC2 instances launched after October 08, 2020, at 9:37 PM PDT within a single Availability Zone (use1-az2) in the US-EAST-1 Region. For instances that are affected, customers can launch replacement instances in another Availability Zone.
 9:57 AM PDT -  We wanted to provide you with some more details on the issue affecting network connectivity for a subset of EC2 instances in a single Availability Zone (use1-az2) in the US-EAST-1 Region. The issue is affecting the subsystem responsible for updating VPC network configuration and mappings when new instances are launched or Elastic Network Interfaces (ENI) are attached to instances, within the affected Availability Zone. This subsystem makes use of a cell-based architecture, which subdivides the Availability Zone into smaller cells, with each cell being responsible for the VPC network configuration and mappings for a subset of instances within the Availability Zone.At 9:37 PM PDT on October 8th, a single cell within this subsystem began experiencing elevated failures in updating VPC network configuration and mappings for instances managed by the affected cell. These elevated failures cause network configuration and mappings to be delayed or to fail for new instance launches and attachments of ENIs within the affected cell. The issue can also cause connectivity issues between an affected instance in the affected Availability Zone and newly launched instances within other Availability Zones in the US-EAST-1 Region, since updated VPC network configuration and mappings are not able to be updated within the affected Availability Zone.We have identified the root cause and have been working to resolve the issue and restore the updating of VPC network configuration and mappings within the affected cell. For instances that are affected by this issue, relaunching the instance within the affected Availability Zone may mitigate the issue. If possible, relaunching the instance in other Availability Zones will mitigate the issue.We will continue to provide updates as we work towards full resolution.
11:11 AM PDT -  We have taken steps to address the issue affecting network connectivity for some instances in a single Availability Zone (use1-az2) in the US-EAST-1 Region. As of 10:20 AM PDT, we started to see recovery for affected instances and continue to work toward full resolution of the issue.
11:54 AM PDT -  Starting at 9:37 PM PDT on October 8th, we experienced increased network connectivity issues for a subset of instances within a single Availability Zone (use1-az2) in the US-EAST-1 Region. This was caused by a single cell within the subsystem responsible for the updating VPC network configuration and mappings experiencing elevated failures. These elevated failures caused network configuration and mappings to be delayed or to fail for new instance launches and attachments of ENIs within the affected cell. The issue has also caused connectivity issues between an affected instance in the affected Availability Zone(use1-az2) and newly launched instances within other Availability Zones in the US-EAST-1 Region, since updated VPC network configuration and mappings were not able to be updated within the affected Availability Zone(use1-az2). The root cause of the issue was addressed and at 10:20 AM PDT on October 9th, we began to see recovery for the affected instances. By 11:10 AM PDT, all affected instances had fully recovered. The issue has been resolved and the service is operating normally"
318,AWS Lambda (N. Virginia),[RESOLVED] Increased Invoke Error Rate,1602234228,1,,"<div><span class=""yellowfg""> 2:03 AM PDT</span>&nbsp;We have identified an increase in invoke error rates in the US-EAST-1 Region and are working towards resolution.</div><div><span class=""yellowfg""> 3:11 AM PDT</span>&nbsp;Between October 8 10:35 PM and October 9 2:25 AM PDT we experienced increased Lambda invoke error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",lambda-us-east-1,2020-10-09 09:03:48,Lambda,N. Virginia,10:35 PM,2:25 AM,PDT,230.0,2020,10,9,2020-10-09,increased invoke error rate," 2:03 AM PDT -  We have identified an increase in invoke error rates in the US-EAST-1 Region and are working towards resolution.
 3:11 AM PDT -  Between October 8 10:35 PM and October 9 2:25 AM PDT we experienced increased Lambda invoke error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
319,AWS Internet Connectivity (Sao Paulo),[RESOLVED] Internet connectivity issues ,1602367502,2,,"<div><span class=""yellowfg""> 3:05 PM PDT</span>&nbsp;We are investigating internet connectivity issues in the SA-EAST-1 Region.</div><div><span class=""yellowfg""> 3:11 PM PDT</span>&nbsp;Between 2:26 PM and 3:00 PM PDT, customers experienced issues connecting to the SA-EAST-1 Region from the internet and other AWS Regions. The issue has been resolved and all services are operating normally.</div>",internetconnectivity-sa-east-1,2020-10-10 22:05:02,Internet Connectivity,Sao Paulo,2:26 PM,3:00 PM,PDT,34.0,2020,10,10,2020-10-10,internet connectivity issues," 3:05 PM PDT -  We are investigating internet connectivity issues in the SA-EAST-1 Region.
 3:11 PM PDT -  Between 2:26 PM and 3:00 PM PDT, customers experienced issues connecting to the SA-EAST-1 Region from the internet and other AWS Regions. The issue has been resolved and all services are operating normally."
320,AWS Marketplace,[RESOLVED] Increased AWS Marketplace Subscription Error Rates and Latencies,1602664203,1,,"<div><span class=""yellowfg""> 1:30 AM PDT</span>&nbsp;We can confirm increased AWS Marketplace subscription error rates and latencies. We have identified the cause of the impact and continue working towards resolution.</div><div><span class=""yellowfg""> 2:27 AM PDT</span>&nbsp;We can confirm increased AWS Marketplace subscription error rates and latencies. We are seeing partial recovery and are working towards full resolution. </div><div><span class=""yellowfg""> 2:54 AM PDT</span>&nbsp;Between October 13 8:45 PM and October 14 2:50 AM PDT, we experienced increased AWS Marketplace subscription error rates and latencies. The issue has been resolved and the service is operating normally.</div>",marketplace,2020-10-14 08:30:03,Marketplace,Global,8:45 PM,2:50 AM,PDT,365.0,2020,10,14,2020-10-14,increased aws marketplace subscription error rates and latencies," 1:30 AM PDT -  We can confirm increased AWS Marketplace subscription error rates and latencies. We have identified the cause of the impact and continue working towards resolution.
 2:27 AM PDT -  We can confirm increased AWS Marketplace subscription error rates and latencies. We are seeing partial recovery and are working towards full resolution. 
 2:54 AM PDT -  Between October 13 8:45 PM and October 14 2:50 AM PDT, we experienced increased AWS Marketplace subscription error rates and latencies. The issue has been resolved and the service is operating normally."
321,Amazon Elastic Compute Cloud (Sydney),EC2 Instance Meta Data Service Errors ,1603333734,1,,"<div><span class=""yellowfg""> 7:28 PM PDT</span>&nbsp;We are investigating increased errors for the EC2 Instance Meta Data Service in the AP-SOUTHEAST-2 Region. Applications within the instance attempting to query the Instance Meta Data Service may experience increased errors or timeouts. Since the Instance Meta Data Service may be used during the operating system boot process, some instances may be experiencing launch failures. We are working to resolve the issue.</div><div><span class=""yellowfg""> 8:46 PM PDT</span>&nbsp;We continue to investigate increased errors for the EC2 Instance Meta Data Service in the AP-SOUTHEAST-2 Region. We are seeing the most impact for newly launched instances. Applications within the instance attempting to query the Instance Meta Data Service may experience increased errors or timeouts. We are working to resolve the issue.</div><div><span class=""yellowfg""> 9:58 PM PDT</span>&nbsp;We have identified the root cause and resolved the issue causing increased errors and timeouts for the EC2 Instance Meta Data Service for newly launched instances in the AP-SOUTHEAST-2 Region. From 9:26 PM PDT, newly launched instances are no longer experiencing increased errors and timeouts for the EC2 Instance Meta Data Service. Instances that were affected by this issue will self-recover over the coming hours, but for immediate recovery, we recommend relaunching, or stopping and starting, the affected instances to resolve the issue. The issue has been resolved and the service is operating normally.</div>",ec2-ap-southeast-2,2020-10-22 02:28:54,Elastic Compute Cloud,Sydney,7:28 PM,9:26 PM,PDT,118.0,2020,10,22,2020-10-22,ec2 instance meta data service errors," 7:28 PM PDT -  We are investigating increased errors for the EC2 Instance Meta Data Service in the AP-SOUTHEAST-2 Region. Applications within the instance attempting to query the Instance Meta Data Service may experience increased errors or timeouts. Since the Instance Meta Data Service may be used during the operating system boot process, some instances may be experiencing launch failures. We are working to resolve the issue.
 8:46 PM PDT -  We continue to investigate increased errors for the EC2 Instance Meta Data Service in the AP-SOUTHEAST-2 Region. We are seeing the most impact for newly launched instances. Applications within the instance attempting to query the Instance Meta Data Service may experience increased errors or timeouts. We are working to resolve the issue.
 9:58 PM PDT -  We have identified the root cause and resolved the issue causing increased errors and timeouts for the EC2 Instance Meta Data Service for newly launched instances in the AP-SOUTHEAST-2 Region. From 9:26 PM PDT, newly launched instances are no longer experiencing increased errors and timeouts for the EC2 Instance Meta Data Service. Instances that were affected by this issue will self-recover over the coming hours, but for immediate recovery, we recommend relaunching, or stopping and starting, the affected instances to resolve the issue. The issue has been resolved and the service is operating normally."
322,Amazon Elastic Compute Cloud (Tokyo),[RESOLVED] ネットワーク接続性の問題 | Network Connectivity Issues ,1603336222,1,,"<div><span class=""yellowfg""> 8:10 PM PDT</span>&nbsp;AP-NORTHEAST-1リージョンのアベイラビリティゾーン(APNE1-AZ2)における一部のEC2インスタンスおよびEBSボリュームのパフォーマンス低下に関するネットワーク接続性の問題を調査しております。 | We are investigating network connectivity issues for some EC2 instances and degraded volume performance for some EBS volumes in a single Available Zone (APNE1-AZ2) in the AP-NORTHEAST-1 Region.</div><div><span class=""yellowfg""> 8:41 PM PDT</span>&nbsp;一部のEC2インスタンスに影響を与えていたネットワーク接続性の問題は解消しました。AP-NORTHEAST-1リージョンのアベイラビリティゾーン(APNE1-AZ2)における一部のEBSボリュームのパフォーマンス低下の問題については、引き続き調査を行っております | The networking connectivity issues affecting some EC2 instances have been resolved but we continue to investigate degraded performance for some EBS volumes within the affected Availability Zone (APNE1-AZ2) in the AP-NORTHEAST-1 Region. </div><div><span class=""yellowfg""> 9:15 PM PDT</span>&nbsp;日本時間11:42から11:53の間、AP-NORTHEAST-1リージョンのアベイラビリティゾーン(APNE1-AZ2)における一部のインスタンスにおいて、ネットワーク接続性の問題がありました。また、日本時間11:42から13:09の間、同アベイラビリティゾーンにおける一部のEBSボリュームにおいて、ボリュームパフォーマンスの低下がありました。問題は解消し、現在正常に稼働しております | Between 7:42 PM PDT and 7:53 PM PDT we experienced network connectivity issues for some instances in a single Availability Zone (APNE1-AZ2) in the AP-NORTHEAST-1 Region. Between 7:42 PM and 9:09 PM PDT, we also had degraded volume performance for some EBS volumes in that Availability Zone. The issue has been resolved and the service is operating normally.</div>",ec2-ap-northeast-1,2020-10-22 03:10:22,Elastic Compute Cloud,Tokyo,7:42 PM,9:09 PM,PDT,87.0,2020,10,22,2020-10-22,network connectivity issues," 8:10 PM PDT -  AP-NORTHEAST-1リージョンのアベイラビリティゾーン(APNE1-AZ2)における一部のEC2インスタンスおよびEBSボリュームのパフォーマンス低下に関するネットワーク接続性の問題を調査しております。 | We are investigating network connectivity issues for some EC2 instances and degraded volume performance for some EBS volumes in a single Available Zone (APNE1-AZ2) in the AP-NORTHEAST-1 Region.
 8:41 PM PDT -  一部のEC2インスタンスに影響を与えていたネットワーク接続性の問題は解消しました。AP-NORTHEAST-1リージョンのアベイラビリティゾーン(APNE1-AZ2)における一部のEBSボリュームのパフォーマンス低下の問題については、引き続き調査を行っております | The networking connectivity issues affecting some EC2 instances have been resolved but we continue to investigate degraded performance for some EBS volumes within the affected Availability Zone (APNE1-AZ2) in the AP-NORTHEAST-1 Region. 
 9:15 PM PDT -  日本時間11:42から11:53の間、AP-NORTHEAST-1リージョンのアベイラビリティゾーン(APNE1-AZ2)における一部のインスタンスにおいて、ネットワーク接続性の問題がありました。また、日本時間11:42から13:09の間、同アベイラビリティゾーンにおける一部のEBSボリュームにおいて、ボリュームパフォーマンスの低下がありました。問題は解消し、現在正常に稼働しております | Between 7:42 PM PDT and 7:53 PM PDT we experienced network connectivity issues for some instances in a single Availability Zone (APNE1-AZ2) in the AP-NORTHEAST-1 Region. Between 7:42 PM and 9:09 PM PDT, we also had degraded volume performance for some EBS volumes in that Availability Zone. The issue has been resolved and the service is operating normally."
323,Amazon Elastic Compute Cloud (Seoul),[RESOLVED] Increased API Error Rates,1603669101,1,,"<div><span class=""yellowfg""> 4:38 PM PDT</span>&nbsp;저희는 AP-NORTHEAST-2 리전의 API 오류율와 지연속도 증가를 조사 중에 있습니다. | We are investigating increased API error rates and latencies in the AP-NORTHEAST-2 Region.</div><div><span class=""yellowfg""> 5:27 PM PDT</span>&nbsp;AP-NORTHEAST-2 리전의 EC2 API에 3:46 PM 부터 5:08 PM PDT 까지 지연속도의 증가가 있었습니다. 해당 문제는 현재 해결되었으며 정상 동작 중입니다. | Between 3:46 PM and 5:08 PM PDT we experienced slightly increased latencies for the EC2 APIs in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally.</div>",ec2-ap-northeast-2,2020-10-25 23:38:21,Elastic Compute Cloud,Seoul,3:46 PM,5:08 PM,PDT,82.0,2020,10,25,2020-10-25,increased api error rates," 4:38 PM PDT -  저희는 AP-NORTHEAST-2 리전의 API 오류율와 지연속도 증가를 조사 중에 있습니다. | We are investigating increased API error rates and latencies in the AP-NORTHEAST-2 Region.
 5:27 PM PDT -  AP-NORTHEAST-2 리전의 EC2 API에 3:46 PM 부터 5:08 PM PDT 까지 지연속도의 증가가 있었습니다. 해당 문제는 현재 해결되었으며 정상 동작 중입니다. | Between 3:46 PM and 5:08 PM PDT we experienced slightly increased latencies for the EC2 APIs in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally."
324,Amazon Route 53 Resolver (N. Virginia),[RESOLVED] Increased DNS query timeouts,1604345391,1,,"<div><span class=""yellowfg"">11:29 AM PST</span>&nbsp;We are investigating intermittent increased DNS query timeouts in a single Availability Zone (use1-az6) of the US-EAST-1 Region. We are already seeing recovery but are working to confirm.</div><div><span class=""yellowfg"">11:42 AM PST</span>&nbsp;Between 9:51 AM and 10:02 AM, and between 10:41 AM and 11:10 AM PST, customers experienced increased DNS query timeouts in a single Availability Zone (use1-az6) of the US-EAST-1 Region. The issue has been resolved and all DNS queries are being answered normally.</div>",route53resolver-us-east-1,2020-11-02 19:29:51,Route 53 Resolver,N. Virginia,10:41 AM,11:10 AM,PST,29.0,2020,11,2,2020-11-02,increased dns query timeouts,"11:29 AM PST -  We are investigating intermittent increased DNS query timeouts in a single Availability Zone (use1-az6) of the US-EAST-1 Region. We are already seeing recovery but are working to confirm.
11:42 AM PST -  Between 9:51 AM and 10:02 AM, and between 10:41 AM and 11:10 AM PST, customers experienced increased DNS query timeouts in a single Availability Zone (use1-az6) of the US-EAST-1 Region. The issue has been resolved and all DNS queries are being answered normally."
325,Amazon CloudFront,[RESOLVED] Change Propagation Delays,1604706910,1,,"<div><span class=""yellowfg""> 3:55 PM PST</span>&nbsp;We are investigating longer than usual propagation times for changes to CloudFront configurations to few of our edge locations. End-user requests for content from our edge locations are not affected by this issue and are being served normally. </div><div><span class=""yellowfg""> 4:25 PM PST</span>&nbsp;We have identified the root cause of the longer than usual propagation times for changes to CloudFront configurations. We continue to work toward resolution. End-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=""yellowfg""> 4:42 PM PST</span>&nbsp;Between 2:21 PM and 4:23 PM PST customers may have experienced longer than usual propagation times while making changes to CloudFront configurations. End-user requests for content from edge locations were not affected. The issue has been resolved and the service is operating normally.</div>",cloudfront,2020-11-06 23:55:10,CloudFront,Global,2:21 PM,4:23 PM,PST,122.0,2020,11,6,2020-11-06,change propagation delays," 3:55 PM PST -  We are investigating longer than usual propagation times for changes to CloudFront configurations to few of our edge locations. End-user requests for content from our edge locations are not affected by this issue and are being served normally. 
 4:25 PM PST -  We have identified the root cause of the longer than usual propagation times for changes to CloudFront configurations. We continue to work toward resolution. End-user requests for content from our edge locations are not affected by this issue and are being served normally.
 4:42 PM PST -  Between 2:21 PM and 4:23 PM PST customers may have experienced longer than usual propagation times while making changes to CloudFront configurations. End-user requests for content from edge locations were not affected. The issue has been resolved and the service is operating normally."
326,Amazon Elastic Compute Cloud (Seoul),[RESOLVED] 네트워크 연결 | Network Connectivity,1605071272,1,,"<div><span class=""yellowfg""> 9:07 PM PST</span>&nbsp;2020년 11월 11 일 오전 4:59에서 오전 11:25 (대한민국 시간 기준) 사이에 AP-NORTHEAST-2 Region 내 한 개의 Availability Zone에서 소수 instance의 연결 (connectivity) 이슈가 있었습니다. 해당 이슈는 해결 되었고 서비스는 정상적으로 제공되고 있습니다. 본 서비스의 이용을 복구하기 위하여 이용자가 추가적으로 조치할 사항은 없습니다. 만약 질문이 있으시거나 서비스 관련 운영상의 이슈가 있을경우, AWS Support Center상 다음 링크(https://console.aws.amazon.com/support ) 를 통하여 AWS Support Department에 연락하여 주시길 바랍니다. | Between 11:59 AM and 6:25 PM PST on November 10, 2020, a small number of instances experienced connectivity issues in a single Availability Zone in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally.  Customers do not need to take additional measures to restore the availability of this service. If you have any questions or are experiencing any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support.</div>",ec2-ap-northeast-2,2020-11-11 05:07:52,Elastic Compute Cloud,Seoul,11:59 AM,6:25 PM,PST,386.0,2020,11,11,2020-11-11,network connectivity," 9:07 PM PST -  2020년 11월 11 일 오전 4:59에서 오전 11:25 (대한민국 시간 기준) 사이에 AP-NORTHEAST-2 Region 내 한 개의 Availability Zone에서 소수 instance의 연결 (connectivity) 이슈가 있었습니다. 해당 이슈는 해결 되었고 서비스는 정상적으로 제공되고 있습니다. 본 서비스의 이용을 복구하기 위하여 이용자가 추가적으로 조치할 사항은 없습니다. 만약 질문이 있으시거나 서비스 관련 운영상의 이슈가 있을경우, AWS Support Center상 다음 링크(https://console.aws.amazon.com/support ) 를 통하여 AWS Support Department에 연락하여 주시길 바랍니다. | Between 11:59 AM and 6:25 PM PST on November 10, 2020, a small number of instances experienced connectivity issues in a single Availability Zone in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally.  Customers do not need to take additional measures to restore the availability of this service. If you have any questions or are experiencing any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support."
327,Amazon Athena (Mumbai),[RESOLVED] Increased Web Console Error Rate ,1605156361,1,,"<div><span class=""yellowfg""> 8:46 PM PST</span>&nbsp;We are investigating increased error rates for requests to the Athena Management Console in the AP-SOUTH-1 Region. The APIs and Command Line Interface (CLI) are not impacted by this issue.</div><div><span class=""yellowfg""> 9:44 PM PST</span>&nbsp;We can confirm increased error rates for requests to the Athena Management Console in the AP-SOUTH-1 Region and continue working towards resolution. The APIs and Command Line Interface (CLI) are not impacted by this issue.</div><div><span class=""yellowfg"">10:31 PM PST</span>&nbsp;We have identified the root cause of the issue causing increased error rates for requests to the Athena Management Console in the AP-SOUTH-1 Region and continue working towards resolution. The APIs and Command Line Interface (CLI) are not impacted by this issue.</div><div><span class=""yellowfg"">11:38 PM PST</span>&nbsp;Between 2:45 PM and 11:20 PM PST, we experienced intermittent increased error rates for requests to the Amazon Athena Management Console in the AP-SOUTH-1 Region. The APIs and Command Line Interface (CLI) were not impacted by this issue. The issue has been resolved and the service is operating normally.</div>",athena-ap-south-1,2020-11-12 04:46:01,Athena,Mumbai,2:45 PM,11:20 PM,PST,515.0,2020,11,12,2020-11-12,increased web console error rate," 8:46 PM PST -  We are investigating increased error rates for requests to the Athena Management Console in the AP-SOUTH-1 Region. The APIs and Command Line Interface (CLI) are not impacted by this issue.
 9:44 PM PST -  We can confirm increased error rates for requests to the Athena Management Console in the AP-SOUTH-1 Region and continue working towards resolution. The APIs and Command Line Interface (CLI) are not impacted by this issue.
10:31 PM PST -  We have identified the root cause of the issue causing increased error rates for requests to the Athena Management Console in the AP-SOUTH-1 Region and continue working towards resolution. The APIs and Command Line Interface (CLI) are not impacted by this issue.
11:38 PM PST -  Between 2:45 PM and 11:20 PM PST, we experienced intermittent increased error rates for requests to the Amazon Athena Management Console in the AP-SOUTH-1 Region. The APIs and Command Line Interface (CLI) were not impacted by this issue. The issue has been resolved and the service is operating normally."
328,AWS QuickSight (N. Virginia),[RESOLVED] Increased Website Latencies and Error Rates ,1605629180,1,,"<div><span class=""yellowfg""> 8:06 AM PST</span>&nbsp;We are investigating increased website latencies and error rates in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 8:49 AM PST</span>&nbsp;We have identified the root cause of the issue and are working towards resolution. We have started to see improvements in error rates and latencies in US-EAST-1 Region.</div><div><span class=""yellowfg""> 9:23 AM PST</span>&nbsp;Between 7:01 AM and 9:14 AM PST, we experienced increased website latencies and error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",quicksight-us-east-1,2020-11-17 16:06:20,QuickSight,N. Virginia,7:01 AM,9:14 AM,PST,133.0,2020,11,17,2020-11-17,increased website latencies and error rates," 8:06 AM PST -  We are investigating increased website latencies and error rates in the US-EAST-1 Region.
 8:49 AM PST -  We have identified the root cause of the issue and are working towards resolution. We have started to see improvements in error rates and latencies in US-EAST-1 Region.
 9:23 AM PST -  Between 7:01 AM and 9:14 AM PST, we experienced increased website latencies and error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. "
329,AWS Internet Connectivity (Sao Paulo),[RESOLVED] Network Connectivity ,1605835318,1,,"<div><span class=""yellowfg""> 5:21 PM PST</span>&nbsp;We are investigating an issue which is affecting connectivity from the Internet and other AWS Regions for some customers using services in the SA-EAST-1 Region.</div><div><span class=""yellowfg""> 5:44 PM PST</span>&nbsp;Between 4:47 PM and 5:19 PM PST we experienced an issue which affected Internet connectivity for some customers using AWS services in the SA-EAST-1 Region. Connectivity between instances within the Region was not affected. The issue has been resolved and connectivity has been restored. </div>",internetconnectivity-sa-east-1,2020-11-20 01:21:58,Internet Connectivity,Sao Paulo,4:47 PM,5:19 PM,PST,32.0,2020,11,20,2020-11-20,network connectivity," 5:21 PM PST -  We are investigating an issue which is affecting connectivity from the Internet and other AWS Regions for some customers using services in the SA-EAST-1 Region.
 5:44 PM PST -  Between 4:47 PM and 5:19 PM PST we experienced an issue which affected Internet connectivity for some customers using AWS services in the SA-EAST-1 Region. Connectivity between instances within the Region was not affected. The issue has been resolved and connectivity has been restored. "
330,Amazon Kinesis Data Streams (N. Virginia),[RESOLVED] Increased API Errors,1605843388,1,,"<div><span class=""yellowfg""> 7:36 PM PST</span>&nbsp;We are investigating increased API error rates in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 8:12 PM PST</span>&nbsp;We are still investigating increased API error rates in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 9:23 PM PST</span>&nbsp;Between 6:36 PM and 9:05 PM PST, Kinesis Data Streams experienced increased API error rates in the US-EAST-1 region. The issue has been resolved and the service is operating normally.</div>",kinesis-us-east-1,2020-11-20 03:36:28,Kinesis Data Streams,N. Virginia,6:36 PM,9:05 PM,PST,149.0,2020,11,20,2020-11-20,increased api errors," 7:36 PM PST -  We are investigating increased API error rates in the US-EAST-1 Region.
 8:12 PM PST -  We are still investigating increased API error rates in the US-EAST-1 Region.
 9:23 PM PST -  Between 6:36 PM and 9:05 PM PST, Kinesis Data Streams experienced increased API error rates in the US-EAST-1 region. The issue has been resolved and the service is operating normally."
331,Amazon CloudWatch (N. Virginia),[RESOLVED] Elevated API Errors and Delayed metrics,1605843713,1,,"<div><span class=""yellowfg""> 7:41 PM PST</span>&nbsp;We are investigating increased delays for CloudWatch metrics in the US-EAST-1 Region. CloudWatch alarms may transition into ""INSUFFICIENT_DATA"" state if set on delayed metrics. </div><div><span class=""yellowfg""> 8:31 PM PST</span>&nbsp;We continue to investigate elevated API error rates and increased delays for CloudWatch metrics in the US-EAST-1 Region. CloudWatch alarms may transition into ""INSUFFICIENT_DATA"" state if set on delayed metrics.</div><div><span class=""yellowfg""> 9:32 PM PST</span>&nbsp;Between 7:16 PM and 9:08 PM PST, some customers experienced elevated error rates when calling CloudWatch APIs in the US-EAST-1 Region. Some metrics were delayed, and CloudWatch alarms on delayed metrics transitioned into INSUFFICIENT_DATA state. We have resolved the issue. Delayed metrics are in the process of backfilling in CloudWatch console graphs and for API retrieval. The service is operating normally.</div>",cloudwatch-us-east-1,2020-11-20 03:41:53,CloudWatch,N. Virginia,7:16 PM,9:08 PM,PST,112.0,2020,11,20,2020-11-20,elevated api errors and delayed metrics," 7:41 PM PST -  We are investigating increased delays for CloudWatch metrics in the US-EAST-1 Region. CloudWatch alarms may transition into ""INSUFFICIENT_DATA"" state if set on delayed metrics. 
 8:31 PM PST -  We continue to investigate elevated API error rates and increased delays for CloudWatch metrics in the US-EAST-1 Region. CloudWatch alarms may transition into ""INSUFFICIENT_DATA"" state if set on delayed metrics.
 9:32 PM PST -  Between 7:16 PM and 9:08 PM PST, some customers experienced elevated error rates when calling CloudWatch APIs in the US-EAST-1 Region. Some metrics were delayed, and CloudWatch alarms on delayed metrics transitioned into INSUFFICIENT_DATA state. We have resolved the issue. Delayed metrics are in the process of backfilling in CloudWatch console graphs and for API retrieval. The service is operating normally."
332,Amazon AppStream 2.0 (N. Virginia),[RESOLVED] Elevated API Error Rates,1605844702,1,,"<div><span class=""yellowfg""> 7:58 PM PST</span>&nbsp;We are investigating increased errors starting fleets and provisioning streaming instances in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 8:51 PM PST</span>&nbsp;We are continuing to investigate increased errors starting fleets and provisioning streaming instances in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 9:26 PM PST</span>&nbsp;Between 6:46 PM and 8:59 PM PST, AppStream 2.0 experienced increased errors starting fleets and provisioning streaming instances in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>",appstream2-us-east-1,2020-11-20 03:58:22,AppStream 2.0,N. Virginia,6:46 PM,8:59 PM,PST,133.0,2020,11,20,2020-11-20,elevated api error rates," 7:58 PM PST -  We are investigating increased errors starting fleets and provisioning streaming instances in the US-EAST-1 Region.
 8:51 PM PST -  We are continuing to investigate increased errors starting fleets and provisioning streaming instances in the US-EAST-1 Region.
 9:26 PM PST -  Between 6:46 PM and 8:59 PM PST, AppStream 2.0 experienced increased errors starting fleets and provisioning streaming instances in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. "
333,AWS WAF,[RESOLVED] Elevated WAF Error Rates,1606064823,1,,"<div><span class=""yellowfg""> 9:07 AM PST</span>&nbsp;We are investigating increased error rates for request inspection by AWS WAF in the EU-WEST-1 Region. Customers who use AWS WAF with ALB in EU-WEST-1 may see increased ALB error rates.</div><div><span class=""yellowfg""> 9:15 AM PST</span>&nbsp;We are investigating increased error rates for request inspection by AWS WAF in the EU-WEST-1 Region. Customers who use AWS WAF with ALB in EU-WEST-1 may see increased ALB error rates. Impacted customers can optionally set ALB to fail open instead of the default of failing closed: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/application-load-balancers.html#load-balancer-waf</div><div><span class=""yellowfg"">10:22 AM PST</span>&nbsp;We are starting to see recovery of error rates for request inspection by AWS WAF in the EU-WEST-1 Region. Impacted customers can optionally set ALB to fail open instead of the default of failing closed:  https://docs.aws.amazon.com/elasticloadbalancing/latest/application/application-load-balancers.html#load-balancer-waf</div><div><span class=""yellowfg"">10:45 AM PST</span>&nbsp;Between 7:54 AM and 9:43 AM PST, some customers may have experienced elevated error rates for request inspection by AWS WAF in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>",awswaf,2020-11-22 17:07:03,WAF,Global,7:54 AM,9:43 AM,PST,109.0,2020,11,22,2020-11-22,elevated waf error rates," 9:07 AM PST -  We are investigating increased error rates for request inspection by AWS WAF in the EU-WEST-1 Region. Customers who use AWS WAF with ALB in EU-WEST-1 may see increased ALB error rates.
 9:15 AM PST -  We are investigating increased error rates for request inspection by AWS WAF in the EU-WEST-1 Region. Customers who use AWS WAF with ALB in EU-WEST-1 may see increased ALB error rates. Impacted customers can optionally set ALB to fail open instead of the default of failing closed: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/application-load-balancers.html#load-balancer-waf
10:22 AM PST -  We are starting to see recovery of error rates for request inspection by AWS WAF in the EU-WEST-1 Region. Impacted customers can optionally set ALB to fail open instead of the default of failing closed:  https://docs.aws.amazon.com/elasticloadbalancing/latest/application/application-load-balancers.html#load-balancer-waf
10:45 AM PST -  Between 7:54 AM and 9:43 AM PST, some customers may have experienced elevated error rates for request inspection by AWS WAF in the EU-WEST-1 Region. The issue has been resolved and the service is operating normally."
334,Amazon Kinesis Data Streams (N. Virginia),[RESOLVED] Additional Information,1606310100,0,,"<div><span class=""yellowfg"">Nov 28, 12:05 AM PST</span>&nbsp;We’d like to share more information about the Kinesis event on Wednesday November 25th. Additional details are available <a href='https://aws.amazon.com/message/11201/'>here</a>. Should you have any questions, please contact <a href='https://aws.amazon.com/support'>AWS Support</a>.</div>",kinesis-us-east-1,2020-11-25 13:15:00,Kinesis Data Streams,N. Virginia,12:05 AM,12:05 AM,PST,0.0,2020,11,25,2020-11-25,additional information,"Nov 28, 12:05 AM PST -  We’d like to share more information about the Kinesis event on Wednesday November 25th. Additional details are available here. Should you have any questions, please contact AWS Support."
335,Amazon Kinesis Data Streams (N. Virginia),[RESOLVED] Increased API Error Rates,1606314960,3,,"<div><span class=""yellowfg""> 6:36 AM PST</span>&nbsp;We are investigating increased error rates for Kinesis Data Streams APIs in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 7:50 AM PST</span>&nbsp;We are continuing to investigate increased Kinesis Data Streams API errors, and are working on identifying root cause.</div><div><span class=""yellowfg""> 8:12 AM PST</span>&nbsp;Kinesis Data Streams customers are still experiencing increased API errors. This is also impacting other services, including ACM, Amplify Console, API Gateway, AppStream2, AppSync, Athena, Cloudformation, Cloudtrail, CloudWatch, Cognito, Connect, DynamoDB, EventBridge, IoT Services, Lambda, LEX, Managed Blockchain, Resource Groups, SageMaker, Support Console, and Workspaces. We are continuing to work on identifying root cause.</div><div><span class=""yellowfg""> 8:52 AM PST</span>&nbsp;The Kinesis Data Streams API is severely impaired. This is also impacting other services, including ACM, Amplify Console, API Gateway, AppStream2, AppSync, Athena, CloudFormation, CloudTrail, CloudWatch, Cognito, Connect, DynamoDB, EventBridge, IoT Services, Lambda, LEX, Managed Blockchain, Resource Groups, SageMaker, Support Console, and Workspaces. We are actively working towards resolution.</div><div><span class=""yellowfg""> 9:32 AM PST</span>&nbsp;The Kinesis Data Streams API is currently impaired in the US-EAST-1 Region. As a result customers are not able to write or read data published to Kinesis streams.

CloudWatch metrics and events are also affected, with elevated PutMetricData API error rates and some delayed metrics.

While EC2 instances and connectivity remain healthy, some instances are experiencing delayed instance health metrics, but remain in a healthy state.

AutoScaling is also experiencing delays in scaling times due to CloudWatch metric delays.

The issue is also affecting other services, including ACM, Amplify Console, API Gateway, AppMesh, AppStream2, AppSync, Athena, Batch, CloudFormation, CloudTrail, Cognito, Connect, DynamoDB, EventBridge, Glue, IoT Services, Lambda, LEX, Managed Blockchain, Marketplace, Personalize, RDS, Resource Groups, SageMaker, Support Console, Well Architected, and Workspaces. For further details on each of these services, please see the Personal Health Dashboard.

Other services, like S3, remain unaffected by this event.

This issue has also affected our ability to post updates to the Service Health Dashboard.

We are continuing to work towards resolution.</div><div><span class=""yellowfg"">11:23 AM PST</span>&nbsp;We continue to work towards recovery of the issue affecting the Kinesis Data Streams API in the US-EAST-1 Region. For Kinesis Data Streams, the issue is affecting the subsystem that is responsible for handling incoming requests. The team has identified the root cause and we continue to make progress in addressing the root cause. We are seeing some improvement in error rates, but continue to work towards full resolution.

The issue also affects other services, or parts of these services, that utilize Kinesis Data Streams within their workflows. While features of multiple services are impacted, some services have seen broader impact and service-specific impact details are included within Recent Events on the Service Health Dashboard.</div><div><span class=""yellowfg""> 1:59 PM PST</span>&nbsp;Kinesis Data Streams API requests are still significantly impaired. We have identified a mitigation for this issue, and are actively working towards resolution.</div><div><span class=""yellowfg""> 2:49 PM PST</span>&nbsp;Kinesis Data Streams API requests are still impaired but are starting to see recovery. We continue to actively work towards resolution.</div><div><span class=""yellowfg""> 4:42 PM PST</span>&nbsp;Kinesis Data Streams API operations are seeing gradual recovery but customers may continue to experience increased latencies and failure rates. We continue to actively work towards resolution.</div><div><span class=""yellowfg""> 6:32 PM PST</span>&nbsp;We have now fully mitigated the impact to the subsystem within Kinesis that is responsible for the processing of incoming requests and are no longer seeing increased error rates or latencies. However, we are not yet taking the full traffic load and are working to relax request throttles on the service. Over the next few hours we expect to relax these throttles to previous levels. We expect customers to begin seeing recovery as these throttles are relaxed over this timeframe.</div><div><span class=""yellowfg""> 8:53 PM PST</span>&nbsp;We are continuing to relax the request throttles for Kinesis Data Streams and are gradually increasing the traffic into the service. We have not yet enabled requests to Kinesis Data Streams from VPC Endpoints. The Kinesis Data Streams subsystem continues to operate normally, and we expect incremental recovery over the next few hours.</div><div><span class=""yellowfg""> 9:26 PM PST</span>&nbsp;We have now enabled a subset of requests to Kinesis Data Streams using VPC Endpoints.</div><div><span class=""yellowfg"">10:06 PM PST</span>&nbsp;We have now enabled all requests to Kinesis Data Streams through Internet-facing endpoints. We are continuing to work to re-enable all requests to Kinesis Data Streams using VPC Endpoints.</div><div><span class=""yellowfg"">11:00 PM PST</span>&nbsp;We have now enabled all requests to Kinesis Data Streams through both Internet-facing endpoints and VPC Endpoints.</div><div><span class=""yellowfg"">Nov 26, 12:03 AM PST</span>&nbsp;Between 5:15 AM and 11:10 PM PST customers experienced a significant impairment to their Amazon Kinesis Data Streams API operations. We have identified the root cause and have completed immediate actions to prevent recurrence. The issue has been resolved and the service is operating normally.</div>",kinesis-us-east-1,2020-11-25 14:36:00,Kinesis Data Streams,N. Virginia,5:15 AM,11:10 PM,PST,1075.0,2020,11,25,2020-11-25,increased api error rates," 6:36 AM PST -  We are investigating increased error rates for Kinesis Data Streams APIs in the US-EAST-1 Region.
 7:50 AM PST -  We are continuing to investigate increased Kinesis Data Streams API errors, and are working on identifying root cause.
 8:12 AM PST -  Kinesis Data Streams customers are still experiencing increased API errors. This is also impacting other services, including ACM, Amplify Console, API Gateway, AppStream2, AppSync, Athena, Cloudformation, Cloudtrail, CloudWatch, Cognito, Connect, DynamoDB, EventBridge, IoT Services, Lambda, LEX, Managed Blockchain, Resource Groups, SageMaker, Support Console, and Workspaces. We are continuing to work on identifying root cause.
 8:52 AM PST -  The Kinesis Data Streams API is severely impaired. This is also impacting other services, including ACM, Amplify Console, API Gateway, AppStream2, AppSync, Athena, CloudFormation, CloudTrail, CloudWatch, Cognito, Connect, DynamoDB, EventBridge, IoT Services, Lambda, LEX, Managed Blockchain, Resource Groups, SageMaker, Support Console, and Workspaces. We are actively working towards resolution.
 9:32 AM PST -  The Kinesis Data Streams API is currently impaired in the US-EAST-1 Region. As a result customers are not able to write or read data published to Kinesis streams.CloudWatch metrics and events are also affected, with elevated PutMetricData API error rates and some delayed metrics.While EC2 instances and connectivity remain healthy, some instances are experiencing delayed instance health metrics, but remain in a healthy state.AutoScaling is also experiencing delays in scaling times due to CloudWatch metric delays.The issue is also affecting other services, including ACM, Amplify Console, API Gateway, AppMesh, AppStream2, AppSync, Athena, Batch, CloudFormation, CloudTrail, Cognito, Connect, DynamoDB, EventBridge, Glue, IoT Services, Lambda, LEX, Managed Blockchain, Marketplace, Personalize, RDS, Resource Groups, SageMaker, Support Console, Well Architected, and Workspaces. For further details on each of these services, please see the Personal Health Dashboard.Other services, like S3, remain unaffected by this event.This issue has also affected our ability to post updates to the Service Health Dashboard.We are continuing to work towards resolution.
11:23 AM PST -  We continue to work towards recovery of the issue affecting the Kinesis Data Streams API in the US-EAST-1 Region. For Kinesis Data Streams, the issue is affecting the subsystem that is responsible for handling incoming requests. The team has identified the root cause and we continue to make progress in addressing the root cause. We are seeing some improvement in error rates, but continue to work towards full resolution.The issue also affects other services, or parts of these services, that utilize Kinesis Data Streams within their workflows. While features of multiple services are impacted, some services have seen broader impact and service-specific impact details are included within Recent Events on the Service Health Dashboard.
 1:59 PM PST -  Kinesis Data Streams API requests are still significantly impaired. We have identified a mitigation for this issue, and are actively working towards resolution.
 2:49 PM PST -  Kinesis Data Streams API requests are still impaired but are starting to see recovery. We continue to actively work towards resolution.
 4:42 PM PST -  Kinesis Data Streams API operations are seeing gradual recovery but customers may continue to experience increased latencies and failure rates. We continue to actively work towards resolution.
 6:32 PM PST -  We have now fully mitigated the impact to the subsystem within Kinesis that is responsible for the processing of incoming requests and are no longer seeing increased error rates or latencies. However, we are not yet taking the full traffic load and are working to relax request throttles on the service. Over the next few hours we expect to relax these throttles to previous levels. We expect customers to begin seeing recovery as these throttles are relaxed over this timeframe.
 8:53 PM PST -  We are continuing to relax the request throttles for Kinesis Data Streams and are gradually increasing the traffic into the service. We have not yet enabled requests to Kinesis Data Streams from VPC Endpoints. The Kinesis Data Streams subsystem continues to operate normally, and we expect incremental recovery over the next few hours.
 9:26 PM PST -  We have now enabled a subset of requests to Kinesis Data Streams using VPC Endpoints.
10:06 PM PST -  We have now enabled all requests to Kinesis Data Streams through Internet-facing endpoints. We are continuing to work to re-enable all requests to Kinesis Data Streams using VPC Endpoints.
11:00 PM PST -  We have now enabled all requests to Kinesis Data Streams through both Internet-facing endpoints and VPC Endpoints.
Nov 26, 12:03 AM PST -  Between 5:15 AM and 11:10 PM PST customers experienced a significant impairment to their Amazon Kinesis Data Streams API operations. We have identified the root cause and have completed immediate actions to prevent recurrence. The issue has been resolved and the service is operating normally."
336,Amazon CloudWatch (N. Virginia),[RESOLVED] API Error Rates and Metric Delays,1606316400,1,,"<div><span class=""yellowfg""> 6:38 AM PST</span>&nbsp;We are investigating increased error rates for CloudWatch PutMetricData API in US-EAST-1 Region.</div><div><span class=""yellowfg""> 8:40 AM PST</span>&nbsp;We can confirm elevated PutMetricData API error rates and some delayed metrics in US-EAST-1 Region. We are actively working to resolve the issue.</div><div><span class=""yellowfg""> 1:49 PM PST</span>&nbsp;We are still seeing elevated PutMetricData API error rates and some delayed metrics in US-EAST-1 Region. We continue to work toward full recovery.</div><div><span class=""yellowfg""> 3:42 PM PST</span>&nbsp;We continue to experience increased error rates impacting the PutMetricData API error rates, and delayed Cloudwatch metrics. We expect to begin seeing recovery once the on-going Kinesis issue is fully resolved. Once we have observed recovery, metrics will backfill if they were queued during the impact</div><div><span class=""yellowfg""> 5:31 PM PST</span>&nbsp;We continue experience API error rates and delayed Cloudwatch metrics. We expect to begin seeing recovery once the on-going Kinesis issue is fully resolved. Once we have observed recovery, metrics will backfill if they were queued during the impact.</div><div><span class=""yellowfg""> 8:18 PM PST</span>&nbsp;We continue to experience API error rates and delayed CloudWatch metrics. We expect to see recovery once the on-going Kinesis issue is fully resolved. Metrics will start to backfill once the Kinesis issue is resolved.</div><div><span class=""yellowfg"">10:44 PM PST</span>&nbsp;We are beginning to see recovery in API error rates and delayed metrics as Kinesis recovers.</div><div><span class=""yellowfg"">10:56 PM PST</span>&nbsp;Between 5:15 AM and 10:31 PM PST, some customers experienced elevated error rates when calling CloudWatch APIs in the US-EAST-1 Region. Metrics were delayed, and CloudWatch alarms on delayed metrics transitioned into the INSUFFICIENT_DATA state. We have resolved the issue. Delayed metrics are in the process of backfilling in CloudWatch console graphs and for API retrieval. The service is operating normally.</div>",cloudwatch-us-east-1,2020-11-25 15:00:00,CloudWatch,N. Virginia,5:15 AM,10:31 PM,PST,1036.0,2020,11,25,2020-11-25,api error rates and metric delays," 6:38 AM PST -  We are investigating increased error rates for CloudWatch PutMetricData API in US-EAST-1 Region.
 8:40 AM PST -  We can confirm elevated PutMetricData API error rates and some delayed metrics in US-EAST-1 Region. We are actively working to resolve the issue.
 1:49 PM PST -  We are still seeing elevated PutMetricData API error rates and some delayed metrics in US-EAST-1 Region. We continue to work toward full recovery.
 3:42 PM PST -  We continue to experience increased error rates impacting the PutMetricData API error rates, and delayed Cloudwatch metrics. We expect to begin seeing recovery once the on-going Kinesis issue is fully resolved. Once we have observed recovery, metrics will backfill if they were queued during the impact
 5:31 PM PST -  We continue experience API error rates and delayed Cloudwatch metrics. We expect to begin seeing recovery once the on-going Kinesis issue is fully resolved. Once we have observed recovery, metrics will backfill if they were queued during the impact.
 8:18 PM PST -  We continue to experience API error rates and delayed CloudWatch metrics. We expect to see recovery once the on-going Kinesis issue is fully resolved. Metrics will start to backfill once the Kinesis issue is resolved.
10:44 PM PST -  We are beginning to see recovery in API error rates and delayed metrics as Kinesis recovers.
10:56 PM PST -  Between 5:15 AM and 10:31 PM PST, some customers experienced elevated error rates when calling CloudWatch APIs in the US-EAST-1 Region. Metrics were delayed, and CloudWatch alarms on delayed metrics transitioned into the INSUFFICIENT_DATA state. We have resolved the issue. Delayed metrics are in the process of backfilling in CloudWatch console graphs and for API retrieval. The service is operating normally."
337,Amazon Cognito (N. Virginia),[RESOLVED] Increased API Error Rates,1606316820,2,,"<div><span class=""yellowfg""> 8:11 AM PST</span>&nbsp;We are investigating an increased API failure rate for Cognito User Pools and Identity Pools operations in the us-east-1 Region.</div><div><span class=""yellowfg""> 8:31 AM PST</span>&nbsp;We are continuing to experience increased API failure rates for Cognito User Pools and Identity Pools operations in the US-EAST-1 Region.</div><div><span class=""yellowfg"">11:14 AM PST</span>&nbsp;We are continuing to experience increased API failure rates for Cognito User Pools and Identity Pools operations in the US-EAST-1 Region. Cognito customers are experiencing errors when authenticating end users or creating new accounts for their Cognito User pools and for obtaining temporary AWS credentials using Cognito Identity pools.</div><div><span class=""yellowfg"">11:54 AM PST</span>&nbsp;We have identified the root cause and continue to work towards recovery of the issue affecting the Cognito User Pools and Identity Pools operations in the US-EAST-1 Region.</div><div><span class=""yellowfg"">12:48 PM PST</span>&nbsp;We continue to work towards recovery of the issue affecting the Cognito User Pools and Identity Pools operations in the US-EAST-1 Region and we see improvement in the error rates experienced by customers.</div><div><span class=""yellowfg""> 1:30 PM PST</span>&nbsp;We continue to work towards recovery of the issue affecting the Cognito User Pools and Identity Pools operations in the US-EAST-1 Region and we see further improvement in the error rates experienced by customers.</div><div><span class=""yellowfg""> 2:23 PM PST</span>&nbsp;We are in the final stages of recovery for Cognito User Pools and customers should be seeing significant improvement. We expect to be fully recovered within 30 minutes.</div><div><span class=""yellowfg""> 2:43 PM PST</span>&nbsp;Between 5:15 AM and 2:28 PM PST customers experienced increased API failure rates for Cognito User Pools and Identity Pools in the US-EAST-1 Region. This was due to an issue with Kinesis Data Streams. We have implemented a mitigation to this issue. Cognito is now operating normally.</div>",cognito-us-east-1,2020-11-25 15:07:00,Cognito,N. Virginia,5:15 AM,2:28 PM,PST,553.0,2020,11,25,2020-11-25,increased api error rates," 8:11 AM PST -  We are investigating an increased API failure rate for Cognito User Pools and Identity Pools operations in the us-east-1 Region.
 8:31 AM PST -  We are continuing to experience increased API failure rates for Cognito User Pools and Identity Pools operations in the US-EAST-1 Region.
11:14 AM PST -  We are continuing to experience increased API failure rates for Cognito User Pools and Identity Pools operations in the US-EAST-1 Region. Cognito customers are experiencing errors when authenticating end users or creating new accounts for their Cognito User pools and for obtaining temporary AWS credentials using Cognito Identity pools.
11:54 AM PST -  We have identified the root cause and continue to work towards recovery of the issue affecting the Cognito User Pools and Identity Pools operations in the US-EAST-1 Region.
12:48 PM PST -  We continue to work towards recovery of the issue affecting the Cognito User Pools and Identity Pools operations in the US-EAST-1 Region and we see improvement in the error rates experienced by customers.
 1:30 PM PST -  We continue to work towards recovery of the issue affecting the Cognito User Pools and Identity Pools operations in the US-EAST-1 Region and we see further improvement in the error rates experienced by customers.
 2:23 PM PST -  We are in the final stages of recovery for Cognito User Pools and customers should be seeing significant improvement. We expect to be fully recovered within 30 minutes.
 2:43 PM PST -  Between 5:15 AM and 2:28 PM PST customers experienced increased API failure rates for Cognito User Pools and Identity Pools in the US-EAST-1 Region. This was due to an issue with Kinesis Data Streams. We have implemented a mitigation to this issue. Cognito is now operating normally."
338,Amazon EventBridge (N. Virginia),[RESOLVED] Increased API error rates and event delivery latencies,1606319040,3,,"<div><span class=""yellowfg""> 6:45 AM PST</span>&nbsp;We are investigating increased API error rates for EventBridge in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 8:37 AM PST</span>&nbsp;We are continuing to investigate increased API error rates for EventBridge in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 9:06 AM PST</span>&nbsp;We can confirm significant API faults and latencies for EventBridge in the US-EAST-1 Region, which is related to an ongoing issue with Kinesis Data Streams. We are continuing to work towards resolution.</div><div><span class=""yellowfg"">10:44 AM PST</span>&nbsp;We can confirm significant API faults and latencies for EventBridge in the US-EAST-1 Region, which is related to an ongoing issue with Kinesis Data Streams. Currently, customer calls to the PutEvents API are failing with 5XX unavailable errors. In addition, rules subscribing to events from AWS services are delayed. We are continuing to work towards resolution.</div><div><span class=""yellowfg""> 1:36 PM PST</span>&nbsp;We have identified the root cause of elevated API errors and latencies in US-EAST-1. While we are seeing partial recovery, we continue to see elevated error rates and latency on API calls to PutEvents. Additionally, Rules subscribing to Events from other AWS Services continue to be delayed as we work toward full resolution.</div><div><span class=""yellowfg""> 3:24 PM PST</span>&nbsp;We have identified the root cause of elevated API errors and latencies in the US-EAST-1 Region and have begun implementing mitigations resulting in partial recovery. We continue to see elevated error rates and latency on API calls to PutEvents. Additionally, for Rules subscribing to Events from other AWS Services, latency is recovering but still elevated. We are continuing to work towards full resolution.</div><div><span class=""yellowfg""> 4:04 PM PST</span>&nbsp;We have identified the root cause of elevated API errors and latencies in the US-EAST-1 Region and have implemented mitigations resulting in significant improvement in error rates and latency on the PutEvents API. We continue to see delays in Rules subscribing to Events from other AWS Services and are working towards full resolution.</div><div><span class=""yellowfg""> 6:04 PM PST</span>&nbsp;Newly published events are no longer experiencing elevated delivery latencies, however, we continue to see delays in Rules subscribing to events from other AWS Services that were delivered earlier in the day and are working towards full resolution.</div><div><span class=""yellowfg""> 7:33 PM PST</span>&nbsp;Newly published events are no longer experiencing elevated delivery latencies. We continue to see delays in Rules subscribing to scheduled events that were delivered earlier in the day and we are working through backlog of these events over the next few hours.</div><div><span class=""yellowfg""> 9:52 PM PST</span>&nbsp;Between 5:15 AM and 9:30 PM PST, we experienced elevated API errors and event delivery latency in the US-EAST-1 Region related to an issue with Kinesis Data Streams. While the issue has been resolved and the service is operating normally, we are continuing to work through the backlog of AWS CloudTrail events which will be delivered over the next several hours.</div>",events-us-east-1,2020-11-25 15:44:00,EventBridge,N. Virginia,5:15 AM,9:30 PM,PST,975.0,2020,11,25,2020-11-25,increased api error rates and event delivery latencies," 6:45 AM PST -  We are investigating increased API error rates for EventBridge in the US-EAST-1 Region.
 8:37 AM PST -  We are continuing to investigate increased API error rates for EventBridge in the US-EAST-1 Region.
 9:06 AM PST -  We can confirm significant API faults and latencies for EventBridge in the US-EAST-1 Region, which is related to an ongoing issue with Kinesis Data Streams. We are continuing to work towards resolution.
10:44 AM PST -  We can confirm significant API faults and latencies for EventBridge in the US-EAST-1 Region, which is related to an ongoing issue with Kinesis Data Streams. Currently, customer calls to the PutEvents API are failing with 5XX unavailable errors. In addition, rules subscribing to events from AWS services are delayed. We are continuing to work towards resolution.
 1:36 PM PST -  We have identified the root cause of elevated API errors and latencies in US-EAST-1. While we are seeing partial recovery, we continue to see elevated error rates and latency on API calls to PutEvents. Additionally, Rules subscribing to Events from other AWS Services continue to be delayed as we work toward full resolution.
 3:24 PM PST -  We have identified the root cause of elevated API errors and latencies in the US-EAST-1 Region and have begun implementing mitigations resulting in partial recovery. We continue to see elevated error rates and latency on API calls to PutEvents. Additionally, for Rules subscribing to Events from other AWS Services, latency is recovering but still elevated. We are continuing to work towards full resolution.
 4:04 PM PST -  We have identified the root cause of elevated API errors and latencies in the US-EAST-1 Region and have implemented mitigations resulting in significant improvement in error rates and latency on the PutEvents API. We continue to see delays in Rules subscribing to Events from other AWS Services and are working towards full resolution.
 6:04 PM PST -  Newly published events are no longer experiencing elevated delivery latencies, however, we continue to see delays in Rules subscribing to events from other AWS Services that were delivered earlier in the day and are working towards full resolution.
 7:33 PM PST -  Newly published events are no longer experiencing elevated delivery latencies. We continue to see delays in Rules subscribing to scheduled events that were delivered earlier in the day and we are working through backlog of these events over the next few hours.
 9:52 PM PST -  Between 5:15 AM and 9:30 PM PST, we experienced elevated API errors and event delivery latency in the US-EAST-1 Region related to an issue with Kinesis Data Streams. While the issue has been resolved and the service is operating normally, we are continuing to work through the backlog of AWS CloudTrail events which will be delivered over the next several hours."
339,Amazon CloudFront,[RESOLVED] Change Propagation and Invalidations Reporting Delay,1606320420,1,,"<div><span class=""yellowfg""> 9:54 AM PST</span>&nbsp;We are investigating longer than usual reporting update delays for change propagation of invalidations and CloudFront configurations. Customer changes are propagating fine across our edge locations but the associated reporting is not getting updated. Also, end-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=""yellowfg"">11:17 AM PST</span>&nbsp;We are investigating longer than usual reporting update delays for change propagation of invalidations and CloudFront configurations. Customer changes are propagating fine across our edge locations but the associated reporting is not getting updated. During this time, CloudFront Access Logs, Metrics and Reporting may also be affected. End-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=""yellowfg""> 1:25 PM PST</span>&nbsp;We are working towards recovery for delays in reporting updates for change propagation of invalidations and CloudFront configurations. Customer changes are propagating fine across our edge locations but the associated reporting is not getting updated. CloudFront Access Logs, Metrics and Reporting may continue to be affected. End-user requests for content from our edge locations are not affected by this issue and are being served normally.</div><div><span class=""yellowfg""> 2:50 PM PST</span>&nbsp;Change propagation of CloudFront configurations and invalidations have recovered and are operating normally. However, CloudFront Access Logs, Metrics and Reporting continue to be affected. End-user requests for content from our Edge Locations are not affected by this issue and are being served normally.</div><div><span class=""yellowfg""> 4:16 PM PST</span>&nbsp;We are still observing partial recovery on Access Logs, Metrics and Reports but intermittent gaps and delays exist. We continue to work toward full resolution. End-user requests for content from our Edge Locations were not affected by this issue and continue to operate normally.</div><div><span class=""yellowfg""> 6:06 PM PST</span>&nbsp;We continue to work toward full resolution, and expect full recovery once the on-going Kinesis issue is resolved. Upon further recovery, we expect Access Logs, Metrics and Reports to fully recover and start backfilling over time for those queued during the impact.</div><div><span class=""yellowfg""> 9:44 PM PST</span>&nbsp;CloudFront Access Logs, Metrics, and Reporting continues to be affected by the Kinesis event but we are observing improving recovery. CloudFront edge locations are serving traffic as expected. Change propagation and cache invalidation times are operating within normal time windows.</div><div><span class=""yellowfg"">11:51 PM PST</span>&nbsp;Between 5:41 AM and 2:40 PM PST, we experienced longer than usual reporting delays for Invalidations and CloudFront configurations to edge locations. Customer changes were propagating normally across our edge locations during this time but the associated reporting was not getting updated correctly. Between 5:41 AM and 11:26 PM PST, CloudFront Real-time Metrics were not available. CloudFront’s Real-time Metrics are now available in CloudWatch. The backlog of CloudFront Access Logs and Reports will be backfilled over the next few hours. During this time, all end-user requests for content from our edge locations were not affected by this issue and were being served normally.</div>",cloudfront,2020-11-25 16:07:00,CloudFront,Global,5:41 AM,11:26 PM,PST,1065.0,2020,11,25,2020-11-25,change propagation and invalidations reporting delay," 9:54 AM PST -  We are investigating longer than usual reporting update delays for change propagation of invalidations and CloudFront configurations. Customer changes are propagating fine across our edge locations but the associated reporting is not getting updated. Also, end-user requests for content from our edge locations are not affected by this issue and are being served normally.
11:17 AM PST -  We are investigating longer than usual reporting update delays for change propagation of invalidations and CloudFront configurations. Customer changes are propagating fine across our edge locations but the associated reporting is not getting updated. During this time, CloudFront Access Logs, Metrics and Reporting may also be affected. End-user requests for content from our edge locations are not affected by this issue and are being served normally.
 1:25 PM PST -  We are working towards recovery for delays in reporting updates for change propagation of invalidations and CloudFront configurations. Customer changes are propagating fine across our edge locations but the associated reporting is not getting updated. CloudFront Access Logs, Metrics and Reporting may continue to be affected. End-user requests for content from our edge locations are not affected by this issue and are being served normally.
 2:50 PM PST -  Change propagation of CloudFront configurations and invalidations have recovered and are operating normally. However, CloudFront Access Logs, Metrics and Reporting continue to be affected. End-user requests for content from our Edge Locations are not affected by this issue and are being served normally.
 4:16 PM PST -  We are still observing partial recovery on Access Logs, Metrics and Reports but intermittent gaps and delays exist. We continue to work toward full resolution. End-user requests for content from our Edge Locations were not affected by this issue and continue to operate normally.
 6:06 PM PST -  We continue to work toward full resolution, and expect full recovery once the on-going Kinesis issue is resolved. Upon further recovery, we expect Access Logs, Metrics and Reports to fully recover and start backfilling over time for those queued during the impact.
 9:44 PM PST -  CloudFront Access Logs, Metrics, and Reporting continues to be affected by the Kinesis event but we are observing improving recovery. CloudFront edge locations are serving traffic as expected. Change propagation and cache invalidation times are operating within normal time windows.
11:51 PM PST -  Between 5:41 AM and 2:40 PM PST, we experienced longer than usual reporting delays for Invalidations and CloudFront configurations to edge locations. Customer changes were propagating normally across our edge locations during this time but the associated reporting was not getting updated correctly. Between 5:41 AM and 11:26 PM PST, CloudFront Real-time Metrics were not available. CloudFront’s Real-time Metrics are now available in CloudWatch. The backlog of CloudFront Access Logs and Reports will be backfilled over the next few hours. During this time, all end-user requests for content from our edge locations were not affected by this issue and were being served normally."
340,Amazon Elastic Kubernetes Service (N. Virginia),[RESOLVED] Increased API Error Rates and Launches,1606324920,2,,"<div><span class=""yellowfg""> 9:22 AM PST</span>&nbsp;We are investigating increased API error rates for cluster and node group operations in the US-EAST-1 region. We are also investigating increased Fargate pod launch failures. Existing EKS clusters and managed node groups are operating normally.</div><div><span class=""yellowfg"">11:03 AM PST</span>&nbsp;Customer’s applications that are backed by pods already running are not impacted. We are continuing to experience API error rates in the US-EAST-1 region. EKS customers are experiencing errors when creating, upgrading and deleting EKS clusters and managed node groups. Existing managed node groups may experience errors scaling up or down. Customers will experience errors launching new Fargate pods.</div><div><span class=""yellowfg""> 1:55 PM PST</span>&nbsp;We continue to work towards recovery of the issue affecting Amazon EKS in the US-EAST-1 Region. Customer applications that are backed by pods already running are not impacted. Also applications and pods can be started and run on EC2 instances that are already part of the cluster. EKS customers are experiencing errors when creating, upgrading and deleting EKS clusters and managed node groups. Customers will experience errors launching new Fargate pods, running Fargate pods are not impacted.</div><div><span class=""yellowfg""> 3:10 PM PST</span>&nbsp;We continue working towards recovery of the issue affecting Amazon EKS in the US-EAST-1 Region. EKS Fargate pod launches are now seeing recovery. Customer applications that are backed by pods already running are not impacted. Applications and pods can be started and run on EC2 instances that are already part of the cluster. Customers can also create Managed Node groups for existing clusters. EKS customers will still experience errors when creating, upgrading and deleting EKS clusters and managed node groups.</div><div><span class=""yellowfg""> 4:29 PM PST</span>&nbsp;We continue to work towards recovery of the issue affecting Amazon EKS in the US-EAST-1 Region. EKS Fargate pod launches have now recovered and is operating normally. Customer applications that are backed by pods already running are not impacted. Applications and pods can be started and run on EC2 instances that are already part of the cluster. Customers can also create Managed Node groups for existing clusters. EKS customers will still experience errors when creating, upgrading and deleting EKS clusters and managed node groups.</div><div><span class=""yellowfg""> 6:11 PM PST</span>&nbsp;We continue to observe partial recovery for EKS cluster and node group API operations in the US-EAST-1 Region and are working toward full resolution. Customers may still experience errors when creating, upgrading and deleting EKS clusters and managed node groups.</div><div><span class=""yellowfg""> 9:48 PM PST</span>&nbsp;We continue to observe partial recovery for EKS cluster and node group API operations in the US-EAST-1 Region and are working towards full resolution. We expect to see complete recovery once the on-going Kinesis issue is fully resolved. Until then, customers may still experience errors when creating, upgrading and deleting EKS clusters and managed node groups.</div><div><span class=""yellowfg"">11:04 PM PST</span>&nbsp;Between 5:15 AM and 10:20 PM PST, we experienced elevated API errors for cluster, node group operations and Fargate pod launches US-EAST-1 Region. Existing clusters and node groups were unaffected during the event. The issue has been resolved and the service is operating normally.</div>",eks-us-east-1,2020-11-25 17:22:00,Elastic Kubernetes Service,N. Virginia,5:15 AM,10:20 PM,PST,1025.0,2020,11,25,2020-11-25,increased api error rates and launches," 9:22 AM PST -  We are investigating increased API error rates for cluster and node group operations in the US-EAST-1 region. We are also investigating increased Fargate pod launch failures. Existing EKS clusters and managed node groups are operating normally.
11:03 AM PST -  Customer’s applications that are backed by pods already running are not impacted. We are continuing to experience API error rates in the US-EAST-1 region. EKS customers are experiencing errors when creating, upgrading and deleting EKS clusters and managed node groups. Existing managed node groups may experience errors scaling up or down. Customers will experience errors launching new Fargate pods.
 1:55 PM PST -  We continue to work towards recovery of the issue affecting Amazon EKS in the US-EAST-1 Region. Customer applications that are backed by pods already running are not impacted. Also applications and pods can be started and run on EC2 instances that are already part of the cluster. EKS customers are experiencing errors when creating, upgrading and deleting EKS clusters and managed node groups. Customers will experience errors launching new Fargate pods, running Fargate pods are not impacted.
 3:10 PM PST -  We continue working towards recovery of the issue affecting Amazon EKS in the US-EAST-1 Region. EKS Fargate pod launches are now seeing recovery. Customer applications that are backed by pods already running are not impacted. Applications and pods can be started and run on EC2 instances that are already part of the cluster. Customers can also create Managed Node groups for existing clusters. EKS customers will still experience errors when creating, upgrading and deleting EKS clusters and managed node groups.
 4:29 PM PST -  We continue to work towards recovery of the issue affecting Amazon EKS in the US-EAST-1 Region. EKS Fargate pod launches have now recovered and is operating normally. Customer applications that are backed by pods already running are not impacted. Applications and pods can be started and run on EC2 instances that are already part of the cluster. Customers can also create Managed Node groups for existing clusters. EKS customers will still experience errors when creating, upgrading and deleting EKS clusters and managed node groups.
 6:11 PM PST -  We continue to observe partial recovery for EKS cluster and node group API operations in the US-EAST-1 Region and are working toward full resolution. Customers may still experience errors when creating, upgrading and deleting EKS clusters and managed node groups.
 9:48 PM PST -  We continue to observe partial recovery for EKS cluster and node group API operations in the US-EAST-1 Region and are working towards full resolution. We expect to see complete recovery once the on-going Kinesis issue is fully resolved. Until then, customers may still experience errors when creating, upgrading and deleting EKS clusters and managed node groups.
11:04 PM PST -  Between 5:15 AM and 10:20 PM PST, we experienced elevated API errors for cluster, node group operations and Fargate pod launches US-EAST-1 Region. Existing clusters and node groups were unaffected during the event. The issue has been resolved and the service is operating normally."
341,Amazon Elastic Container Service (N. Virginia),[RESOLVED] Increased API Error Rates and Launches,1606328375,2,,"<div><span class=""yellowfg"">10:22 AM PST</span>&nbsp;We are investigating increased API error rates and delays delivering task events and metrics in the US-EAST-1 region. We are also investigating increased task launch error rates for the Fargate launch type. Running tasks are not impacted.</div><div><span class=""yellowfg"">11:07 AM PST</span>&nbsp;Currently running ECS tasks are not impacted for either the EC2 or Fargate launch types. We are continuing to experience API error rates and delays delivering task events and metrics in the US-EAST-1 region. ECS clusters are also not able to scale up or down due to task launch errors. Customers are missing metrics and events from their running tasks as ECS Insights is not able to propagate information. Task Set and Capacity Providers are also impacted. Customers using ECS on Fargate are not able to launch new tasks, running Fargate tasks are not impacted.</div><div><span class=""yellowfg""> 1:57 PM PST</span>&nbsp;We continue working towards recovery for the issue impacting Amazon ECS in the US-EAST-1 region. Currently running ECS tasks are not impacted for either the EC2 or Fargate launch types. Customers are experiencing API error rates and delays delivering task events and metrics in the US-EAST-1 region. ECS clusters are also not able to scale up or down due to task launch errors. Customers are missing metrics and events from their running tasks as ECS Insights is not able to propagate information. Task Set and Capacity Providers are also impacted. Customers using ECS on Fargate are not able to launch new tasks, running Fargate tasks are not impacted.</div><div><span class=""yellowfg""> 2:13 PM PST</span>&nbsp;We continue working towards recovery for the issue impacting Amazon ECS in the US-EAST-1 region. Currently running ECS tasks are not impacted for either the EC2 or Fargate launch types. We are seeing increased delivery rates for task events and metrics. ECS clusters are not able to scale up or down due to task launch errors with Task Set and Capacity Providers impacted. Customers using ECS on Fargate are not able to launch new tasks, running Fargate tasks are not impacted.</div><div><span class=""yellowfg""> 3:04 PM PST</span>&nbsp;We continue working towards recovery for the issue impacting Amazon ECS in the US-EAST-1 region. ECS on Fargate task launches are seeing recovery. Currently running ECS tasks are not impacted for either the EC2 or Fargate launch types. We are seeing increased delivery rates for task events and metrics. ECS clusters using Capacity Providers are still seeing impact.</div><div><span class=""yellowfg""> 4:24 PM PST</span>&nbsp;We continue working towards recovery for the issue impacting Amazon ECS in the US-EAST-1 region. Currently running ECS tasks are not impacted for either the EC2 or Fargate launch types. ECS on Fargate task launches are continuing to see recovery. Delivery of task events and metrics is starting to catch up, and API error rates are declining.</div><div><span class=""yellowfg""> 5:36 PM PST</span>&nbsp;We continue working towards recovery for the issue impacting Amazon ECS in the US-EAST-1 region. Currently running ECS tasks are not impacted for either the EC2 or Fargate launch types. ECS on Fargate task launches are continuing to see recovery with a small number of task launches failing. Task event delivery is fully recovered. There continues to be higher than normal latencies for metrics due to continued CloudWatch impact.</div><div><span class=""yellowfg""> 7:16 PM PST</span>&nbsp;We continue working towards recovery for the issue impacting Amazon ECS in the US-EAST-1 region. Currently running ECS tasks are not impacted for either the EC2 or Fargate launch types. ECS on Fargate task launches are continuing to see recovery with a very small number of task launches failing. Task event delivery is fully recovered. We are investigating slow deprovisioning of tasks. Capacity Providers and metrics delivery latency are both impacted until CloudWatch recovers.</div><div><span class=""yellowfg""> 9:06 PM PST</span>&nbsp;ECS is investigating slow deprovisioning of tasks causing tasks to remain in a deactivating or deprovisioning for extended periods of time. For ECS tasks using awsvpc networking mode, including Fargate tasks, this means the ENI associated with the task remains provisioned longer than normal. CloudWatch Container Insights is now correctly showing recent metrics from ECS. CloudWatch metrics for ECS and Capacity Providers are continuing to see impact while we wait for Kinesis and CloudWatch recovery.</div><div><span class=""yellowfg"">10:49 PM PST</span>&nbsp;We are starting to see recovery for CloudWatch metrics for ECS and Capacity Providers. CloudWatch Container Insights is now showing recent metrics from ECS. ECS continues working to resolve slow tasks deprovisioning which causes tasks to remain in a deactivating or deprovisioning states for extended periods of time. For ECS tasks using awsvpc networking mode, including Fargate tasks, this means the ENI associated with the task remains provisioned longer than normal.</div><div><span class=""yellowfg"">Nov 26, 12:11 AM PST</span>&nbsp;CloudWatch metrics for ECS and Capacity Providers have recovered. We continue working to resolve slow task deprovisioning which is causing tasks to remain in a deactivating or deprovisioning state for extended periods of time. For ECS tasks using awsvpc networking mode, including Fargate tasks, this means the ENI associated with the task remains provisioned longer than normal.</div><div><span class=""yellowfg"">Nov 26, 12:29 AM PST</span>&nbsp;CloudWatch metrics for ECS and Capacity Providers have recovered. We expect to see recovery on slow task deprovisioning. which is causing tasks to remain in a deactivating or deprovisioning state for extended periods of time, once CloudMap is fully recovered. For ECS tasks using awsvpc networking mode, including Fargate tasks, this means the ENI associated with the task remains provisioned longer than normal.</div><div><span class=""yellowfg"">Nov 26,  1:17 AM PST</span>&nbsp;Between November 25 5:15 AM and November 26 1:08 AM PST, we experienced elevated API and task launch error rates, delayed metrics impacting Capacity Provider scaling, CloudWatch Container Insights, and CloudWatch metrics for ECS. Running tasks were not impacted. The issue has been resolved and the service is operating normally.</div>",ecs-us-east-1,2020-11-25 18:19:35,Elastic Container Service,N. Virginia,5:15 AM,1:08 AM,PST,1193.0,2020,11,25,2020-11-25,increased api error rates and launches,"10:22 AM PST -  We are investigating increased API error rates and delays delivering task events and metrics in the US-EAST-1 region. We are also investigating increased task launch error rates for the Fargate launch type. Running tasks are not impacted.
11:07 AM PST -  Currently running ECS tasks are not impacted for either the EC2 or Fargate launch types. We are continuing to experience API error rates and delays delivering task events and metrics in the US-EAST-1 region. ECS clusters are also not able to scale up or down due to task launch errors. Customers are missing metrics and events from their running tasks as ECS Insights is not able to propagate information. Task Set and Capacity Providers are also impacted. Customers using ECS on Fargate are not able to launch new tasks, running Fargate tasks are not impacted.
 1:57 PM PST -  We continue working towards recovery for the issue impacting Amazon ECS in the US-EAST-1 region. Currently running ECS tasks are not impacted for either the EC2 or Fargate launch types. Customers are experiencing API error rates and delays delivering task events and metrics in the US-EAST-1 region. ECS clusters are also not able to scale up or down due to task launch errors. Customers are missing metrics and events from their running tasks as ECS Insights is not able to propagate information. Task Set and Capacity Providers are also impacted. Customers using ECS on Fargate are not able to launch new tasks, running Fargate tasks are not impacted.
 2:13 PM PST -  We continue working towards recovery for the issue impacting Amazon ECS in the US-EAST-1 region. Currently running ECS tasks are not impacted for either the EC2 or Fargate launch types. We are seeing increased delivery rates for task events and metrics. ECS clusters are not able to scale up or down due to task launch errors with Task Set and Capacity Providers impacted. Customers using ECS on Fargate are not able to launch new tasks, running Fargate tasks are not impacted.
 3:04 PM PST -  We continue working towards recovery for the issue impacting Amazon ECS in the US-EAST-1 region. ECS on Fargate task launches are seeing recovery. Currently running ECS tasks are not impacted for either the EC2 or Fargate launch types. We are seeing increased delivery rates for task events and metrics. ECS clusters using Capacity Providers are still seeing impact.
 4:24 PM PST -  We continue working towards recovery for the issue impacting Amazon ECS in the US-EAST-1 region. Currently running ECS tasks are not impacted for either the EC2 or Fargate launch types. ECS on Fargate task launches are continuing to see recovery. Delivery of task events and metrics is starting to catch up, and API error rates are declining.
 5:36 PM PST -  We continue working towards recovery for the issue impacting Amazon ECS in the US-EAST-1 region. Currently running ECS tasks are not impacted for either the EC2 or Fargate launch types. ECS on Fargate task launches are continuing to see recovery with a small number of task launches failing. Task event delivery is fully recovered. There continues to be higher than normal latencies for metrics due to continued CloudWatch impact.
 7:16 PM PST -  We continue working towards recovery for the issue impacting Amazon ECS in the US-EAST-1 region. Currently running ECS tasks are not impacted for either the EC2 or Fargate launch types. ECS on Fargate task launches are continuing to see recovery with a very small number of task launches failing. Task event delivery is fully recovered. We are investigating slow deprovisioning of tasks. Capacity Providers and metrics delivery latency are both impacted until CloudWatch recovers.
 9:06 PM PST -  ECS is investigating slow deprovisioning of tasks causing tasks to remain in a deactivating or deprovisioning for extended periods of time. For ECS tasks using awsvpc networking mode, including Fargate tasks, this means the ENI associated with the task remains provisioned longer than normal. CloudWatch Container Insights is now correctly showing recent metrics from ECS. CloudWatch metrics for ECS and Capacity Providers are continuing to see impact while we wait for Kinesis and CloudWatch recovery.
10:49 PM PST -  We are starting to see recovery for CloudWatch metrics for ECS and Capacity Providers. CloudWatch Container Insights is now showing recent metrics from ECS. ECS continues working to resolve slow tasks deprovisioning which causes tasks to remain in a deactivating or deprovisioning states for extended periods of time. For ECS tasks using awsvpc networking mode, including Fargate tasks, this means the ENI associated with the task remains provisioned longer than normal.
Nov 26, 12:11 AM PST -  CloudWatch metrics for ECS and Capacity Providers have recovered. We continue working to resolve slow task deprovisioning which is causing tasks to remain in a deactivating or deprovisioning state for extended periods of time. For ECS tasks using awsvpc networking mode, including Fargate tasks, this means the ENI associated with the task remains provisioned longer than normal.
Nov 26, 12:29 AM PST -  CloudWatch metrics for ECS and Capacity Providers have recovered. We expect to see recovery on slow task deprovisioning. which is causing tasks to remain in a deactivating or deprovisioning state for extended periods of time, once CloudMap is fully recovered. For ECS tasks using awsvpc networking mode, including Fargate tasks, this means the ENI associated with the task remains provisioned longer than normal.
Nov 26,  1:17 AM PST -  Between November 25 5:15 AM and November 26 1:08 AM PST, we experienced elevated API and task launch error rates, delayed metrics impacting Capacity Provider scaling, CloudWatch Container Insights, and CloudWatch metrics for ECS. Running tasks were not impacted. The issue has been resolved and the service is operating normally."
342,AWS IoT Core (N. Virginia),"[RESOLVED] Increased latency and error rates
",1606328375,2,,"<div><span class=""yellowfg"">10:49 AM PST</span>&nbsp;We continue to experience increased latency and API failure rates for Connect, Subscribe, Messaging, and Shadow operations in the US-EAST-1 region. This is also impacting AWS IoT Device Management and AWS IoT Device Defender. We continue to work towards resolution.</div><div><span class=""yellowfg"">11:44 AM PST</span>&nbsp;We are continuing to experience increased latency and failure rates for Connect and Subscribe operations in the US-EAST-1 region. Messaging on existing connections and Shadow operations are now operating normally. We continue to work towards resolution.</div><div><span class=""yellowfg""> 3:11 PM PST</span>&nbsp;We continue to experience increased latency and failure rates for Connect and Subscribe operations in the US-EAST-1 region. Messaging on existing connections and Shadow operations continue to operate normally. We have identified the root cause and continue to work towards resolution.</div><div><span class=""yellowfg""> 4:51 PM PST</span>&nbsp;We continue to experience increased latency and failure rates for Connect and Subscribe operations in the US-EAST-1 region. We expect to begin seeing recovery once the on-going Kinesis issue is fully resolved. Messaging on existing connections and Shadow operations continue to operate normally.</div><div><span class=""yellowfg""> 7:19 PM PST</span>&nbsp;We continue to experience increased latency and failure rates for Connect and Subscribe operations in the US-EAST-1 region. We expect to see recovery once the on-going Kinesis issue is fully resolved. Messaging on existing connections and Shadow operations continue to operate normally.</div><div><span class=""yellowfg""> 9:21 PM PST</span>&nbsp;We are seeing recovery for Connect and Subscribe operations in the US-EAST-1 region. We continue to work towards full resolution. Messaging on existing connections and Shadow operations continue to operate normally.</div><div><span class=""yellowfg"">10:02 PM PST</span>&nbsp;Between 5:15 AM and 9:52 PM PST we experienced increased error rates and latency for Connect, Subscribe, Publish and Shadow operations. The issue has been resolved and the service is operating normally.</div>",awsiot-us-east-1,2020-11-25 18:19:35,IoT Core,N. Virginia,5:15 AM,9:52 PM,PST,997.0,2020,11,25,2020-11-25,increased latency and error rates,"10:49 AM PST -  We continue to experience increased latency and API failure rates for Connect, Subscribe, Messaging, and Shadow operations in the US-EAST-1 region. This is also impacting AWS IoT Device Management and AWS IoT Device Defender. We continue to work towards resolution.
11:44 AM PST -  We are continuing to experience increased latency and failure rates for Connect and Subscribe operations in the US-EAST-1 region. Messaging on existing connections and Shadow operations are now operating normally. We continue to work towards resolution.
 3:11 PM PST -  We continue to experience increased latency and failure rates for Connect and Subscribe operations in the US-EAST-1 region. Messaging on existing connections and Shadow operations continue to operate normally. We have identified the root cause and continue to work towards resolution.
 4:51 PM PST -  We continue to experience increased latency and failure rates for Connect and Subscribe operations in the US-EAST-1 region. We expect to begin seeing recovery once the on-going Kinesis issue is fully resolved. Messaging on existing connections and Shadow operations continue to operate normally.
 7:19 PM PST -  We continue to experience increased latency and failure rates for Connect and Subscribe operations in the US-EAST-1 region. We expect to see recovery once the on-going Kinesis issue is fully resolved. Messaging on existing connections and Shadow operations continue to operate normally.
 9:21 PM PST -  We are seeing recovery for Connect and Subscribe operations in the US-EAST-1 region. We continue to work towards full resolution. Messaging on existing connections and Shadow operations continue to operate normally.
10:02 PM PST -  Between 5:15 AM and 9:52 PM PST we experienced increased error rates and latency for Connect, Subscribe, Publish and Shadow operations. The issue has been resolved and the service is operating normally."
343,AWS IoT SiteWise (N. Virginia),[RESOLVED] Increased Error rates and Latency,1606331100,2,,"<div><span class=""yellowfg"">11:07 AM PST</span>&nbsp;We are continuing to experience elevated error rate on data ingestion and increased computation latency for IoT SiteWise auto-computed aggregates, transforms and metrics in the US-EAST-1 Region. This is also impacting IoT Events and IoT Analytics. We continue to work towards resolution.</div><div><span class=""yellowfg"">12:24 PM PST</span>&nbsp;We are continuing to experience elevated error rates on data ingestion and increased computation latency for IoT SiteWise auto-computed aggregates, transforms and metrics in the US-EAST-1 Region. This is also impacting IoT Events and IoT Analytics. We continue to work towards resolution.</div><div><span class=""yellowfg""> 3:06 PM PST</span>&nbsp;We are beginning to see recovery for data ingestion and increased computation latency for IoT SiteWise auto-computed aggregates, transforms and metrics in the US-EAST-1 Region. This is also impacting IoT Events and IoT Analytics. We continue to work toward full resolution.</div><div><span class=""yellowfg""> 4:50 PM PST</span>&nbsp;We are beginning to see recovery for data ingestion. Access to existing data, transforms, and metrics is unaffected. We are continuing to experience increased computation latency for IoT SiteWise auto-computed aggregates, transforms and metrics in the US-EAST-1 Region. This is also impacting IoT Events and IoT Analytics. We continue to work toward full resolution.</div><div><span class=""yellowfg""> 7:23 PM PST</span>&nbsp;We have seen recovery for data ingestion. Access to existing data, transforms, and metrics is unaffected. We are continuing to experience increased computation latency for IoT SiteWise auto-computed aggregates, transforms and metrics in the US-EAST-1 Region. This is also impacting IoT Events and IoT Analytics. We continue to work toward full resolution.</div><div><span class=""yellowfg"">10:23 PM PST</span>&nbsp;We have seen recovery for data ingestion and the generation of auto-computed aggregates. Access to existing data, transforms, and metrics is unaffected. We are beginning to see recovery in the execution of computations for IoT SiteWise transforms and metrics but are still experiencing computation latency in the US-EAST-1 Region. We continue to work toward full resolution. IoT Events and IoT Analytics have recovered and those services are operating normally.</div><div><span class=""yellowfg"">Nov 26, 12:27 AM PST</span>&nbsp;We are beginning to see recovery in the execution of computations for IoT SiteWise transforms and metrics but are still experiencing computation latency in the US-EAST-1 Region. All other functions of IoT SiteWise including data ingestion and the generation of auto-computed aggregates have recovered and are operating normally. We continue to work toward full resolution.</div><div><span class=""yellowfg"">Nov 26,  3:55 AM PST</span>&nbsp;Between November 25 5:15 AM PST and November 26 3:49 AM PST, we experienced elevated error rates and increased latency on data ingestion, computation of aggregates, transforms, and metrics in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. We are still executing computations for transforms and metrics on data that may have arrived during the impact window. These will appear in customers' accounts as we process them over the next few hours.</div>",iotsitewise-us-east-1,2020-11-25 19:05:00,IoT SiteWise,N. Virginia,5:15 AM,3:49 AM,PST,1354.0,2020,11,25,2020-11-25,increased error rates and latency,"11:07 AM PST -  We are continuing to experience elevated error rate on data ingestion and increased computation latency for IoT SiteWise auto-computed aggregates, transforms and metrics in the US-EAST-1 Region. This is also impacting IoT Events and IoT Analytics. We continue to work towards resolution.
12:24 PM PST -  We are continuing to experience elevated error rates on data ingestion and increased computation latency for IoT SiteWise auto-computed aggregates, transforms and metrics in the US-EAST-1 Region. This is also impacting IoT Events and IoT Analytics. We continue to work towards resolution.
 3:06 PM PST -  We are beginning to see recovery for data ingestion and increased computation latency for IoT SiteWise auto-computed aggregates, transforms and metrics in the US-EAST-1 Region. This is also impacting IoT Events and IoT Analytics. We continue to work toward full resolution.
 4:50 PM PST -  We are beginning to see recovery for data ingestion. Access to existing data, transforms, and metrics is unaffected. We are continuing to experience increased computation latency for IoT SiteWise auto-computed aggregates, transforms and metrics in the US-EAST-1 Region. This is also impacting IoT Events and IoT Analytics. We continue to work toward full resolution.
 7:23 PM PST -  We have seen recovery for data ingestion. Access to existing data, transforms, and metrics is unaffected. We are continuing to experience increased computation latency for IoT SiteWise auto-computed aggregates, transforms and metrics in the US-EAST-1 Region. This is also impacting IoT Events and IoT Analytics. We continue to work toward full resolution.
10:23 PM PST -  We have seen recovery for data ingestion and the generation of auto-computed aggregates. Access to existing data, transforms, and metrics is unaffected. We are beginning to see recovery in the execution of computations for IoT SiteWise transforms and metrics but are still experiencing computation latency in the US-EAST-1 Region. We continue to work toward full resolution. IoT Events and IoT Analytics have recovered and those services are operating normally.
Nov 26, 12:27 AM PST -  We are beginning to see recovery in the execution of computations for IoT SiteWise transforms and metrics but are still experiencing computation latency in the US-EAST-1 Region. All other functions of IoT SiteWise including data ingestion and the generation of auto-computed aggregates have recovered and are operating normally. We continue to work toward full resolution.
Nov 26,  3:55 AM PST -  Between November 25 5:15 AM PST and November 26 3:49 AM PST, we experienced elevated error rates and increased latency on data ingestion, computation of aggregates, transforms, and metrics in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. We are still executing computations for transforms and metrics on data that may have arrived during the impact window. These will appear in customers' accounts as we process them over the next few hours."
344,AWS Batch (N. Virginia),[RESOLVED] Job State Transition Delays,1606331520,2,,"<div><span class=""yellowfg"">11:15 AM PST</span>&nbsp;We are experiencing increased error rates for job state transitions and compute environment scaling in the US-EAST-1 Region.</div><div><span class=""yellowfg"">11:41 AM PST</span>&nbsp;We continue to experience increased error rates and delays for job state transitions and compute environment scaling in the US-EAST-1 Region.</div><div><span class=""yellowfg"">12:58 PM PST</span>&nbsp;We have identified the root cause of increased error rates and delays for job state transitions and compute environment scaling in the US-EAST-1 Region and continue to work toward resolution.</div><div><span class=""yellowfg""> 1:35 PM PST</span>&nbsp;We are no longer seeing compute environment scaling delays but are still experiencing elevated job state transition times in the US-EAST-1 Region and continue to work toward resolution.</div><div><span class=""yellowfg""> 3:23 PM PST</span>&nbsp;We are beginning to see recovery for the elevated job state transition times in the US-EAST-1 Region, and continue to work toward full resolution.</div><div><span class=""yellowfg""> 4:11 PM PST</span>&nbsp;We continue to see some recovery for the elevated job state transition times in the US-EAST-1 Region, and are working toward full resolution.</div><div><span class=""yellowfg""> 6:15 PM PST</span>&nbsp;We continue to see recovery for elevated job state transition times in the US-EAST-1 Region, and are working toward full resolution. Customers can expect jobs to work correctly, but may still see issues with Multi-node Parallel workloads.</div><div><span class=""yellowfg""> 7:23 PM PST</span>&nbsp;We continue to see recovery for elevated job state transition times in the US-EAST-1 Region, and are working toward full resolution. Customers can expect most jobs to work correctly, but may still see Multi-node Parallel job delays.</div><div><span class=""yellowfg""> 9:23 PM PST</span>&nbsp;We have full recovery for AWS Batch jobs not using AWSvpc networking mode. Customers running Multi-Node Parallel jobs may see deprovisioning delays while we wait for ECS recovery.</div><div><span class=""yellowfg"">10:31 PM PST</span>&nbsp;Between 5:17 AM and 7:20 PM PST we experienced delayed job state transitions and compute environment scaling delays in AWS Batch Jobs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",batch-us-east-1,2020-11-25 19:12:00,Batch,N. Virginia,5:17 AM,7:20 PM,PST,843.0,2020,11,25,2020-11-25,job state transition delays,"11:15 AM PST -  We are experiencing increased error rates for job state transitions and compute environment scaling in the US-EAST-1 Region.
11:41 AM PST -  We continue to experience increased error rates and delays for job state transitions and compute environment scaling in the US-EAST-1 Region.
12:58 PM PST -  We have identified the root cause of increased error rates and delays for job state transitions and compute environment scaling in the US-EAST-1 Region and continue to work toward resolution.
 1:35 PM PST -  We are no longer seeing compute environment scaling delays but are still experiencing elevated job state transition times in the US-EAST-1 Region and continue to work toward resolution.
 3:23 PM PST -  We are beginning to see recovery for the elevated job state transition times in the US-EAST-1 Region, and continue to work toward full resolution.
 4:11 PM PST -  We continue to see some recovery for the elevated job state transition times in the US-EAST-1 Region, and are working toward full resolution.
 6:15 PM PST -  We continue to see recovery for elevated job state transition times in the US-EAST-1 Region, and are working toward full resolution. Customers can expect jobs to work correctly, but may still see issues with Multi-node Parallel workloads.
 7:23 PM PST -  We continue to see recovery for elevated job state transition times in the US-EAST-1 Region, and are working toward full resolution. Customers can expect most jobs to work correctly, but may still see Multi-node Parallel job delays.
 9:23 PM PST -  We have full recovery for AWS Batch jobs not using AWSvpc networking mode. Customers running Multi-Node Parallel jobs may see deprovisioning delays while we wait for ECS recovery.
10:31 PM PST -  Between 5:17 AM and 7:20 PM PST we experienced delayed job state transitions and compute environment scaling delays in AWS Batch Jobs in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
345,Amazon WorkSpaces (N. Virginia),[RESOLVED] Increased provisioning error rates,1606334820,1,,"<div><span class=""yellowfg"">12:09 PM PST</span>&nbsp;We are experiencing increased error rates in provisioning of new Amazon WorkSpaces in the US-EAST-1 Region</div><div><span class=""yellowfg""> 2:06 PM PST</span>&nbsp;We are continuing to experience increased error rates in the provisioning of new Amazon WorkSpaces in the US-EAST-1 Region. We recommend customers do not perform modify, migrate, reboot, rebuild and/or restore operations on existing WorkSpaces at this time. We are actively working to resolve the issue.</div><div><span class=""yellowfg""> 4:24 PM PST</span>&nbsp;We continue to work towards recovery of the issue affecting provisioning of Amazon WorkSpaces in the US-EAST-1 Region. We recommend customers do not perform modify, migrate, reboot, rebuild and/or restore operations on existing WorkSpaces at this time.</div><div><span class=""yellowfg""> 7:56 PM PST</span>&nbsp;We have identified the root cause of the issue affecting provisioning of Amazon WorkSpaces in the US-EAST-1 Region. We expect to see recovery once the on-going Kinesis issue is fully resolved. We recommend customers do not perform modify, migrate, reboot, rebuild and/or restore operations on existing WorkSpaces at this time.</div><div><span class=""yellowfg""> 9:22 PM PST</span>&nbsp;We are beginning to see recovery on provisioning of Linux and non-BYOL Windows WorkSpaces in the US-EAST-1 Region. We recommend customers continue to not perform modify, migrate, reboot, rebuild and/or restore operations on all existing WorkSpaces until complete recovery. We continue to work toward full resolution.</div><div><span class=""yellowfg"">11:31 PM PST</span>&nbsp;We are continuing to see recovery on provisioning of Linux and Windows (including BYOL) WorkSpaces in the US-EAST-1 Region. At this time, we expect customers can modify, migrate, reboot, rebuild and restore their existing WorkSpaces normally. We continue to work towards full resolution.</div><div><span class=""yellowfg"">Nov 26, 12:01 AM PST</span>&nbsp;Between 5:15 AM PST and 11:55 PM PST, we experienced increased error rates in provisioning of new Amazon WorkSpaces in the US-EAST-1 Region. The issue has now been resolved and the service is operating normally.</div>",workspaces-us-east-1,2020-11-25 20:07:00,WorkSpaces,N. Virginia,5:15 AM,11:55 PM,PST,1120.0,2020,11,25,2020-11-25,increased provisioning error rates,"12:09 PM PST -  We are experiencing increased error rates in provisioning of new Amazon WorkSpaces in the US-EAST-1 Region
 2:06 PM PST -  We are continuing to experience increased error rates in the provisioning of new Amazon WorkSpaces in the US-EAST-1 Region. We recommend customers do not perform modify, migrate, reboot, rebuild and/or restore operations on existing WorkSpaces at this time. We are actively working to resolve the issue.
 4:24 PM PST -  We continue to work towards recovery of the issue affecting provisioning of Amazon WorkSpaces in the US-EAST-1 Region. We recommend customers do not perform modify, migrate, reboot, rebuild and/or restore operations on existing WorkSpaces at this time.
 7:56 PM PST -  We have identified the root cause of the issue affecting provisioning of Amazon WorkSpaces in the US-EAST-1 Region. We expect to see recovery once the on-going Kinesis issue is fully resolved. We recommend customers do not perform modify, migrate, reboot, rebuild and/or restore operations on existing WorkSpaces at this time.
 9:22 PM PST -  We are beginning to see recovery on provisioning of Linux and non-BYOL Windows WorkSpaces in the US-EAST-1 Region. We recommend customers continue to not perform modify, migrate, reboot, rebuild and/or restore operations on all existing WorkSpaces until complete recovery. We continue to work toward full resolution.
11:31 PM PST -  We are continuing to see recovery on provisioning of Linux and Windows (including BYOL) WorkSpaces in the US-EAST-1 Region. At this time, we expect customers can modify, migrate, reboot, rebuild and restore their existing WorkSpaces normally. We continue to work towards full resolution.
Nov 26, 12:01 AM PST -  Between 5:15 AM PST and 11:55 PM PST, we experienced increased error rates in provisioning of new Amazon WorkSpaces in the US-EAST-1 Region. The issue has now been resolved and the service is operating normally."
346,Amazon CloudWatch (N. Virginia),[RESOLVED] Increased CloudWatch API error rates,1606412297,1,,"<div><span class=""yellowfg""> 9:38 AM PST</span>&nbsp;We are investigating increased error rates with the CloudWatch GetMetricStatistics and GetMetricData APIs when requesting metrics older than 3 hours in the US-EAST-1 Region. CloudWatch alarms may transition into ""INSUFFICIENT_DATA"" state if the period of the alarm is longer than 3 hours. Querying metrics less than 3 hours as well as publishing metrics remains unimpacted.</div><div><span class=""yellowfg"">10:42 AM PST</span>&nbsp;Between 7:59 AM and 10:05 AM PST, we experienced increased error rates with the CloudWatch GetMetricStatistics and GetMetricData APIs when requesting metrics older than 3 hours in the US-EAST-1 Region. CloudWatch alarms may have transitioned into ""INSUFFICIENT_DATA"" state if the period of the alarm was longer than 3 hours. Querying metrics less than 3 hours as well as publishing metrics were unimpacted. The issue has been resolved and the service is operating normally.</div>",cloudwatch-us-east-1,2020-11-26 17:38:17,CloudWatch,N. Virginia,7:59 AM,10:05 AM,PST,126.0,2020,11,26,2020-11-26,increased cloudwatch api error rates," 9:38 AM PST -  We are investigating increased error rates with the CloudWatch GetMetricStatistics and GetMetricData APIs when requesting metrics older than 3 hours in the US-EAST-1 Region. CloudWatch alarms may transition into ""INSUFFICIENT_DATA"" state if the period of the alarm is longer than 3 hours. Querying metrics less than 3 hours as well as publishing metrics remains unimpacted.
10:42 AM PST -  Between 7:59 AM and 10:05 AM PST, we experienced increased error rates with the CloudWatch GetMetricStatistics and GetMetricData APIs when requesting metrics older than 3 hours in the US-EAST-1 Region. CloudWatch alarms may have transitioned into ""INSUFFICIENT_DATA"" state if the period of the alarm was longer than 3 hours. Querying metrics less than 3 hours as well as publishing metrics were unimpacted. The issue has been resolved and the service is operating normally."
347,Amazon Elastic Compute Cloud (Sao Paulo),[RESOLVED] Instances unavailable in a single availability zone ,1607344379,1,,"<div><span class=""yellowfg""> 4:32 AM PST</span>&nbsp;We can confirm connectivity and power issues affecting some instances in a single Availability Zone (sae1-az3) in the SA-EAST-1 Region. We have identified the cause of the issue and are working towards resolution.</div><div><span class=""yellowfg""> 5:03 AM PST</span>&nbsp;At 11:10 PM PST on December 6 a small number of underlying hosts experienced power loss, affecting some customers in sae1-az3 in the SA-EAST-1 Region. Impacted customers were automatically notified via the Personal Health Dashboard. We quickly identified root cause and began working toward recovery. At 2:25 AM PST on December 7 additional hosts were impacted. Shortly after that we began to successfully mitigate impact and as of 4:00 AM PST we have begun to see recovery. We continue to work toward full resolution.</div><div><span class=""yellowfg""> 6:18 AM PST</span>&nbsp;Between December 6, 2020 at 11:10 PM PST and December 7, 2020 at 5:45 AM PST we experienced connectivity and power issues affecting some instances in a single Availability Zone (sae1-az3) in the SA-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",ec2-sa-east-1,2020-12-07 12:32:59,Elastic Compute Cloud,Sao Paulo,11:10 PM,5:45 AM,PST,395.0,2020,12,7,2020-12-07,instances unavailable in a single availability zone," 4:32 AM PST -  We can confirm connectivity and power issues affecting some instances in a single Availability Zone (sae1-az3) in the SA-EAST-1 Region. We have identified the cause of the issue and are working towards resolution.
 5:03 AM PST -  At 11:10 PM PST on December 6 a small number of underlying hosts experienced power loss, affecting some customers in sae1-az3 in the SA-EAST-1 Region. Impacted customers were automatically notified via the Personal Health Dashboard. We quickly identified root cause and began working toward recovery. At 2:25 AM PST on December 7 additional hosts were impacted. Shortly after that we began to successfully mitigate impact and as of 4:00 AM PST we have begun to see recovery. We continue to work toward full resolution.
 6:18 AM PST -  Between December 6, 2020 at 11:10 PM PST and December 7, 2020 at 5:45 AM PST we experienced connectivity and power issues affecting some instances in a single Availability Zone (sae1-az3) in the SA-EAST-1 Region. The issue has been resolved and the service is operating normally."
348,Amazon Elastic Compute Cloud (Oregon),[RESOLVED] API Error Rates and Latencies,1607992032,1,,"<div><span class=""yellowfg""> 4:27 PM PST</span>&nbsp;We are investigating increased error rates and latencies for the EC2 APIs in the US-WEST-2 Region.</div><div><span class=""yellowfg""> 4:40 PM PST</span>&nbsp;We can confirm increased error rates and latencies for the EC2 network-related APIs and elevated launch failures for newly launched instances within the US-WEST-2 Region. We are working to resolve the issue.</div><div><span class=""yellowfg""> 4:46 PM PST</span>&nbsp;We are beginning to see signs of recovery with error rates and latencies returning to normal levels for the EC2 APIs in the US-WEST-2 Region. Launches of new instances are once again succeeding, and we continue to work towards full recovery.</div><div><span class=""yellowfg""> 4:55 PM PST</span>&nbsp;Between 4:05 PM and 4:39 PM PST, we experienced increased error rates and latencies for the EC2 APIs, and elevated launch failures for new EC2 instances in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",ec2-us-west-2,2020-12-15 00:27:12,Elastic Compute Cloud,Oregon,4:05 PM,4:39 PM,PST,34.0,2020,12,15,2020-12-15,api error rates and latencies," 4:27 PM PST -  We are investigating increased error rates and latencies for the EC2 APIs in the US-WEST-2 Region.
 4:40 PM PST -  We can confirm increased error rates and latencies for the EC2 network-related APIs and elevated launch failures for newly launched instances within the US-WEST-2 Region. We are working to resolve the issue.
 4:46 PM PST -  We are beginning to see signs of recovery with error rates and latencies returning to normal levels for the EC2 APIs in the US-WEST-2 Region. Launches of new instances are once again succeeding, and we continue to work towards full recovery.
 4:55 PM PST -  Between 4:05 PM and 4:39 PM PST, we experienced increased error rates and latencies for the EC2 APIs, and elevated launch failures for new EC2 instances in the US-WEST-2 Region. The issue has been resolved and the service is operating normally."
349,Amazon Elastic Load Balancing (Seoul),[RESOLVED] NLB의 네트워크 연결 관련 | Network connectivity for NLB,1608306650,1,,"<div><span class=""yellowfg""> 7:50 AM PST</span>&nbsp;AP-NORTHEAST-2 리전의 단일 가용 영역 (APNE2-AZ1) 내에서 TLS를 사용하는 Network Load Balancer의 네트워크 연결 문제를 조사하고 있습니다. 다수의 가용 영역을 사용하도록 구성된 Network Load Balancer는 이슈의 영향을 받는 가용 영역으로부터 자동으로 장애 조치되어 그 영향을 완화하게 됩니다. 영향을 받은 한개의 가용 영역 만을 사용하도록 구성된 Network Load Balancer에 대해선, 이슈가 완전히 해결 될 때까지 문제를 완화하기 위해 고객이 다른 가용 영역에 대체 Network Load Balancer를 생성 할 수 있습니다. 질문이 있으시거나 운영상의 문제가 발생하는 경우 https://console.aws.amazon.com/support의 AWS 지원 센터를 통해 AWS 기술지원 부서에 문의하십시오. | We are investigating network connectivity issues for Network Load Balancers using TLS within a single Availability Zone (APNE2-AZ1) in the AP-NORTHEAST-2 Region. Network Load Balancers configured for multiple Availability Zones will automatically fail away from the affected Availability Zone, mitigating any impact. For Network Load Balancers configured for only the affected Availability Zone, customers can create a replacement Network Load Balancer in another Availability Zone to mitigate the issue until it is fully resolved. If you have any questions or are experiencing any operational issue, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support</div><div><span class=""yellowfg""> 8:08 AM PST</span>&nbsp;AP-NORTHEAST-2 리전의 단일 가용 영역 (APNE2-AZ1) 내에서 TLS를 사용하는 Network Load Balancer의 네트워크 연결 문제에 대해 복구 징후가 보입니다. 우리는 완전한 복구를 위해 계속 노력하고 있습니다. 질문이 있으시거나 운영상의 문제가 발생하는 경우 https://console.aws.amazon.com/support의 AWS 지원 센터를 통해 AWS 기술지원 부서에 문의하십시오. | We are seeing signs of recovery for the network connectivity issues for Network Load Balancers using TLS within a single Availability Zone (APNE2-AZ1) in the AP-NORTHEAST-2 Region. We continue to work towards full recovery. If you have any questions or are experiencing any operational issue, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support</div><div><span class=""yellowfg""> 8:59 AM PST</span>&nbsp;AP-NORTHEAST-2 리전의 단일 가용 영역 (APNE2-AZ1) 내에서 TLS를 사용하는 Network Load Balancer의 네트워크 연결 문제에 영향을 미치는 문제를 해결했습니다. 태평양 시간 오전12:30 (한국시간 오후5:30) 시부터 AP-NORTHEAST-2 리전 내의 일부 Network Load Balancer에서 네트워크 연결 문제가 발생했습니다. 태평양 시간 7:13시에 (한국시간 오전 12:13)단일 가용 영역 (APNE2-AZ1) 내의 Network Load Balancer에서 TLS 연결에 대한 연결 문제가 발생했습니다. 태평양 시간 오전7:56 시에(한국시간 오전 12:56) AP-NORTHEAST-2리전의 Network Load Balancers와 관련된 모든 연결 문제는 해결 되었으며, 서비스는 정상적으로 운영되고 있습니다. 고객분들은 서비스의 가용률을 복원하기 위한 추가 조치를 취할 필요가 없습니다. 이와 관련하여 질문이나 운영관련 문제를 겪고 있으시다면 https://console.aws.amazon.com/support의 AWS 지원 센터를 통해 AWS 지원 부서에 문의하십시오. | We have resolved the issue affecting network connectivity issues for Network Load Balancers using TLS within a single Availability Zone (APNE2-AZ1) in the AP-NORTHEAST-2 Region. Starting at 12:30 AM PST, some Network Load Balancers within the AP-NORTHEAST-2 Region experienced periods of network connectivity issues. At 7:13 AM, Network Load Balancers within a single Availability Zone (APNE2-AZ1) experienced connectivity issues for TLS connections. At 7:56 AM PST, all connectivity issues for Network Load Balancers within the AP-NORTHEAST-2 Region were resolved. The issue has been resolved and the service is operating normally. Customers do not need to take additional measures to restore the availability of this service. If you have any questions or are experiencing any operational issue, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support .</div>",elb-ap-northeast-2,2020-12-18 15:50:50,Elastic Load Balancing,Seoul,12:30 AM,7:56 AM,PST,446.0,2020,12,18,2020-12-18,nlb    network connectivity for nlb," 7:50 AM PST -  AP-NORTHEAST-2 리전의 단일 가용 영역 (APNE2-AZ1) 내에서 TLS를 사용하는 Network Load Balancer의 네트워크 연결 문제를 조사하고 있습니다. 다수의 가용 영역을 사용하도록 구성된 Network Load Balancer는 이슈의 영향을 받는 가용 영역으로부터 자동으로 장애 조치되어 그 영향을 완화하게 됩니다. 영향을 받은 한개의 가용 영역 만을 사용하도록 구성된 Network Load Balancer에 대해선, 이슈가 완전히 해결 될 때까지 문제를 완화하기 위해 고객이 다른 가용 영역에 대체 Network Load Balancer를 생성 할 수 있습니다. 질문이 있으시거나 운영상의 문제가 발생하는 경우 https://console.aws.amazon.com/support의 AWS 지원 센터를 통해 AWS 기술지원 부서에 문의하십시오. | We are investigating network connectivity issues for Network Load Balancers using TLS within a single Availability Zone (APNE2-AZ1) in the AP-NORTHEAST-2 Region. Network Load Balancers configured for multiple Availability Zones will automatically fail away from the affected Availability Zone, mitigating any impact. For Network Load Balancers configured for only the affected Availability Zone, customers can create a replacement Network Load Balancer in another Availability Zone to mitigate the issue until it is fully resolved. If you have any questions or are experiencing any operational issue, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support
 8:08 AM PST -  AP-NORTHEAST-2 리전의 단일 가용 영역 (APNE2-AZ1) 내에서 TLS를 사용하는 Network Load Balancer의 네트워크 연결 문제에 대해 복구 징후가 보입니다. 우리는 완전한 복구를 위해 계속 노력하고 있습니다. 질문이 있으시거나 운영상의 문제가 발생하는 경우 https://console.aws.amazon.com/support의 AWS 지원 센터를 통해 AWS 기술지원 부서에 문의하십시오. | We are seeing signs of recovery for the network connectivity issues for Network Load Balancers using TLS within a single Availability Zone (APNE2-AZ1) in the AP-NORTHEAST-2 Region. We continue to work towards full recovery. If you have any questions or are experiencing any operational issue, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support
 8:59 AM PST -  AP-NORTHEAST-2 리전의 단일 가용 영역 (APNE2-AZ1) 내에서 TLS를 사용하는 Network Load Balancer의 네트워크 연결 문제에 영향을 미치는 문제를 해결했습니다. 태평양 시간 오전12:30 (한국시간 오후5:30) 시부터 AP-NORTHEAST-2 리전 내의 일부 Network Load Balancer에서 네트워크 연결 문제가 발생했습니다. 태평양 시간 7:13시에 (한국시간 오전 12:13)단일 가용 영역 (APNE2-AZ1) 내의 Network Load Balancer에서 TLS 연결에 대한 연결 문제가 발생했습니다. 태평양 시간 오전7:56 시에(한국시간 오전 12:56) AP-NORTHEAST-2리전의 Network Load Balancers와 관련된 모든 연결 문제는 해결 되었으며, 서비스는 정상적으로 운영되고 있습니다. 고객분들은 서비스의 가용률을 복원하기 위한 추가 조치를 취할 필요가 없습니다. 이와 관련하여 질문이나 운영관련 문제를 겪고 있으시다면 https://console.aws.amazon.com/support의 AWS 지원 센터를 통해 AWS 지원 부서에 문의하십시오. | We have resolved the issue affecting network connectivity issues for Network Load Balancers using TLS within a single Availability Zone (APNE2-AZ1) in the AP-NORTHEAST-2 Region. Starting at 12:30 AM PST, some Network Load Balancers within the AP-NORTHEAST-2 Region experienced periods of network connectivity issues. At 7:13 AM, Network Load Balancers within a single Availability Zone (APNE2-AZ1) experienced connectivity issues for TLS connections. At 7:56 AM PST, all connectivity issues for Network Load Balancers within the AP-NORTHEAST-2 Region were resolved. The issue has been resolved and the service is operating normally. Customers do not need to take additional measures to restore the availability of this service. If you have any questions or are experiencing any operational issue, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support ."
350,AWS NAT Gateway (Seoul),[RESOLVED] 네트워크 연결 | Network connectivity,1608318576,1,,"<div><span class=""yellowfg"">11:09 AM PST</span>&nbsp;태평양시간 오전12:30 (한국시간 오후 5:30) 에서 오전7:10 (한국시간 오전 12:10) 사이에 AP-NORTHEAST-2 리전 내에서 NAT 게이트웨이에 대한 네트워크 연결 문제가 발생합니다. 문제가 해결되었으며 서비스가 정상적으로 작동하고 있습니다. 고객분들은 서비스의 가용률을 복원하기 위한 추가 조치를 취할 필요가 없습니다. 이와 관련하여 질문이나 운영관련 문제를 겪고 있으시다면 https://console.aws.amazon.com/support AWS 지원 센터를 통해 AWS 지원 부서에 문의하십시오. | Between 12:30 AM and 7:10 AM PST we experience periods of network connectivity issues for NAT Gateway within the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally. Customers do not need to take additional measures to restore the availability of this service. If you have any questions or are experiencing any operational issue, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support .</div>",natgateway-ap-northeast-2,2020-12-18 19:09:36,NAT Gateway,Seoul,12:30 AM,7:10 AM,PST,400.0,2020,12,18,2020-12-18,network connectivity,"11:09 AM PST -  태평양시간 오전12:30 (한국시간 오후 5:30) 에서 오전7:10 (한국시간 오전 12:10) 사이에 AP-NORTHEAST-2 리전 내에서 NAT 게이트웨이에 대한 네트워크 연결 문제가 발생합니다. 문제가 해결되었으며 서비스가 정상적으로 작동하고 있습니다. 고객분들은 서비스의 가용률을 복원하기 위한 추가 조치를 취할 필요가 없습니다. 이와 관련하여 질문이나 운영관련 문제를 겪고 있으시다면 https://console.aws.amazon.com/support AWS 지원 센터를 통해 AWS 지원 부서에 문의하십시오. | Between 12:30 AM and 7:10 AM PST we experience periods of network connectivity issues for NAT Gateway within the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally. Customers do not need to take additional measures to restore the availability of this service. If you have any questions or are experiencing any operational issue, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support ."
351,Amazon Elastic Compute Cloud (N. Virginia),[RESOLVED] Traffic Impacted for NAT Gateways,1608329580,1,,"<div><span class=""yellowfg""> 2:13 PM PST</span>&nbsp;We are investigating connectivity issues affecting NAT Gateway in a single Availability Zone (use1-az1) in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 3:16 PM PST</span>&nbsp;We wanted to provide you with some more details on the periods of network connectivity issues for NAT Gateway within a single Availability Zone (use1-az1) within the US-EAST-1 Region. While we are currently not observing any network connectivity issues at this moment, we did observe two periods of impact, the first between 11:30 AM and 12:00 PM PST and the second between 1:00 PM and 1:45 PM PST. We have identified the subsystem that was responsible for this issue but continue to work towards identifying root cause and mitigating any further impact. We will continue to provide updates on our progress towards full recovery, and immediately should the network connectivity issues return.</div><div><span class=""yellowfg""> 3:59 PM PST</span>&nbsp;We continue to investigate root cause of the issue affecting network connectivity for NAT Gateway within a single Availability Zone (use1-az1) in the US-EAST-1 Region. We have not seen impact since 1:45 PM PST and our engineering teams remain engaged as we work to determine root cause, and apply mitigations before resolving this event. Since this issue only affected a single Availability Zone, customers who are able to route network traffic through alternative Availability Zones for the affected services, should consider doing so.</div><div><span class=""yellowfg""> 4:33 PM PST</span>&nbsp;We have not seen any further network connectivity issues for NAT Gateway since 1:45 PM PST. We have also identified the root cause and prepared a fix to address the issue. The engineering team is currently working on testing this fix and preparing for deployment to the affected subset of hosts that were affected by this issue. We will continue to provide updates as we work through that deployment.</div><div><span class=""yellowfg""> 5:55 PM PST</span>&nbsp;We have seen no further network connectivity issues for NAT Gateway since 1:45 PM PST (5 hours ago). We have identified the root cause and prepared a fix to resolve the issue, which is now being deployed. We have also taken steps to mitigate any impact should the issue reoccur before the deployment is fully completed. This issue resulted in two periods of network connectivity issues between 11:30 AM and 12:00 PM PST, and between 1:00 PM and 1:45 PM PST. The issue has been resolved and the service is operating normally.</div>",ec2-us-east-1,2020-12-18 22:13:00,Elastic Compute Cloud,N. Virginia,11:30 AM,1:45 PM,PST,135.0,2020,12,18,2020-12-18,traffic impacted for nat gateways," 2:13 PM PST -  We are investigating connectivity issues affecting NAT Gateway in a single Availability Zone (use1-az1) in the US-EAST-1 Region.
 3:16 PM PST -  We wanted to provide you with some more details on the periods of network connectivity issues for NAT Gateway within a single Availability Zone (use1-az1) within the US-EAST-1 Region. While we are currently not observing any network connectivity issues at this moment, we did observe two periods of impact, the first between 11:30 AM and 12:00 PM PST and the second between 1:00 PM and 1:45 PM PST. We have identified the subsystem that was responsible for this issue but continue to work towards identifying root cause and mitigating any further impact. We will continue to provide updates on our progress towards full recovery, and immediately should the network connectivity issues return.
 3:59 PM PST -  We continue to investigate root cause of the issue affecting network connectivity for NAT Gateway within a single Availability Zone (use1-az1) in the US-EAST-1 Region. We have not seen impact since 1:45 PM PST and our engineering teams remain engaged as we work to determine root cause, and apply mitigations before resolving this event. Since this issue only affected a single Availability Zone, customers who are able to route network traffic through alternative Availability Zones for the affected services, should consider doing so.
 4:33 PM PST -  We have not seen any further network connectivity issues for NAT Gateway since 1:45 PM PST. We have also identified the root cause and prepared a fix to address the issue. The engineering team is currently working on testing this fix and preparing for deployment to the affected subset of hosts that were affected by this issue. We will continue to provide updates as we work through that deployment.
 5:55 PM PST -  We have seen no further network connectivity issues for NAT Gateway since 1:45 PM PST (5 hours ago). We have identified the root cause and prepared a fix to resolve the issue, which is now being deployed. We have also taken steps to mitigate any impact should the issue reoccur before the deployment is fully completed. This issue resulted in two periods of network connectivity issues between 11:30 AM and 12:00 PM PST, and between 1:00 PM and 1:45 PM PST. The issue has been resolved and the service is operating normally."
352,AWS VPCE PrivateLink (N. Virginia),[RESOLVED] Traffic Impacted for VPC Endpoints,1608329794,1,,"<div><span class=""yellowfg""> 2:18 PM PST</span>&nbsp;We are investigating connectivity issues affecting VPCE PrivateLink Endpoints in a single Availability Zone (use1-az1) in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 3:17 PM PST</span>&nbsp;We wanted to provide you with some more details on the periods of network connectivity issues for VPCE Private Link within a single Availability Zone (use1-az1) within the US-EAST-1 Region. While we are currently not observing any network connectivity issues at this moment, we did observe two periods of impact, the first between 11:30 AM and 12:00 PM PST and the second between 1:00 PM and 1:45 PM PST. We have identified the subsystem that was responsible for this issue but continue to work towards identifying root cause and mitigating any further impact. We will continue to provide updates on our progress towards full recovery, and immediately should the network connectivity issues return.</div><div><span class=""yellowfg""> 3:59 PM PST</span>&nbsp;We continue to investigate root cause of the issue affecting network connectivity for PrivateLink within a single Availability Zone (use1-az1) in the US-EAST-1 Region. We have not seen impact since 1:45 PM PST and our engineering teams remain engaged as we work to determine root cause, and apply mitigations before resolving this event. Since this issue only affected a single Availability Zone, customers who are able to route network traffic through alternative Availability Zones for the affected services, should consider doing so.</div><div><span class=""yellowfg""> 4:33 PM PST</span>&nbsp;We have not seen any further network connectivity issues for PrivateLink since 1:45 PM PST. We have also identified the root cause and prepared a fix to address the issue. The engineering team is currently working on testing this fix and preparing for deployment to the affected subset of hosts that were affected by this issue. We will continue to provide updates as we work through that deployment.</div><div><span class=""yellowfg""> 5:37 PM PST</span>&nbsp;We have seen no further network connectivity issues for PrivateLink since 1:45 PM PST (5 hours ago). We have identified the root cause and prepared a fix to resolve the issue, which is now being deployed. We have also taken steps to mitigate any impact should the issue reoccur before the deployment is fully completed. This issue resulted in two periods of network connectivity issues between 11:30 AM and 12:00 PM PST, and between 1:00 PM and 1:45 PM PST. The issue has been resolved and the service is operating normally.</div>",privatelink-us-east-1,2020-12-18 22:16:34,VPCE PrivateLink,N. Virginia,11:30 AM,1:45 PM,PST,135.0,2020,12,18,2020-12-18,traffic impacted for vpc endpoints," 2:18 PM PST -  We are investigating connectivity issues affecting VPCE PrivateLink Endpoints in a single Availability Zone (use1-az1) in the US-EAST-1 Region.
 3:17 PM PST -  We wanted to provide you with some more details on the periods of network connectivity issues for VPCE Private Link within a single Availability Zone (use1-az1) within the US-EAST-1 Region. While we are currently not observing any network connectivity issues at this moment, we did observe two periods of impact, the first between 11:30 AM and 12:00 PM PST and the second between 1:00 PM and 1:45 PM PST. We have identified the subsystem that was responsible for this issue but continue to work towards identifying root cause and mitigating any further impact. We will continue to provide updates on our progress towards full recovery, and immediately should the network connectivity issues return.
 3:59 PM PST -  We continue to investigate root cause of the issue affecting network connectivity for PrivateLink within a single Availability Zone (use1-az1) in the US-EAST-1 Region. We have not seen impact since 1:45 PM PST and our engineering teams remain engaged as we work to determine root cause, and apply mitigations before resolving this event. Since this issue only affected a single Availability Zone, customers who are able to route network traffic through alternative Availability Zones for the affected services, should consider doing so.
 4:33 PM PST -  We have not seen any further network connectivity issues for PrivateLink since 1:45 PM PST. We have also identified the root cause and prepared a fix to address the issue. The engineering team is currently working on testing this fix and preparing for deployment to the affected subset of hosts that were affected by this issue. We will continue to provide updates as we work through that deployment.
 5:37 PM PST -  We have seen no further network connectivity issues for PrivateLink since 1:45 PM PST (5 hours ago). We have identified the root cause and prepared a fix to resolve the issue, which is now being deployed. We have also taken steps to mitigate any impact should the issue reoccur before the deployment is fully completed. This issue resulted in two periods of network connectivity issues between 11:30 AM and 12:00 PM PST, and between 1:00 PM and 1:45 PM PST. The issue has been resolved and the service is operating normally."
353,Amazon Elastic Load Balancing (N. Virginia),[RESOLVED] Traffic impacted for Network Load Balancer ,1608331345,1,,"<div><span class=""yellowfg""> 2:42 PM PST</span>&nbsp;We are investigating connectivity issues affecting Network Load Balancers in a single Availability Zone (use1-az1) in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 3:27 PM PST</span>&nbsp;We wanted to provide you with some more details on the periods of network connectivity issues for Network Load Balancer within a single Availability Zone(use1-az1) within the US-EAST-1 Region. While we are currently not observing any network connectivity issues at this moment, we did observe two periods of impact, the first between 11:30 AM and 12:00 PM PST and the second between 1:00 PM and 1:45 PM PST. We have identified the subsystem that was responsible for this issue but continue to work towards identifying root cause and mitigating any further impact. We will continue to provide updates on our progress towards full recovery, and immediately should the network connectivity issues return.</div><div><span class=""yellowfg""> 4:02 PM PST</span>&nbsp;We continue to investigate root cause of the issue affecting network connectivity for Network Load Balancer within a single Availability Zone (use1-az1) in the US-EAST-1 Region. We have not seen impact since 1:45 PM PST and our engineering teams remain engaged as we work to determine root cause, and apply mitigations before resolving this event. Since this issue only affected a single Availability Zone, customers who are able to route network traffic through alternative Availability Zones for the affected services, should consider doing so.</div><div><span class=""yellowfg""> 4:35 PM PST</span>&nbsp;We have not seen any further network connectivity issues for Network Load Balancer since 1:45 PM PST. We have also identified the root cause and prepared a fix to address the issue. The engineering team is currently working on testing this fix and preparing for deployment to the affected subset of hosts that were affected by this issue. We will continue to provide updates as we work through that deployment.</div><div><span class=""yellowfg""> 5:34 PM PST</span>&nbsp;We have seen no further network connectivity issues for Network Load Balancer since 1:45 PM PST (5 hours ago). We have identified the root cause and prepared a fix to resolve the issue, which is now being deployed. We have also taken steps to mitigate any impact should the issue reoccur before the deployment is fully completed. This issue resulted in two periods of network connectivity issues between 11:30 AM and 12:00 PM PST, and between 1:00 PM and 1:45 PM PST. The issue has been resolved and the service is operating normally.</div>",elb-us-east-1,2020-12-18 22:42:25,Elastic Load Balancing,N. Virginia,11:30 AM,1:45 PM,PST,135.0,2020,12,18,2020-12-18,traffic impacted for network load balancer," 2:42 PM PST -  We are investigating connectivity issues affecting Network Load Balancers in a single Availability Zone (use1-az1) in the US-EAST-1 Region.
 3:27 PM PST -  We wanted to provide you with some more details on the periods of network connectivity issues for Network Load Balancer within a single Availability Zone(use1-az1) within the US-EAST-1 Region. While we are currently not observing any network connectivity issues at this moment, we did observe two periods of impact, the first between 11:30 AM and 12:00 PM PST and the second between 1:00 PM and 1:45 PM PST. We have identified the subsystem that was responsible for this issue but continue to work towards identifying root cause and mitigating any further impact. We will continue to provide updates on our progress towards full recovery, and immediately should the network connectivity issues return.
 4:02 PM PST -  We continue to investigate root cause of the issue affecting network connectivity for Network Load Balancer within a single Availability Zone (use1-az1) in the US-EAST-1 Region. We have not seen impact since 1:45 PM PST and our engineering teams remain engaged as we work to determine root cause, and apply mitigations before resolving this event. Since this issue only affected a single Availability Zone, customers who are able to route network traffic through alternative Availability Zones for the affected services, should consider doing so.
 4:35 PM PST -  We have not seen any further network connectivity issues for Network Load Balancer since 1:45 PM PST. We have also identified the root cause and prepared a fix to address the issue. The engineering team is currently working on testing this fix and preparing for deployment to the affected subset of hosts that were affected by this issue. We will continue to provide updates as we work through that deployment.
 5:34 PM PST -  We have seen no further network connectivity issues for Network Load Balancer since 1:45 PM PST (5 hours ago). We have identified the root cause and prepared a fix to resolve the issue, which is now being deployed. We have also taken steps to mitigate any impact should the issue reoccur before the deployment is fully completed. This issue resulted in two periods of network connectivity issues between 11:30 AM and 12:00 PM PST, and between 1:00 PM and 1:45 PM PST. The issue has been resolved and the service is operating normally."
354,Amazon Kinesis Data Streams (Oregon),[RESOLVED] Increase Error Rate,1610053270,1,,"<div><span class=""yellowfg""> 1:01 PM PST</span>&nbsp;We are investigating increased error rates for the Kinesis in the US-WEST-2 Region.</div><div><span class=""yellowfg""> 1:17 PM PST</span>&nbsp;We are seeing recovery for the increased error rates affecting Kinesis in the US-WEST-2 Region and continue to work towards full resolution.</div><div><span class=""yellowfg""> 1:42 PM PST</span>&nbsp;Between 12:40 PM and 1:07 PM PST, we experienced increased error rates for Kinesis in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",kinesis-us-west-2,2021-01-07 21:01:10,Kinesis Data Streams,Oregon,12:40 PM,1:07 PM,PST,27.0,2021,1,7,2021-01-07,increase error rate," 1:01 PM PST -  We are investigating increased error rates for the Kinesis in the US-WEST-2 Region.
 1:17 PM PST -  We are seeing recovery for the increased error rates affecting Kinesis in the US-WEST-2 Region and continue to work towards full resolution.
 1:42 PM PST -  Between 12:40 PM and 1:07 PM PST, we experienced increased error rates for Kinesis in the US-WEST-2 Region. The issue has been resolved and the service is operating normally."
355,Amazon EventBridge (Oregon),[RESOLVED] Increased API Errors and Event Delivery Latencies,1610053516,1,,"<div><span class=""yellowfg""> 1:05 PM PST</span>&nbsp;We are experiencing elevated API errors and event delivery latencies in the US-WEST-2 Region.</div><div><span class=""yellowfg""> 1:16 PM PST</span>&nbsp;We are seeing recovery for the increased error rates affecting EventBridge in the US-WEST-2 Region and continue to work towards full resolution.</div><div><span class=""yellowfg""> 1:46 PM PST</span>&nbsp;Between 12:40 PM and 1:07 PM PST, we experienced elevated API errors and event delivery latencies in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>",events-us-west-2,2021-01-07 21:05:16,EventBridge,Oregon,12:40 PM,1:07 PM,PST,27.0,2021,1,7,2021-01-07,increased api errors and event delivery latencies," 1:05 PM PST -  We are experiencing elevated API errors and event delivery latencies in the US-WEST-2 Region.
 1:16 PM PST -  We are seeing recovery for the increased error rates affecting EventBridge in the US-WEST-2 Region and continue to work towards full resolution.
 1:46 PM PST -  Between 12:40 PM and 1:07 PM PST, we experienced elevated API errors and event delivery latencies in the US-WEST-2 Region. The issue has been resolved and the service is operating normally."
356,Amazon CloudWatch (Oregon),[RESOLVED] Increased Error Rates and Latencies,1610054463,1,,"<div><span class=""yellowfg""> 1:21 PM PST</span>&nbsp;We are investigating increased error rates and latencies for CloudWatch Logs APIs in the US-WEST-2 Region. Log events and metrics may be delayed. Alarms may transition into ""INSUFFICIENT_DATA"" state if set on delayed metrics.</div><div><span class=""yellowfg""> 1:42 PM PST</span>&nbsp;Between 12:41 PM and 1:07 PM PST, we experienced elevated error rates when calling CloudWatch Logs APIs in the US-WEST-2 Region. Some log events and metrics were delayed, and CloudWatch alarms on delayed metrics transitioned into INSUFFICIENT_DATA state. We have resolved the issue. Delayed metrics and log events are in the process of backfilling in CloudWatch console graphs and for API retrieval. The issue is resolved and the service is operating normally.</div>",cloudwatch-us-west-2,2021-01-07 21:21:03,CloudWatch,Oregon,12:41 PM,1:07 PM,PST,26.0,2021,1,7,2021-01-07,increased error rates and latencies," 1:21 PM PST -  We are investigating increased error rates and latencies for CloudWatch Logs APIs in the US-WEST-2 Region. Log events and metrics may be delayed. Alarms may transition into ""INSUFFICIENT_DATA"" state if set on delayed metrics.
 1:42 PM PST -  Between 12:41 PM and 1:07 PM PST, we experienced elevated error rates when calling CloudWatch Logs APIs in the US-WEST-2 Region. Some log events and metrics were delayed, and CloudWatch alarms on delayed metrics transitioned into INSUFFICIENT_DATA state. We have resolved the issue. Delayed metrics and log events are in the process of backfilling in CloudWatch console graphs and for API retrieval. The issue is resolved and the service is operating normally."
357,Amazon Personalize (N. Virginia),[RESOLVED] Increased API Faults,1611523748,1,,"<div><span class=""yellowfg""> 1:29 PM PST</span>&nbsp;We are currently experiencing increased error rates in the GetRecommendations API in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 1:46 PM PST</span>&nbsp;Between 12:36 PM and 1:42 PM PST, we experienced increased error rates in the GetRecommendations API in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",personalize-us-east-1,2021-01-24 21:29:08,Personalize,N. Virginia,12:36 PM,1:42 PM,PST,66.0,2021,1,24,2021-01-24,increased api faults," 1:29 PM PST -  We are currently experiencing increased error rates in the GetRecommendations API in the US-EAST-1 Region.
 1:46 PM PST -  Between 12:36 PM and 1:42 PM PST, we experienced increased error rates in the GetRecommendations API in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
358,AWS Internet Connectivity (N. Virginia),[RESOLVED] External Network Connectivity Issues,1611681635,1,,"<div><span class=""yellowfg""> 9:20 AM PST</span>&nbsp;We are investigating connectivity issues with an internet provider, mainly affecting the East Coast of the United States, outside of the AWS Network. We are investigating the issue with the external provider.</div><div><span class=""yellowfg"">10:05 AM PST</span>&nbsp;

We are starting to see recovery for the connectivity issues from end customers in the East Coast of the United States to AWS services. Connectivity to instances and services within the Region are not impacted by the event. Internet traffic to/from other external providers is also not impacted. All AWS services are operating normally at this time.
</div><div><span class=""yellowfg"">10:14 AM PST</span>&nbsp;Between 8:26 AM and 9:46 AM PST, some customers experienced connectivity issues from the East Coast of the United States to AWS services. Connectivity to instances and services within the Region were not impacted by the event. Internet traffic to/from other external providers was also not impacted. All AWS services continue to operate normally. </div>",internetconnectivity-us-east-1,2021-01-26 17:20:35,Internet Connectivity,N. Virginia,8:26 AM,9:46 AM,PST,80.0,2021,1,26,2021-01-26,external network connectivity issues," 9:20 AM PST -  We are investigating connectivity issues with an internet provider, mainly affecting the East Coast of the United States, outside of the AWS Network. We are investigating the issue with the external provider.
10:05 AM PST -  We are starting to see recovery for the connectivity issues from end customers in the East Coast of the United States to AWS services. Connectivity to instances and services within the Region are not impacted by the event. Internet traffic to/from other external providers is also not impacted. All AWS services are operating normally at this time.
10:14 AM PST -  Between 8:26 AM and 9:46 AM PST, some customers experienced connectivity issues from the East Coast of the United States to AWS services. Connectivity to instances and services within the Region were not impacted by the event. Internet traffic to/from other external providers was also not impacted. All AWS services continue to operate normally. "
359,Amazon Elastic Compute Cloud (Seoul),[RESOLVED] 인스턴스 기동실패 | Elevated Launch Failures,1611812973,1,,"<div><span class=""yellowfg""> 9:49 PM PST</span>&nbsp;AP-NORTHEAST-2 리전의 단일 가용 영역 (APNE2-AZ3)에서 발생하고 있는 EC2 인스턴스의 기동 실패와 VPC 업데이트의 네트워크 전파 시간 지연에 대해 조사하고 있습니다. | We are investigating elevated launch failures for EC2 instances and delayed network propagation times for VPC update in a single Availability Zone (APNE2-AZ3) in the AP-NORTHEAST-2 Region.</div><div><span class=""yellowfg""> 9:55 PM PST</span>&nbsp;AP-NORTHEAST-2 리전의 단일 가용 영역 (APNE2-AZ3)에서 새로운 EC2 인스턴스 시작 및 VPC에서의 네트워크 전파 시간에 영향을 미치는 문제에 대한 복구 징후가 보입니다. AWS는 문제 해결을 위해 계속 노력하고 있습니다. | We are seeing signs of recovery for the issue affecting new EC2 instance launches and network propagation times for VPC in a single Availability Zone (APNE2-AZ3) in the AP-NORTHEAST-2 Region. We continue to work on resolving the issues.</div><div><span class=""yellowfg"">10:37 PM PST</span>&nbsp;태평양 표준시 (PST) 기준으로 오후 8시 13 분에서 오후 8시 33 분 사이에 AP-NORTHEAST-2 리전의 단일 가용 영역 (APNE2-AZ3)에서 EC2 인스턴스에 대한 기동 실패가 발생했습니다. 인스턴스 기동 실패는 PST기준 오후 8시 33 분에 복구되었지만, 영향을 받은 가용 영역 내에서 새로 시작된 EC2 인스턴스에 대한 VPC 네트워크 구성 전파 시간이 계속 지연되었습니다. PST 기준 오후 9시 46 분에 VPC 네트워크 구성 전파가 정상 수준으로 돌아 왔습니다. 발생했던 문제는 해결되었으며 현재 서비스가 정상적으로 작동하고 있습니다. 이 서비스의 가용성을 복원하기 위해 고객이 추가 조치를 취하실 필요는 없습니다. 혹시라도 질문이 있거나 AWS 서비스 운영에 문제가 있는 경우 AWS 지원 센터 (https://console.aws.amazon.com/support)를 통해 AWS 서포트팀에 문의하시기 바랍니다. | Between 8:13 PM and 8:33 PM PST, we experience elevated launch failures for EC2 instances in a single Availability Zone (APNE2-AZ3) in the AP-NORTHEAST-2 Region. Although the launch failures recovered at 8:33 PM PST, we continued to experience delayed propagation times of VPC network configuration for newly launched EC2 instances within the affected Availability Zone. By 9:46 PM PST, VPC network configuration propagation had returned to normal levels. The issue has been resolved and the service is operating normally. Customers do not need to take additional measures to restore the availability of this service. If you have any questions or are experiencing any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support</div>",ec2-ap-northeast-2,2021-01-28 05:49:33,Elastic Compute Cloud,Seoul,8:13 PM,8:33 PM,PST,20.0,2021,1,28,2021-01-28,elevated launch failures," 9:49 PM PST -  AP-NORTHEAST-2 리전의 단일 가용 영역 (APNE2-AZ3)에서 발생하고 있는 EC2 인스턴스의 기동 실패와 VPC 업데이트의 네트워크 전파 시간 지연에 대해 조사하고 있습니다. | We are investigating elevated launch failures for EC2 instances and delayed network propagation times for VPC update in a single Availability Zone (APNE2-AZ3) in the AP-NORTHEAST-2 Region.
 9:55 PM PST -  AP-NORTHEAST-2 리전의 단일 가용 영역 (APNE2-AZ3)에서 새로운 EC2 인스턴스 시작 및 VPC에서의 네트워크 전파 시간에 영향을 미치는 문제에 대한 복구 징후가 보입니다. AWS는 문제 해결을 위해 계속 노력하고 있습니다. | We are seeing signs of recovery for the issue affecting new EC2 instance launches and network propagation times for VPC in a single Availability Zone (APNE2-AZ3) in the AP-NORTHEAST-2 Region. We continue to work on resolving the issues.
10:37 PM PST -  태평양 표준시 (PST) 기준으로 오후 8시 13 분에서 오후 8시 33 분 사이에 AP-NORTHEAST-2 리전의 단일 가용 영역 (APNE2-AZ3)에서 EC2 인스턴스에 대한 기동 실패가 발생했습니다. 인스턴스 기동 실패는 PST기준 오후 8시 33 분에 복구되었지만, 영향을 받은 가용 영역 내에서 새로 시작된 EC2 인스턴스에 대한 VPC 네트워크 구성 전파 시간이 계속 지연되었습니다. PST 기준 오후 9시 46 분에 VPC 네트워크 구성 전파가 정상 수준으로 돌아 왔습니다. 발생했던 문제는 해결되었으며 현재 서비스가 정상적으로 작동하고 있습니다. 이 서비스의 가용성을 복원하기 위해 고객이 추가 조치를 취하실 필요는 없습니다. 혹시라도 질문이 있거나 AWS 서비스 운영에 문제가 있는 경우 AWS 지원 센터 (https://console.aws.amazon.com/support)를 통해 AWS 서포트팀에 문의하시기 바랍니다. | Between 8:13 PM and 8:33 PM PST, we experience elevated launch failures for EC2 instances in a single Availability Zone (APNE2-AZ3) in the AP-NORTHEAST-2 Region. Although the launch failures recovered at 8:33 PM PST, we continued to experience delayed propagation times of VPC network configuration for newly launched EC2 instances within the affected Availability Zone. By 9:46 PM PST, VPC network configuration propagation had returned to normal levels. The issue has been resolved and the service is operating normally. Customers do not need to take additional measures to restore the availability of this service. If you have any questions or are experiencing any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support"
360,AWS CloudFormation (Tokyo),[RESOLVED] Increased Error Rates and Latencies,1611869244,1,,"<div><span class=""yellowfg""> 1:27 PM PST</span>&nbsp;AP-NORTHEAST-1 リージョンにおける AWS CloudFormation スタックを作成、更新、削除する際のエラーレートおよびレイテンシーの増加について調査中です。| We are investigating increased error rates and latencies when creating, updating, and deleting AWS CloudFormation stacks in the AP-NORTHEAST-1 Region.</div><div><span class=""yellowfg""> 1:48 PM PST</span>&nbsp;AP-NORTHEAST-1 リージョンで AWS CloudFormation スタックを作成、更新、削除するときのエラーレートおよびレイテンシーの増加について確認いたしました。現在解決に向けて対応しております。We can confirm increased error rates and latencies when creating, updating, and deleting AWS CloudFormation stacks in the AP-NORTHEAST-1 Region and are actively working toward resolution.</div><div><span class=""yellowfg""> 2:02 PM PST</span>&nbsp;現在本問題の回復を確認しており、現在解決に向けて引き続き対応しております。| We are beginning to see recovery and continue to work toward full resolution.</div><div><span class=""yellowfg""> 2:10 PM PST</span>&nbsp;日本時間 1/29 午前 4:25 から 6:55 にかけて、AP-NORTHEAST-1 リージョンで AWS CloudFormation スタック作成、更新、削除のエラーレートおよびレイテンシーが増加しました。問題は解決され、サービスは正常に動作しています。 | Between 11:25 AM and 1:55 PM PST we experienced increased error rates and latencies creating, updating, and deleting AWS CloudFormation stacks in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally.</div>",cloudformation-ap-northeast-1,2021-01-28 21:27:24,CloudFormation,Tokyo,11:25 AM,1:55 PM,PST,150.0,2021,1,28,2021-01-28,increased error rates and latencies," 1:27 PM PST -  AP-NORTHEAST-1 リージョンにおける AWS CloudFormation スタックを作成、更新、削除する際のエラーレートおよびレイテンシーの増加について調査中です。| We are investigating increased error rates and latencies when creating, updating, and deleting AWS CloudFormation stacks in the AP-NORTHEAST-1 Region.
 1:48 PM PST -  AP-NORTHEAST-1 リージョンで AWS CloudFormation スタックを作成、更新、削除するときのエラーレートおよびレイテンシーの増加について確認いたしました。現在解決に向けて対応しております。We can confirm increased error rates and latencies when creating, updating, and deleting AWS CloudFormation stacks in the AP-NORTHEAST-1 Region and are actively working toward resolution.
 2:02 PM PST -  現在本問題の回復を確認しており、現在解決に向けて引き続き対応しております。| We are beginning to see recovery and continue to work toward full resolution.
 2:10 PM PST -  日本時間 1/29 午前 4:25 から 6:55 にかけて、AP-NORTHEAST-1 リージョンで AWS CloudFormation スタック作成、更新、削除のエラーレートおよびレイテンシーが増加しました。問題は解決され、サービスは正常に動作しています。 | Between 11:25 AM and 1:55 PM PST we experienced increased error rates and latencies creating, updating, and deleting AWS CloudFormation stacks in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally."
361,Amazon Elastic Compute Cloud (London),[RESOLVED] Instance Connectivity,1612182273,1,,"<div><span class=""yellowfg""> 4:24 AM PST</span>&nbsp;We are experiencing instance connectivity issues in a single Availability Zone (euw2-az3) in the EU-WEST-2 Region.</div><div><span class=""yellowfg""> 5:34 AM PST</span>&nbsp;We're continuing to address connectivity issues to impacted instances in a single Availability Zone (euw2-az3) in the EU-WEST-2 Region. Services that use EC2 and EBS including Elastic Load Balancing, EFS, Elasticache, ECS Fargate, Sagemaker and ELB, are also impacted. </div><div><span class=""yellowfg""> 6:08 AM PST</span>&nbsp;We're continuing to address connectivity issues to impacted instances in a single Availability Zone (euw2-az3) in the EU-WEST-2 Region. We are experiencing elevated error rates for new instance launches in the impacted Availability Zone. Services that use EC2 and EBS including Connect, EFS, Elasticache, ECS Fargate, Managed Streaming for Apache Kafka and Sagemaker are also impacted.</div><div><span class=""yellowfg""> 6:50 AM PST</span>&nbsp;We're continuing to address connectivity issues to impacted instances in a single Availability Zone (euw2-az3) in the EU-WEST-2 Region. We are starting to see recovery for new instance launches in the impacted Availability Zone. Services that use EC2 and EBS including EFS, Elasticache, ECS Fargate, Managed Streaming for Kafka, EMR and Sagemaker are also impacted. Between 3:02 AM and 5:20 AM PST, Amazon Connect customers experienced degraded call and Chat connectivity and handling for agents in the EU-WEST-2 Region. Amazon Connect issues have been resolved and the service is operating normally.</div><div><span class=""yellowfg""> 7:41 AM PST</span>&nbsp;We're continuing to address connectivity issues to impacted instances in a single Availability Zone (euw2-az3) in the EU-WEST-2 Region. The elevated error rates that impacted some new instance launches in the affected Availability Zone have fully recovered.</div><div><span class=""yellowfg""> 8:29 AM PST</span>&nbsp;We have resolved the connectivity issues for the vast majority of affected instances in a single Availability Zone (euw2-az3) in the EU-WEST-2 Region. We are also seeing recovery for the vast majority of EBS volumes experiencing degraded performance due to this event. Services that use EC2 and EBS including RDS, EFS, Elasticache, ECS Fargate, Managed Streaming for Apache Kafka, EMR and Sagemaker are also seeing recovery. We continue to work towards full recovery for the remaining instances and volumes.</div><div><span class=""yellowfg""> 8:53 AM PST</span>&nbsp;Starting at 2:57 AM PST we experienced power and network connectivity issues for some instances, and degraded performance for some EBS volumes in the affected Availability Zone (euw2-az3). By 3:59 AM PST, power and networking connectivity had been restored to the majority of affected instances and, by 7:32 AM PST, degraded performance for the majority of affected EBS volumes had been resolved. Since the beginning of the impact, we have been working to recover the remaining instances and volumes. A small number of remaining instances and volumes are hosted on hardware which was adversely affected by the event. We continue to work to recover all affected instances and volumes and have opened notifications for the remaining impacted customers via the Personal Health Dashboard. For immediate recovery, we recommend replacing any remaining affected instances or volumes if possible.</div>",ec2-eu-west-2,2021-02-01 12:24:33,Elastic Compute Cloud,London,3:02 AM,5:20 AM,PST,138.0,2021,2,1,2021-02-01,instance connectivity," 4:24 AM PST -  We are experiencing instance connectivity issues in a single Availability Zone (euw2-az3) in the EU-WEST-2 Region.
 5:34 AM PST -  We're continuing to address connectivity issues to impacted instances in a single Availability Zone (euw2-az3) in the EU-WEST-2 Region. Services that use EC2 and EBS including Elastic Load Balancing, EFS, Elasticache, ECS Fargate, Sagemaker and ELB, are also impacted. 
 6:08 AM PST -  We're continuing to address connectivity issues to impacted instances in a single Availability Zone (euw2-az3) in the EU-WEST-2 Region. We are experiencing elevated error rates for new instance launches in the impacted Availability Zone. Services that use EC2 and EBS including Connect, EFS, Elasticache, ECS Fargate, Managed Streaming for Apache Kafka and Sagemaker are also impacted.
 6:50 AM PST -  We're continuing to address connectivity issues to impacted instances in a single Availability Zone (euw2-az3) in the EU-WEST-2 Region. We are starting to see recovery for new instance launches in the impacted Availability Zone. Services that use EC2 and EBS including EFS, Elasticache, ECS Fargate, Managed Streaming for Kafka, EMR and Sagemaker are also impacted. Between 3:02 AM and 5:20 AM PST, Amazon Connect customers experienced degraded call and Chat connectivity and handling for agents in the EU-WEST-2 Region. Amazon Connect issues have been resolved and the service is operating normally.
 7:41 AM PST -  We're continuing to address connectivity issues to impacted instances in a single Availability Zone (euw2-az3) in the EU-WEST-2 Region. The elevated error rates that impacted some new instance launches in the affected Availability Zone have fully recovered.
 8:29 AM PST -  We have resolved the connectivity issues for the vast majority of affected instances in a single Availability Zone (euw2-az3) in the EU-WEST-2 Region. We are also seeing recovery for the vast majority of EBS volumes experiencing degraded performance due to this event. Services that use EC2 and EBS including RDS, EFS, Elasticache, ECS Fargate, Managed Streaming for Apache Kafka, EMR and Sagemaker are also seeing recovery. We continue to work towards full recovery for the remaining instances and volumes.
 8:53 AM PST -  Starting at 2:57 AM PST we experienced power and network connectivity issues for some instances, and degraded performance for some EBS volumes in the affected Availability Zone (euw2-az3). By 3:59 AM PST, power and networking connectivity had been restored to the majority of affected instances and, by 7:32 AM PST, degraded performance for the majority of affected EBS volumes had been resolved. Since the beginning of the impact, we have been working to recover the remaining instances and volumes. A small number of remaining instances and volumes are hosted on hardware which was adversely affected by the event. We continue to work to recover all affected instances and volumes and have opened notifications for the remaining impacted customers via the Personal Health Dashboard. For immediate recovery, we recommend replacing any remaining affected instances or volumes if possible."
362,AWS IoT Core (N. Virginia),[RESOLVED] Increased API Error Rates and Latency,1612294997,1,,"<div><span class=""yellowfg"">11:43 AM PST</span>&nbsp;We are investigating increased error rates and latency for the Identity, Registry, and Rules Engine APIs in the US-EAST-1 Region. Device connectivity, messaging and rules evaluations are unaffected.</div><div><span class=""yellowfg"">11:59 AM PST</span>&nbsp;Between 10:34 AM and 11:46 AM PST we experienced increased error rates and latency for the Identity, Registry and Rules Engine APIs in the US-EAST-1 Region. Device connectivity, messaging and rules evaluations remained unaffected during the event. The issue has been resolved and the service is operating normally.</div>",awsiot-us-east-1,2021-02-02 19:43:17,IoT Core,N. Virginia,10:34 AM,11:46 AM,PST,72.0,2021,2,2,2021-02-02,increased api error rates and latency,"11:43 AM PST -  We are investigating increased error rates and latency for the Identity, Registry, and Rules Engine APIs in the US-EAST-1 Region. Device connectivity, messaging and rules evaluations are unaffected.
11:59 AM PST -  Between 10:34 AM and 11:46 AM PST we experienced increased error rates and latency for the Identity, Registry and Rules Engine APIs in the US-EAST-1 Region. Device connectivity, messaging and rules evaluations remained unaffected during the event. The issue has been resolved and the service is operating normally."
363,AWS CloudFormation (N. Virginia),"[RESOLVED] Increased Stack Create, Delete and Update Times",1612437136,1,,"<div><span class=""yellowfg""> 3:12 AM PST</span>&nbsp;We are investigating increased latencies when creating, updating, and deleting AWS CloudFormation stacks in the US-EAST-1 Region and are actively working to resolve the issue.</div><div><span class=""yellowfg""> 3:56 AM PST</span>&nbsp;We can confirm increased latencies for creating and deleting stacks in the US-EAST-1 Region and continue to work towards resolution.</div><div><span class=""yellowfg""> 4:16 AM PST</span>&nbsp;Between 2:03 AM and 4:03 AM PST we experienced increased latencies for stack creation and deletion in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>",cloudformation-us-east-1,2021-02-04 11:12:16,CloudFormation,N. Virginia,2:03 AM,4:03 AM,PST,120.0,2021,2,4,2021-02-04,"increased stack create, delete and update times"," 3:12 AM PST -  We are investigating increased latencies when creating, updating, and deleting AWS CloudFormation stacks in the US-EAST-1 Region and are actively working to resolve the issue.
 3:56 AM PST -  We can confirm increased latencies for creating and deleting stacks in the US-EAST-1 Region and continue to work towards resolution.
 4:16 AM PST -  Between 2:03 AM and 4:03 AM PST we experienced increased latencies for stack creation and deletion in the US-EAST-1 Region. The issue has been resolved and the service is operating normally."
364,Amazon Elastic Load Balancing (N. Virginia),[RESOLVED] Connectivity Issues for Network Load Balancers,1612988354,1,,"<div><span class=""yellowfg"">12:19 PM PST</span>&nbsp;We are investigating increased connectivity issues for Network Load Balancers within the US-EAST-1 Region. </div><div><span class=""yellowfg"">12:45 PM PST</span>&nbsp;We continue to work to identify the root cause of the event resulting in connectivity issues for some Network Load Balancers within the US-EAST-1 Region. Some Network Load Balancers in USE1-AZ2, USE1-AZ4, and USE1-AZ5 are experiencing connection issues for TLS (SSL) requests. Non-TLS (SSL) requests are not affected by this issue. We are also experiencing some delays in the provisioning of new Network Load Balancers and registration of new targets behind existing Network Load Balancers. We have identified the underlying subsystems responsible for this issue and continue to work towards root cause and resolution.</div><div><span class=""yellowfg""> 1:12 PM PST</span>&nbsp;We have identified root cause and are now working to test a fix for the issue affecting connectivity for Networking Load Balancers in the US-EAST-1 Region. Once we have confirmed that it resolves the issue, we will update all affected Network Load Balancers. The issue continues to affect TLS (SSL) connections for Network Load Balancers within the USE1-AZ2, USE1-AZ4, and USE1-AZ5 Availability Zones. The issue does not affect other Availability Zones in the Region and is not expected to affect any additional Network Load Balancers that are currently operating normally. Affected customers can create new Network Load Balancers (with or without TLS) in Availability Zones that are not affected by this issue for immediate mitigation.</div><div><span class=""yellowfg""> 1:37 PM PST</span>&nbsp;We have confirmed that the fix to resolve the issue is working as expected and are currently deploying it to the affected Network Load Balancers. We will be deploying one Availability Zone at a time, in the following order: USE1-AZ4, USE1-AZ2, and USE1-AZ5. We expect to see recovery as we progress through the affected Availability Zones.</div><div><span class=""yellowfg""> 2:03 PM PST</span>&nbsp;We have made steady progress and are close to completing the rollout of the fix in the USE1-AZ4 Availability Zone. Most of the affected Networking Load Balancers in that Availability Zone are now operating normally. We’ll continue at this pace through the remaining Network Load Balancers in USE1-AZ4, followed by the Network Load Balancers in USE1-AZ2 and USE1-AZ5 Availability Zones. Some customers have reported impact for PrivateLink endpoints, which is related to this issue and will see recovery as we continue to deploy the fix.
</div><div><span class=""yellowfg""> 2:43 PM PST</span>&nbsp;We continue to make progress in deploying the fix to Network Load Balancers in the USE1-AZ4 Availability Zone. While the vast majority of the affected Network Load Balancers in that Availability Zone have now recovered, we are making slower progress than expected on the final cell within the Availability Zone. We are working to resolve that issue before moving onto the remaining Availability Zones. We’ll continue to keep you updated on our progress.</div><div><span class=""yellowfg""> 3:27 PM PST</span>&nbsp;We have resolved the issue that affected our deployment of the fix to the final cell in the USE1-AZ4 Availability Zone, and continue to deploy to Availability Zones USE1-AZ2 and USE1-AZ5. We have taken steps to accelerate recovery but are also being cautious to not apply the fix to more than one Availability Zone at a time out of an abundance of caution. For customers that have seen their Network Load Balancer recover, we do not expect any further impact. For customers that are still waiting for recovery, we expect to see that in the coming hour or two as we work through the remaining Availability Zone. For immediate mitigation, customers can create a new Network Load Balancer in the unaffected Availability Zones and register the existing back-end targets with it. We’ll continue to provide updates as we progress through the remaining Availability Zones.</div><div><span class=""yellowfg""> 4:03 PM PST</span>&nbsp;We continue to make progress and are now seeing recovery for affected Network Load Balancers in the USE1-AZ2 Availability Zone. We will continue on the remainder of that Availability Zone before completing USE1-AZ5, the final Availability Zone. Again, for Network Load Balancers that have recovered, we do not expect further impact.</div><div><span class=""yellowfg""> 4:28 PM PST</span>&nbsp;We have completed applying the fix in both the USE1-AZ4 and USE1-AZ2 Availability Zones. We have started the process of updating the final Availability Zone, USE1-AZ5. We expect to see full recovery within the next 30 minutes. </div><div><span class=""yellowfg""> 4:38 PM PST</span>&nbsp;We have completed the update in all affected Availability Zones and all Network Load Balancers are now operating normally. We are working through a backlog of Network Load Balancer created and back-end instance registrations, but at this stage all operations should be operating normally. We will continue to monitor recovery and post an update shortly. </div><div><span class=""yellowfg""> 5:07 PM PST</span>&nbsp;Starting at 11:24 AM PST, some load balancers in the USE1-AZ2, USE1-AZ4, and USE1-AZ5 Availability Zones experienced connectivity issues when terminating TLS (SSL) connections in the US-EAST-1 Region. Engineers worked to identify the root cause of the event, ultimately deploying an update to recover USE1-AZ4 at 1:50 PM PST, USE1-AZ2 at 4:26 PM PST and USE1-AZ5 at 4:31 PM PST. Network Load Balancers not terminating TLS (SSL) connections were not affected by this event. Some PrivateLink endpoints were also affected by the issue. The issue has been resolved and the service is operating normally. </div>",elb-us-east-1,2021-02-10 20:19:14,Elastic Load Balancing,N. Virginia,11:24 AM,4:31 PM,PST,307.0,2021,2,10,2021-02-10,connectivity issues for network load balancers,"12:19 PM PST -  We are investigating increased connectivity issues for Network Load Balancers within the US-EAST-1 Region. 
12:45 PM PST -  We continue to work to identify the root cause of the event resulting in connectivity issues for some Network Load Balancers within the US-EAST-1 Region. Some Network Load Balancers in USE1-AZ2, USE1-AZ4, and USE1-AZ5 are experiencing connection issues for TLS (SSL) requests. Non-TLS (SSL) requests are not affected by this issue. We are also experiencing some delays in the provisioning of new Network Load Balancers and registration of new targets behind existing Network Load Balancers. We have identified the underlying subsystems responsible for this issue and continue to work towards root cause and resolution.
 1:12 PM PST -  We have identified root cause and are now working to test a fix for the issue affecting connectivity for Networking Load Balancers in the US-EAST-1 Region. Once we have confirmed that it resolves the issue, we will update all affected Network Load Balancers. The issue continues to affect TLS (SSL) connections for Network Load Balancers within the USE1-AZ2, USE1-AZ4, and USE1-AZ5 Availability Zones. The issue does not affect other Availability Zones in the Region and is not expected to affect any additional Network Load Balancers that are currently operating normally. Affected customers can create new Network Load Balancers (with or without TLS) in Availability Zones that are not affected by this issue for immediate mitigation.
 1:37 PM PST -  We have confirmed that the fix to resolve the issue is working as expected and are currently deploying it to the affected Network Load Balancers. We will be deploying one Availability Zone at a time, in the following order: USE1-AZ4, USE1-AZ2, and USE1-AZ5. We expect to see recovery as we progress through the affected Availability Zones.
 2:03 PM PST -  We have made steady progress and are close to completing the rollout of the fix in the USE1-AZ4 Availability Zone. Most of the affected Networking Load Balancers in that Availability Zone are now operating normally. We’ll continue at this pace through the remaining Network Load Balancers in USE1-AZ4, followed by the Network Load Balancers in USE1-AZ2 and USE1-AZ5 Availability Zones. Some customers have reported impact for PrivateLink endpoints, which is related to this issue and will see recovery as we continue to deploy the fix.
 2:43 PM PST -  We continue to make progress in deploying the fix to Network Load Balancers in the USE1-AZ4 Availability Zone. While the vast majority of the affected Network Load Balancers in that Availability Zone have now recovered, we are making slower progress than expected on the final cell within the Availability Zone. We are working to resolve that issue before moving onto the remaining Availability Zones. We’ll continue to keep you updated on our progress.
 3:27 PM PST -  We have resolved the issue that affected our deployment of the fix to the final cell in the USE1-AZ4 Availability Zone, and continue to deploy to Availability Zones USE1-AZ2 and USE1-AZ5. We have taken steps to accelerate recovery but are also being cautious to not apply the fix to more than one Availability Zone at a time out of an abundance of caution. For customers that have seen their Network Load Balancer recover, we do not expect any further impact. For customers that are still waiting for recovery, we expect to see that in the coming hour or two as we work through the remaining Availability Zone. For immediate mitigation, customers can create a new Network Load Balancer in the unaffected Availability Zones and register the existing back-end targets with it. We’ll continue to provide updates as we progress through the remaining Availability Zones.
 4:03 PM PST -  We continue to make progress and are now seeing recovery for affected Network Load Balancers in the USE1-AZ2 Availability Zone. We will continue on the remainder of that Availability Zone before completing USE1-AZ5, the final Availability Zone. Again, for Network Load Balancers that have recovered, we do not expect further impact.
 4:28 PM PST -  We have completed applying the fix in both the USE1-AZ4 and USE1-AZ2 Availability Zones. We have started the process of updating the final Availability Zone, USE1-AZ5. We expect to see full recovery within the next 30 minutes. 
 4:38 PM PST -  We have completed the update in all affected Availability Zones and all Network Load Balancers are now operating normally. We are working through a backlog of Network Load Balancer created and back-end instance registrations, but at this stage all operations should be operating normally. We will continue to monitor recovery and post an update shortly. 
 5:07 PM PST -  Starting at 11:24 AM PST, some load balancers in the USE1-AZ2, USE1-AZ4, and USE1-AZ5 Availability Zones experienced connectivity issues when terminating TLS (SSL) connections in the US-EAST-1 Region. Engineers worked to identify the root cause of the event, ultimately deploying an update to recover USE1-AZ4 at 1:50 PM PST, USE1-AZ2 at 4:26 PM PST and USE1-AZ5 at 4:31 PM PST. Network Load Balancers not terminating TLS (SSL) connections were not affected by this event. Some PrivateLink endpoints were also affected by the issue. The issue has been resolved and the service is operating normally. "
365,Amazon Virtual Private Cloud (N. Virginia),[RESOLVED] Increased Console Error Rates,1613516078,1,,"<div><span class=""yellowfg""> 2:54 PM PST</span>&nbsp;We are investigating increased error rates displaying Virtual Private Cloud (VPC) in the VPC Management Console in the US-EAST-1 Region. This issue only affects the new VPC Management Console experience, so switching back to the previous VPC Management Console experience (top left toggle switch) will resolve the issue. The VPC Command Line Tools and APIs are not affected by this issue. We are working to resolve the issue.</div><div><span class=""yellowfg""> 3:10 PM PST</span>&nbsp;We have resolved the issue causing increased error rates displaying Virtual Private Cloud (VPC) in the VPC Management Console in the US-EAST-1 Region. This issue only affected the new experience of the VPC Management Console. The issue has been resolved and the service is operating normally.</div>",vpc-us-east-1,2021-02-16 22:54:38,Virtual Private Cloud,N. Virginia,2:54 PM,3:10 PM,PST,16.0,2021,2,16,2021-02-16,increased console error rates," 2:54 PM PST -  We are investigating increased error rates displaying Virtual Private Cloud (VPC) in the VPC Management Console in the US-EAST-1 Region. This issue only affects the new VPC Management Console experience, so switching back to the previous VPC Management Console experience (top left toggle switch) will resolve the issue. The VPC Command Line Tools and APIs are not affected by this issue. We are working to resolve the issue.
 3:10 PM PST -  We have resolved the issue causing increased error rates displaying Virtual Private Cloud (VPC) in the VPC Management Console in the US-EAST-1 Region. This issue only affected the new experience of the VPC Management Console. The issue has been resolved and the service is operating normally."
366,Amazon Elastic Compute Cloud (Tokyo),[RESOLVED] インスタンスの障害について | Instance impairments,1613747375,1,,"<div><span class=""yellowfg""> 7:09 AM PST</span>&nbsp;(12:09AM JST)現在、東京リージョン AP-NORTHEAST-1 のひとつのアベイラビリティゾーン apne1-az1 において、インスタンスに影響を及ぼす接続性の問題が発生しており、対応を行っております。 |  We are investigating connectivity issues affecting instances in a single Availability Zone (apne1-az1) in the AP-NORTHEAST-1 Region.</div><div><span class=""yellowfg""> 7:58 AM PST</span>&nbsp;(12:58AM JST)現在、東京リージョン AP-NORTHEAST-1 における一つのアベイラビリティゾーン（apne1-az1）の一部で、周囲の温度が上昇している状況を確認いたしました。影響を受けているアベイラビリティーゾーンの一部 EC2 インスタンスでは、接続性の問題または温度上昇の影響に伴い、電源が切れている問題が発生しております。当該問題の影響により、一部 EBS ボリュームにてパフォーマンスが低下しております。本問題の根本原因を特定し、現在解決に向けて対応しております。東京リージョン AP-NORTHEAST-1 におけるその他アベイラビリティゾーンは、この問題の影響を受けておりません。 |  We can confirm that a small area of a single Availability Zone (apne1-az1) is experiencing an increase in ambient temperature in the AP-NORTHEAST-1 Region. Some EC2 instances within the affected section of the Availability Zone have experienced connectivity issues or have powered down as a result of the increasing temperatures. Some EBS volumes are also experiencing degraded performance as a result of the event. We have identified the root cause of the issue and are working towards resolution. Other Availability Zones within the AP-NORTHEAST-1 Region are not affected by this event.</div><div><span class=""yellowfg""> 8:40 AM PST</span>&nbsp;(1:40AM JST)AP-NORTHEAST-1 リージョンのうちの 1 つのアベイラビリティーゾーン (apne1-az1) のある一部の区画での温度上昇に対処するために引き続き取り組んでいます。温度の上昇は、当該セクション内の冷却システムへの電力の損失によって発生しました。引き続き、電源の回復に取り組んでおりこれまでに冷却システムの 1つを正常に復旧させました。引き続き温度を通常レベルに復元し、影響を受けた EC2 インスタンスと EBS ボリュームの回復に取り組んでまいります。EC2 および EBS API を含むその他のシステムは、影響を受けたアベイラビリティーゾーン内で正常に動作しています。影響のあった EC2 インスタンスおよび EBS ボリュームをお持ちのお客様は、影響を受けたアベイラビリティーゾーン、または AP-NORTHEAST-1 リージョン内のその別のアベイラビリティーゾーンで再起動を試みることができます。 | We continue to work on addressing the increase in ambient temperature affecting a small section of a single Availability Zone (apne1-az1) in the AP-NORTHEAST-1 region. The increase in temperature is caused by a loss of power to the cooling systems within the affected section of the Availability Zone. We are working to restore power and have successfully brought online one of the cooling systems. We continue to work on restoring temperatures to normal levels and then recovering affecting EC2 instances and EBS volumes. Other systems, including EC2 and EBS APIs, are operating normally within the affected Availability Zone. Customers with affected EC2 instances and EBS volumes can attempt to relaunch in the affected Availability Zone, or another Availability Zone within the AP-NORTHEAST-1 Region. </div><div><span class=""yellowfg""> 9:43 AM PST</span>&nbsp;(2:43AM JST)AP-NORTHEAST-1 リージョンのうちの 1 つのアベイラビリティーゾーン (apne1-az1) のある一部の区画での温度上昇に対処するために引き続き取り組んでいます。温度の上昇は当該セクション内の冷却装置への電力損失によって発生しました。当該セクション内のいくつかの冷却ユニットの電力はすでに復元しており、温度が低下し始めていることを確認しております。残りのオフラインの冷却ユニットは引き続き作業を続け、温度を通常レベルに戻します。温度が回復次第、影響を受ける EC2 インスタンスと EBS ボリュームが回復します。EC2 および EBS API を含むその他のシステムは、影響を受けるアベイラビリティーゾーン内で正常に動作しています。影響を受けた EC2 インスタンスおよび EBS ボリュームをお持ちのお客様は、影響を受けたアベイラビリティーゾーン、または AP-NORTHEAST-1 リージョン内の別のアベイラビリティーゾーンでインスタンスの再作成を試みることができます。| We continue to work on addressing the increase in ambient temperature affecting a small section of a single Availability Zone (apne1-az1) in the AP-NORTHEAST-1 region. The increase in temperature is caused by a loss of power to the cooling units within the affected section of the Availability Zone. We have now restored power to a number of the cooling units within this section of the Availability Zone and are starting to see temperatures decreasing. We will continue to work through the remaining cooling units that are still offline, which will return temperatures to normal levels. Once temperatures have recovered, we would expect to see affected EC2 instances and EBS volumes begin to recover. Other systems, including EC2 and EBS APIs, are operating normally within the affected Availability Zone. Customers with affected EC2 instances and EBS volumes can attempt to relaunch in the affected Availability Zone, or another Availability Zone within the AP-NORTHEAST-1 Region.</div><div><span class=""yellowfg"">10:42 AM PST</span>&nbsp;(3:42AM JST)AP-NORTHEAST-1 リージョンのうちの 1 つのアベイラビリティーゾーン (apne1-az1) のある一部の区画で影響を受けていた冷却ユニットの多くの電源が回復しました。室温は通常のレベルに近い状況まで戻り、ネットワーク、EC2 および EBS ボリュームの回復処理を開始しています。ネットワークはすでに回復し、EC2とEBSボリューム の回復処理に着手しております。回復処理が始まると再起動が発生するため、お客様にはお使いのインスタンスでアクションをとっていただく場合がございます。EBSボリュームに関しましては、ボリュームが回復するにつれ、degraded I/Oパフォーマンスが通常に戻ります。 ”stopping” もしくは ”shutting-down” のまま止まってしまっているインスタンスに関しましては、回復処理が進むにつれ、 ”stopped” もしくは “terminated” に戻ります。| We have now restored power to the majority of the cooling units within the affected section of the Availability Zone (apne1-az1) in the AP-NORTHEAST-1 Region. Temperatures are now close to normal levels and we have begun the process of restoring networking, EC2 instances and EBS volumes. The network has been restored within the affected section of the Availability Zone and we are now working on EC2 instances and EBS volumes. As they begin to recover, customers may need to take action on their instance as it will have experienced a reboot. For EBS volumes, degraded I/O performance will return to normal levels as volumes recover. For instances that are stuck “stopping” or “shutting-down”, these will return to the “stopped” or “terminated” state as recovery proceeds.
</div><div><span class=""yellowfg"">11:26 AM PST</span>&nbsp;(4:26AM JST) AP-NORTHEAST-1 リージョンのうちの 1 つのアベイラビリティーゾーン (apne1-az1) で影響を受けていた冷却サブシステムの電源が回復しました。現在、室温は通常レベルで運用されています。大部分の ES2 インスタンスと EBS ボリュームが復旧しておりますが、残りのインスタンスとボリュームの復旧作業に引き続き取り組んでいます。| We have now restored power to the cooling subsystem within the affected section of the Availability Zone (apne1-az1) in the AP-NORTHEAST-1 Region. Temperatures are now operating at normal levels. We are also seeing recovery for the majority of EC2 instances and EBS volumes and continue to work on the remaining instance and volumes.</div><div><span class=""yellowfg"">12:09 PM PST</span>&nbsp;(5:09AM JST)アベイラビリティゾーン (apne1-az1) で影響を受けた一部の区画の室温は安定し、通常のレベルに戻りました。多くの EC2インスタンスは回復済みとなっております。多くの EBSボリュームも回復済みですが、残りの少数のボリュームの復旧作業に引き続き取り組んでおります。| Temperatures within the affected section of the Availability Zone (apne1-az1) remain stable and at normal levels. We have now recovered the vast majority of EC2 instances. The majority of EBS volumes have also recovered but there are a few that have required some engineering intervention that we are working on.</div><div><span class=""yellowfg"">12:54 PM PST</span>&nbsp;(5:54AM JST)日本時間 02/19 11:01 PM から、AP-NORTHEAST-1 リージョンのうちの１つのアベイラビリティーゾーンの一部の区画で室温の上昇を確認いたしました。日本時間 02/19 11:03 PM から、室温が上昇した結果として、一部の EC2インスタンスが影響を受け、一部のEBSボリュームではパフォーマンスが低下しました。根本的な原因は、影響を受けたアベイラビリティーゾーンのセクション内の冷却システムへの電力の喪失であり、すでに回復済みです。日本時間 02/20 03:30 AM までに、電力は冷却システム内のほとんどのユニットで復旧し、室温は通常のレベルに戻りました。日本時間 02/20 04:00 AM までに、EC2 インスタンスと EBS ボリュームの回復が始まり、日本時間 02/20 05:30 AM 時点で、影響を受けた EC2 インスタンスと EBS ボリュームの大部分は通常通り動作しております。一部のインスタンスとボリュームは、イベントによって影響を受けたハードウェア上でホストされていました。引き続き影響を受けたすべてのインスタンスとボリュームの復旧に取り組み、Personal Health Dashboard を通じて、現在も影響を受けているお客様に対し通知を行います。即時の復旧が必要な場合は、影響を受けているインスタンスまたはボリュームを置き換えていただくことをお勧めします。| Starting at 6:01 AM PST, we experienced an increase in ambient temperatures within a section of a single Availability Zone within the AP-NORTHEAST-1 Region. Starting at 6:03 AM PST, some EC2 instances were impaired and some EBS volumes experienced degraded performance as a result of the increase in temperature. The root cause was a loss of power to the cooling system within a section of the affected Availability Zone, which engineers worked to restore. By 10:30 AM PST, power had been restored to the majority of the units within the cooling system and temperatures were returning to normal levels. By 11:00 AM PST, EC2 instances and EBS volumes had begun to recover and by 12:30 PM PST, the vast majority of affected EC2 instances and EBS volumes were operating normally. A small number of remaining instances and volumes are hosted on hardware which was adversely affected by the event. We continue to work to recover all affected instances and volumes and have opened notifications for the remaining impacted customers via the Personal Health Dashboard. For immediate recovery, we recommend replacing any remaining affected instances or volumes, if possible.</div>",ec2-ap-northeast-1,2021-02-19 15:09:35,Elastic Compute Cloud,Tokyo,6:03 AM,12:30 PM,PST,387.0,2021,2,19,2021-02-19,instance impairments," 7:09 AM PST -  (12:09AM JST)現在、東京リージョン AP-NORTHEAST-1 のひとつのアベイラビリティゾーン apne1-az1 において、インスタンスに影響を及ぼす接続性の問題が発生しており、対応を行っております。 |  We are investigating connectivity issues affecting instances in a single Availability Zone (apne1-az1) in the AP-NORTHEAST-1 Region.
 7:58 AM PST -  (12:58AM JST)現在、東京リージョン AP-NORTHEAST-1 における一つのアベイラビリティゾーン（apne1-az1）の一部で、周囲の温度が上昇している状況を確認いたしました。影響を受けているアベイラビリティーゾーンの一部 EC2 インスタンスでは、接続性の問題または温度上昇の影響に伴い、電源が切れている問題が発生しております。当該問題の影響により、一部 EBS ボリュームにてパフォーマンスが低下しております。本問題の根本原因を特定し、現在解決に向けて対応しております。東京リージョン AP-NORTHEAST-1 におけるその他アベイラビリティゾーンは、この問題の影響を受けておりません。 |  We can confirm that a small area of a single Availability Zone (apne1-az1) is experiencing an increase in ambient temperature in the AP-NORTHEAST-1 Region. Some EC2 instances within the affected section of the Availability Zone have experienced connectivity issues or have powered down as a result of the increasing temperatures. Some EBS volumes are also experiencing degraded performance as a result of the event. We have identified the root cause of the issue and are working towards resolution. Other Availability Zones within the AP-NORTHEAST-1 Region are not affected by this event.
 8:40 AM PST -  (1:40AM JST)AP-NORTHEAST-1 リージョンのうちの 1 つのアベイラビリティーゾーン (apne1-az1) のある一部の区画での温度上昇に対処するために引き続き取り組んでいます。温度の上昇は、当該セクション内の冷却システムへの電力の損失によって発生しました。引き続き、電源の回復に取り組んでおりこれまでに冷却システムの 1つを正常に復旧させました。引き続き温度を通常レベルに復元し、影響を受けた EC2 インスタンスと EBS ボリュームの回復に取り組んでまいります。EC2 および EBS API を含むその他のシステムは、影響を受けたアベイラビリティーゾーン内で正常に動作しています。影響のあった EC2 インスタンスおよび EBS ボリュームをお持ちのお客様は、影響を受けたアベイラビリティーゾーン、または AP-NORTHEAST-1 リージョン内のその別のアベイラビリティーゾーンで再起動を試みることができます。 | We continue to work on addressing the increase in ambient temperature affecting a small section of a single Availability Zone (apne1-az1) in the AP-NORTHEAST-1 region. The increase in temperature is caused by a loss of power to the cooling systems within the affected section of the Availability Zone. We are working to restore power and have successfully brought online one of the cooling systems. We continue to work on restoring temperatures to normal levels and then recovering affecting EC2 instances and EBS volumes. Other systems, including EC2 and EBS APIs, are operating normally within the affected Availability Zone. Customers with affected EC2 instances and EBS volumes can attempt to relaunch in the affected Availability Zone, or another Availability Zone within the AP-NORTHEAST-1 Region. 
 9:43 AM PST -  (2:43AM JST)AP-NORTHEAST-1 リージョンのうちの 1 つのアベイラビリティーゾーン (apne1-az1) のある一部の区画での温度上昇に対処するために引き続き取り組んでいます。温度の上昇は当該セクション内の冷却装置への電力損失によって発生しました。当該セクション内のいくつかの冷却ユニットの電力はすでに復元しており、温度が低下し始めていることを確認しております。残りのオフラインの冷却ユニットは引き続き作業を続け、温度を通常レベルに戻します。温度が回復次第、影響を受ける EC2 インスタンスと EBS ボリュームが回復します。EC2 および EBS API を含むその他のシステムは、影響を受けるアベイラビリティーゾーン内で正常に動作しています。影響を受けた EC2 インスタンスおよび EBS ボリュームをお持ちのお客様は、影響を受けたアベイラビリティーゾーン、または AP-NORTHEAST-1 リージョン内の別のアベイラビリティーゾーンでインスタンスの再作成を試みることができます。| We continue to work on addressing the increase in ambient temperature affecting a small section of a single Availability Zone (apne1-az1) in the AP-NORTHEAST-1 region. The increase in temperature is caused by a loss of power to the cooling units within the affected section of the Availability Zone. We have now restored power to a number of the cooling units within this section of the Availability Zone and are starting to see temperatures decreasing. We will continue to work through the remaining cooling units that are still offline, which will return temperatures to normal levels. Once temperatures have recovered, we would expect to see affected EC2 instances and EBS volumes begin to recover. Other systems, including EC2 and EBS APIs, are operating normally within the affected Availability Zone. Customers with affected EC2 instances and EBS volumes can attempt to relaunch in the affected Availability Zone, or another Availability Zone within the AP-NORTHEAST-1 Region.
10:42 AM PST -  (3:42AM JST)AP-NORTHEAST-1 リージョンのうちの 1 つのアベイラビリティーゾーン (apne1-az1) のある一部の区画で影響を受けていた冷却ユニットの多くの電源が回復しました。室温は通常のレベルに近い状況まで戻り、ネットワーク、EC2 および EBS ボリュームの回復処理を開始しています。ネットワークはすでに回復し、EC2とEBSボリューム の回復処理に着手しております。回復処理が始まると再起動が発生するため、お客様にはお使いのインスタンスでアクションをとっていただく場合がございます。EBSボリュームに関しましては、ボリュームが回復するにつれ、degraded I/Oパフォーマンスが通常に戻ります。 ”stopping” もしくは ”shutting-down” のまま止まってしまっているインスタンスに関しましては、回復処理が進むにつれ、 ”stopped” もしくは “terminated” に戻ります。| We have now restored power to the majority of the cooling units within the affected section of the Availability Zone (apne1-az1) in the AP-NORTHEAST-1 Region. Temperatures are now close to normal levels and we have begun the process of restoring networking, EC2 instances and EBS volumes. The network has been restored within the affected section of the Availability Zone and we are now working on EC2 instances and EBS volumes. As they begin to recover, customers may need to take action on their instance as it will have experienced a reboot. For EBS volumes, degraded I/O performance will return to normal levels as volumes recover. For instances that are stuck “stopping” or “shutting-down”, these will return to the “stopped” or “terminated” state as recovery proceeds.
11:26 AM PST -  (4:26AM JST) AP-NORTHEAST-1 リージョンのうちの 1 つのアベイラビリティーゾーン (apne1-az1) で影響を受けていた冷却サブシステムの電源が回復しました。現在、室温は通常レベルで運用されています。大部分の ES2 インスタンスと EBS ボリュームが復旧しておりますが、残りのインスタンスとボリュームの復旧作業に引き続き取り組んでいます。| We have now restored power to the cooling subsystem within the affected section of the Availability Zone (apne1-az1) in the AP-NORTHEAST-1 Region. Temperatures are now operating at normal levels. We are also seeing recovery for the majority of EC2 instances and EBS volumes and continue to work on the remaining instance and volumes.
12:09 PM PST -  (5:09AM JST)アベイラビリティゾーン (apne1-az1) で影響を受けた一部の区画の室温は安定し、通常のレベルに戻りました。多くの EC2インスタンスは回復済みとなっております。多くの EBSボリュームも回復済みですが、残りの少数のボリュームの復旧作業に引き続き取り組んでおります。| Temperatures within the affected section of the Availability Zone (apne1-az1) remain stable and at normal levels. We have now recovered the vast majority of EC2 instances. The majority of EBS volumes have also recovered but there are a few that have required some engineering intervention that we are working on.
12:54 PM PST -  (5:54AM JST)日本時間 02/19 11:01 PM から、AP-NORTHEAST-1 リージョンのうちの１つのアベイラビリティーゾーンの一部の区画で室温の上昇を確認いたしました。日本時間 02/19 11:03 PM から、室温が上昇した結果として、一部の EC2インスタンスが影響を受け、一部のEBSボリュームではパフォーマンスが低下しました。根本的な原因は、影響を受けたアベイラビリティーゾーンのセクション内の冷却システムへの電力の喪失であり、すでに回復済みです。日本時間 02/20 03:30 AM までに、電力は冷却システム内のほとんどのユニットで復旧し、室温は通常のレベルに戻りました。日本時間 02/20 04:00 AM までに、EC2 インスタンスと EBS ボリュームの回復が始まり、日本時間 02/20 05:30 AM 時点で、影響を受けた EC2 インスタンスと EBS ボリュームの大部分は通常通り動作しております。一部のインスタンスとボリュームは、イベントによって影響を受けたハードウェア上でホストされていました。引き続き影響を受けたすべてのインスタンスとボリュームの復旧に取り組み、Personal Health Dashboard を通じて、現在も影響を受けているお客様に対し通知を行います。即時の復旧が必要な場合は、影響を受けているインスタンスまたはボリュームを置き換えていただくことをお勧めします。| Starting at 6:01 AM PST, we experienced an increase in ambient temperatures within a section of a single Availability Zone within the AP-NORTHEAST-1 Region. Starting at 6:03 AM PST, some EC2 instances were impaired and some EBS volumes experienced degraded performance as a result of the increase in temperature. The root cause was a loss of power to the cooling system within a section of the affected Availability Zone, which engineers worked to restore. By 10:30 AM PST, power had been restored to the majority of the units within the cooling system and temperatures were returning to normal levels. By 11:00 AM PST, EC2 instances and EBS volumes had begun to recover and by 12:30 PM PST, the vast majority of affected EC2 instances and EBS volumes were operating normally. A small number of remaining instances and volumes are hosted on hardware which was adversely affected by the event. We continue to work to recover all affected instances and volumes and have opened notifications for the remaining impacted customers via the Personal Health Dashboard. For immediate recovery, we recommend replacing any remaining affected instances or volumes, if possible."
367,Amazon Elastic Load Balancing (Tokyo),[RESOLVED] API Error Rate,1613755243,1,,"<div><span class=""yellowfg""> 9:20 AM PST</span>&nbsp;(2:20AM JST)現在、AP-NORTHEAST-1 リージョンでの、ELB API エラー率の上昇について調査を進めております。既存のロードバランサーへの接続には影響はありません。 |  We are investigating increased error rates for ELB APIs in the AP-NORTHEAST-1 Region. Connectivity to existing load balancers is not affected.</div><div><span class=""yellowfg""> 9:27 AM PST</span>&nbsp;(2:27AM JST)日本時間 2/20 AM 2:00 から AM 2:18 にかけて AP-NORTHEAST-1 リージョンにおいて API エラーレートの増加を確認しました。すでに問題は復旧し、通常通り動作しております。 |  Between 9:00 AM and 9:18 AM PST we experienced increased API error rates in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally.</div>",elb-ap-northeast-1,2021-02-19 17:20:43,Elastic Load Balancing,Tokyo,9:00 AM,9:18 AM,PST,18.0,2021,2,19,2021-02-19,api error rate," 9:20 AM PST -  (2:20AM JST)現在、AP-NORTHEAST-1 リージョンでの、ELB API エラー率の上昇について調査を進めております。既存のロードバランサーへの接続には影響はありません。 |  We are investigating increased error rates for ELB APIs in the AP-NORTHEAST-1 Region. Connectivity to existing load balancers is not affected.
 9:27 AM PST -  (2:27AM JST)日本時間 2/20 AM 2:00 から AM 2:18 にかけて AP-NORTHEAST-1 リージョンにおいて API エラーレートの増加を確認しました。すでに問題は復旧し、通常通り動作しております。 |  Between 9:00 AM and 9:18 AM PST we experienced increased API error rates in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally."
368,Amazon Elasticsearch Service (N. Virginia),[RESOLVED]  Increased Domain Operation Error Rates,1614293418,1,,"<div><span class=""yellowfg""> 2:50 PM PST</span>&nbsp;Beginning at 4:06 AM PST we began experiencing increased error rates impacting Create and Modify operations for Elasticsearch Domains in the US-EAST-1 Region. While many customers have been notified in their Personal Health Dashboard, we wanted to share more information about this event as the impact is ongoing. We have identified the component responsible for these errors are actively working toward identifying the root cause and mitigating the issue.</div><div><span class=""yellowfg""> 3:08 PM PST</span>&nbsp;We have identified the root cause of the increased error rates impacting Create and Modify operations for Elasticsearch Domains in the US-EAST-1 Region. We have begun mitigating the issue and are working towards recovery.</div><div><span class=""yellowfg""> 3:52 PM PST</span>&nbsp;We are continuing to make progress in mitigating the issue. The rate of elevated latencies and errors for affected customers will begin declining, however we expect it may take several hours for the issue to be resolved completely. </div><div><span class=""yellowfg""> 4:22 PM PST</span>&nbsp;We continue to make progress in mitigating the issue and are more than halfway complete. The rate of elevated latencies and errors for affected customers is declining. However, we expect it may take 2-3 hours for the issue to be resolved completely. </div><div><span class=""yellowfg""> 5:37 PM PST</span>&nbsp;From 4:06 AM to 5:15 PM PST, Create and Modify operations were experiencing increased error rates and latencies. The issue has been resolved and the service is now operating normally. We've contacted customers directly on their Personal Health Dashboard who have domains created during the impact period and are still experiencing issues.</div>",elasticsearch-us-east-1,2021-02-25 22:50:18,Elasticsearch Service,N. Virginia,4:06 AM,5:15 PM,PST,789.0,2021,2,25,2021-02-25,increased domain operation error rates," 2:50 PM PST -  Beginning at 4:06 AM PST we began experiencing increased error rates impacting Create and Modify operations for Elasticsearch Domains in the US-EAST-1 Region. While many customers have been notified in their Personal Health Dashboard, we wanted to share more information about this event as the impact is ongoing. We have identified the component responsible for these errors are actively working toward identifying the root cause and mitigating the issue.
 3:08 PM PST -  We have identified the root cause of the increased error rates impacting Create and Modify operations for Elasticsearch Domains in the US-EAST-1 Region. We have begun mitigating the issue and are working towards recovery.
 3:52 PM PST -  We are continuing to make progress in mitigating the issue. The rate of elevated latencies and errors for affected customers will begin declining, however we expect it may take several hours for the issue to be resolved completely. 
 4:22 PM PST -  We continue to make progress in mitigating the issue and are more than halfway complete. The rate of elevated latencies and errors for affected customers is declining. However, we expect it may take 2-3 hours for the issue to be resolved completely. 
 5:37 PM PST -  From 4:06 AM to 5:15 PM PST, Create and Modify operations were experiencing increased error rates and latencies. The issue has been resolved and the service is now operating normally. We've contacted customers directly on their Personal Health Dashboard who have domains created during the impact period and are still experiencing issues."
369,AWS Transfer for SFTP (N. Virginia),[RESOLVED] Increased Login Failures,1614879450,1,,"<div><span class=""yellowfg""> 9:37 AM PST</span>&nbsp;Between 3:34 AM and 6:39 AM PST we experienced increased login failures for Transfer Family in the US-EAST-1 Region. While many customers have been notified in their Personal Health Dashboard, we wanted to share more information about this event. We have identified the component responsible for these errors, and have mitigated the issue. No customer actions are required. The issue has been resolved and the service is operating normally.</div>",transfer-us-east-1,2021-03-04 17:37:30,Transfer for SFTP,N. Virginia,3:34 AM,6:39 AM,PST,185.0,2021,3,4,2021-03-04,increased login failures," 9:37 AM PST -  Between 3:34 AM and 6:39 AM PST we experienced increased login failures for Transfer Family in the US-EAST-1 Region. While many customers have been notified in their Personal Health Dashboard, we wanted to share more information about this event. We have identified the component responsible for these errors, and have mitigated the issue. No customer actions are required. The issue has been resolved and the service is operating normally."
370,AWS CloudFormation (N. Virginia),[RESOLVED] Increased Console Error Rates,1614879598,1,,"<div><span class=""yellowfg""> 9:39 AM PST</span>&nbsp;We are investigating increased error rates accessing the CloudFormation Management Console in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 9:51 AM PST</span>&nbsp;We can confirm increased error rates accessing the CloudFormation Management Console in the US-EAST-1 Region. CloudFormation APIs are unaffected by this issue and can still be accessed via the CLI or SDK.</div><div><span class=""yellowfg"">10:18 AM PST</span>&nbsp;We are beginning to observe recovery for the increased error rates accessing the CloudFormation Management Console in the US-EAST-1 Region. CloudFormation APIs continue to be unaffected by this issue.</div><div><span class=""yellowfg"">10:40 AM PST</span>&nbsp;Between 9:14 AM and 9:52 AM PST we experienced increased error rates accessing the CloudFormation Management Console in the US-EAST-1 Region. CloudFormation APIs were unaffected by this issue and were still able to be accessed via the CLI or SDK during this event. The issue has been resolved and the service is operating normally.</div>",cloudformation-us-east-1,2021-03-04 17:39:58,CloudFormation,N. Virginia,9:14 AM,9:52 AM,PST,38.0,2021,3,4,2021-03-04,increased console error rates," 9:39 AM PST -  We are investigating increased error rates accessing the CloudFormation Management Console in the US-EAST-1 Region.
 9:51 AM PST -  We can confirm increased error rates accessing the CloudFormation Management Console in the US-EAST-1 Region. CloudFormation APIs are unaffected by this issue and can still be accessed via the CLI or SDK.
10:18 AM PST -  We are beginning to observe recovery for the increased error rates accessing the CloudFormation Management Console in the US-EAST-1 Region. CloudFormation APIs continue to be unaffected by this issue.
10:40 AM PST -  Between 9:14 AM and 9:52 AM PST we experienced increased error rates accessing the CloudFormation Management Console in the US-EAST-1 Region. CloudFormation APIs were unaffected by this issue and were still able to be accessed via the CLI or SDK during this event. The issue has been resolved and the service is operating normally."
371,AWS Internet Connectivity (Sydney),[RESOLVED] Network Connectivity,1615136782,1,,"<div><span class=""yellowfg""> 9:06 AM PST</span>&nbsp;We are investigating an issue with an external provider outside of our network, which may be impacting Internet connectivity between some customer networks and the AP-SOUTHEAST-2 Region. Connectivity to instances and services within the Region is not impacted by the event.</div><div><span class=""yellowfg""> 9:39 AM PST</span>&nbsp;We are continuing to experience connectivity issues with an internet provider external to our network. This affects international connectivity from AP-SOUTHEAST-2 Region to internet destinations outside Australia. Connectivity within the AP-SOUTHEAST-2 or to internet destinations within Australia are not impacted at this time. We are continuing to work towards shifting traffic from the external provider to recover from the event.</div><div><span class=""yellowfg"">10:24 AM PST</span>&nbsp;We are continuing to experience connectivity issues with an internet provider external to our network. This affects internet traffic both to and from the AP-SOUTHEAST-2 Region to international internet destinations outside Australia. Connectivity within the AP-SOUTHEAST-2 Region or to internet destinations within Australia are not impacted at this time. We are continuing to work towards shifting traffic from the external provider to recover from the event.</div><div><span class=""yellowfg"">11:51 AM PST</span>&nbsp;Between 8:19 AM and 10:59 AM PST we experienced internet connectivity issue with an external provider outside of our network. This impacted internet traffic both to and from the AP-SOUTHEAST-2 Region to internet destinations outside Australia. Connectivity from locations within Australia to the AP-SOUTHEAST-2 Region or to instances and services within the Region was not impacted. The issue has been resolved and connectivity has been restored.
</div>",internetconnectivity-ap-southeast-2,2021-03-07 17:06:22,Internet Connectivity,Sydney,8:19 AM,10:59 AM,PST,160.0,2021,3,7,2021-03-07,network connectivity," 9:06 AM PST -  We are investigating an issue with an external provider outside of our network, which may be impacting Internet connectivity between some customer networks and the AP-SOUTHEAST-2 Region. Connectivity to instances and services within the Region is not impacted by the event.
 9:39 AM PST -  We are continuing to experience connectivity issues with an internet provider external to our network. This affects international connectivity from AP-SOUTHEAST-2 Region to internet destinations outside Australia. Connectivity within the AP-SOUTHEAST-2 or to internet destinations within Australia are not impacted at this time. We are continuing to work towards shifting traffic from the external provider to recover from the event.
10:24 AM PST -  We are continuing to experience connectivity issues with an internet provider external to our network. This affects internet traffic both to and from the AP-SOUTHEAST-2 Region to international internet destinations outside Australia. Connectivity within the AP-SOUTHEAST-2 Region or to internet destinations within Australia are not impacted at this time. We are continuing to work towards shifting traffic from the external provider to recover from the event.
11:51 AM PST -  Between 8:19 AM and 10:59 AM PST we experienced internet connectivity issue with an external provider outside of our network. This impacted internet traffic both to and from the AP-SOUTHEAST-2 Region to internet destinations outside Australia. Connectivity from locations within Australia to the AP-SOUTHEAST-2 Region or to instances and services within the Region was not impacted. The issue has been resolved and connectivity has been restored."
372,Amazon Cognito (Ohio),[RESOLVED] Increased Errors,1615329039,1,,"<div><span class=""yellowfg""> 2:30 PM PST</span>&nbsp;We are investigating increased Cognito User Pool API errors in the US-EAST-2 Region.</div><div><span class=""yellowfg""> 2:54 PM PST</span>&nbsp;We can confirm increased Cognito User Pool API errors in the US-EAST-2 Region. We have confirmed the root cause to be a subsystem deployment which we are now in the process of rolling back. We are observing steady signs of recovery as the rollback has progressed.</div><div><span class=""yellowfg""> 3:04 PM PST</span>&nbsp;Between 1:56 and 2:50 PM PST, we experienced increased Cognito User Pool API errors in the US-EAST-2 Region. We have confirmed the root cause to be a subsystem deployment and the rollback of that deployment has completed, resulting in recovery. The issue has been resolved and the service is operating normally.</div>",cognito-us-east-2,2021-03-09 22:30:39,Cognito,Ohio,2:30 PM,2:50 PM,PST,20.0,2021,3,9,2021-03-09,increased errors," 2:30 PM PST -  We are investigating increased Cognito User Pool API errors in the US-EAST-2 Region.
 2:54 PM PST -  We can confirm increased Cognito User Pool API errors in the US-EAST-2 Region. We have confirmed the root cause to be a subsystem deployment which we are now in the process of rolling back. We are observing steady signs of recovery as the rollback has progressed.
 3:04 PM PST -  Between 1:56 and 2:50 PM PST, we experienced increased Cognito User Pool API errors in the US-EAST-2 Region. We have confirmed the root cause to be a subsystem deployment and the rollback of that deployment has completed, resulting in recovery. The issue has been resolved and the service is operating normally."
373,AWS Lambda (N. Virginia),[RESOLVED] Increased Invoke Errors,1615925287,1,,"<div><span class=""yellowfg""> 1:08 PM PDT</span>&nbsp;We are investigating increased invoke times and errors in the US-EAST-1 Region.</div><div><span class=""yellowfg""> 1:21 PM PDT</span>&nbsp;Between 12:10 PM and 12:58 PM PDT, we experienced increased invoke error rates and invoke times in the US-EAST-1 Region. This issue has been resolved and the service is operating normally.</div>",lambda-us-east-1,2021-03-16 20:08:07,Lambda,N. Virginia,12:10 PM,12:58 PM,PDT,48.0,2021,3,16,2021-03-16,increased invoke errors," 1:08 PM PDT -  We are investigating increased invoke times and errors in the US-EAST-1 Region.
 1:21 PM PDT -  Between 12:10 PM and 12:58 PM PDT, we experienced increased invoke error rates and invoke times in the US-EAST-1 Region. This issue has been resolved and the service is operating normally."
